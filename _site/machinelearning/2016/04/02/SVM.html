<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1" /> <title>Support Vector Machine</title> <meta name="twitter:card" content="summary" /> <meta name="twitter:site" content="@kymo" /> <meta name="twitter:title" content="Support Vector Machine" /> <meta name="twitter:description" content="Support Vector Machine ，支持向量机，通常用来进行classification，但是也有做regression。SVM在面对非线性问题上具有独特的优势。本文从linear和nonlinear两种情况下对SVM的建模过程、优化目标的求解推导过程以及优化算法SMO进行阐述。"> <meta name="description" content="Support Vector Machine ，支持向量机，通常用来进行classification，但是也有做regression。SVM在面对非线性问题上具有独特的优势。本文从linear和nonlinear两种情况下对SVM的建模过程、优化目标的求解推导过程以及优化算法SMO进行阐述。"> <link rel="icon" href="/assets/favicon.png"> <link rel="apple-touch-icon" href="/assets/touch-icon.png"> <link rel="stylesheet" href="//code.cdn.mozilla.net/fonts/fira.css"> <link rel="stylesheet" href="/assets/core.css"> <link rel="canonical" href="/machinelearning/2016/04/02/SVM.html"> <link rel="alternate" type="application/atom+xml" title="Aron's blog" href="/feed.xml" /> <!-- mathjax config similar to math.stackexchange --> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ extensions: ["tex2jax.js"], jax: ["input/TeX", "output/HTML-CSS"], tex2jax: { inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$']], processEscapes: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }, messageStyle: "none", "HTML-CSS": { preferredFont: "TeX", availableFonts: ["TeX"] } }); </script> <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> </head> <body> <aside class="logo"> <a href="/"> <img src="/public/image/avatar.jpg" class="gravatar"> </a> <span class="logo-prompt">Back to Home</span> </aside> <main> <noscript> <style> article .footnotes { display: block; } </style> </noscript> <article> <div style='float:left;'> <div class="center"> <h1>Support Vector Machine</h1> <time>April 2, 2016</time> </div> <div class="divider"></div> <p>Support Vector Machine ，支持向量机，通常用来进行classification，但是也有做regression。SVM在面对非线性问题上具有独特的优势。本文从linear和nonlinear两种情况下对SVM的建模过程、优化目标的求解推导过程以及优化算法SMO进行阐述。</p> <script type="math/tex; mode=display">a^2 + b^2 = c^2</script> <h3 id="linear-svm">1. linear SVM</h3> <p>在分类任务中，样本label为<script type="math/tex">{-1,1}</script>，关于从sign distance转换到geometry distance的过程其实很容易理解， sign distance可以衡量某个样本被分类的置信，如果sign distance越大，那么该样本被分为该类别的可信度就更大； 而geometry distance可以理解为样本距离超平面<script type="math/tex">Y = w^TX + b</script>的距离，是sign distance归一化的结果， 求解目标为<script type="math/tex">argmax(\frac { \mid w^Tx + b \mid } { \mid \mid w \mid \mid })</script>，并且需要满足约束：<script type="math/tex">y_i(w^Tx_i + b) \geq 1</script>，为了求解方便，可以不加证明的令<script type="math/tex">\mid w^Tx+b \mid =1</script>，形式化如下：</p> <script type="math/tex; mode=display">\begin{cases}argmax \frac{1}{ \mid \mid w \mid \mid } \\ s.t ~~ y_i(w^Tx_i+b) \geq 1 ~~i=1,2,\cdot \cdot ,n \end{cases}</script> <p>而此时最大化 <script type="math/tex">\frac{1}{ \mid \mid w \mid \mid }</script>等价于最小化 <script type="math/tex">w</script> ，并且最小化和最小化<script type="math/tex">\frac {1}{2} w^Tw</script>等价，</p> <p>所以1.1可以变为：</p> <script type="math/tex; mode=display">\begin{cases}argmmin \frac {1}{2} w^Tw \\ s.t ~~ y_i(w^Tx_i+b) \geq 1 ~~i=1,2,\cdot \cdot ,n \end{cases}</script> <p>由此可以得到不等式约束问题，原始问题通过分析不难发现，求解十分困难，不过对于PSO(粒子群算法)而言，往往可以求得比较好的解。不过在碰到这种带不等式约束的问题的时候，我们可以通过拉格朗日对偶性质将原始问题转化为对偶问题，在最大熵模型中也有类似的处理过程。首先构造拉格朗日函数：</p> <script type="math/tex; mode=display">L(w,b,\alpha) = \frac {1} {2} w^Tw - \sum_{i=1}^N a_i[y_i(w^Tx_i + b) - 1]</script> <p>首先我们可以得到这个函数的等价形式，令<script type="math/tex">\theta = argmax_\alpha L(w,b,\alpha)</script>，那么：</p> <script type="math/tex; mode=display">% <![CDATA[ \theta=\begin{cases}\frac {1} {2} w^Tw~~~y_i(w^Tx_i+b) \geq 1\\ \infty~~~~ y_i(w^Tx_i+b) < 1~~\end{cases} %]]></script> <p>可见，<script type="math/tex">min\theta</script>和原始优化目标<script type="math/tex">argmin \frac {1}{2}w^Tw</script>等价。令<script type="math/tex">p=min\theta</script>，其对偶形式<script type="math/tex">d=max_{\alpha}min_{w,b}L(w,\alpha)</script>，那么必然会有<script type="math/tex">p \geq d</script>。此时令<script type="math/tex">g=argmin_{w} L(w,b,\alpha)</script>，<script type="math/tex">g \leq L(w^*,b,\alpha) \leq \frac {1}{2} {w^*}^Tw^* = p</script>，所以<script type="math/tex">p \geq d</script>。所以此时该算法满足弱对偶，我们可以通过求解该弱对偶问题去近似求解原始问题，在EM中就是不断优化极大似然下界~ 并且可以知道的是，不管原始问题是何种优化，对偶问题都会是凸优化，也即都会存在极值。不过在SVM中，我们是可以把弱对偶加强，变成strong duality，也即<script type="math/tex">p = d</script>，优化对偶问题等价于对原始问题的求解。那么怎么判断该对偶问题是强对偶问题呢？KKT条件。如下：</p> <p>\begin{cases} \bigtriangledown L(w,b,\alpha) = 0 \ \alpha_i(y_i(w^Tx_i+b) - 1) = 0\ \alpha_i \geq 0 \end{cases}</p> <p>很明显，此时KKT条件成立，所以满足强对偶。其中<script type="math/tex">y_i(w^Tx_i+b)-1) = 0</script>但此时KKT只是必要条件，不过由于我们的原始问题是凸优化，所以KKT便是充要条件了。</p> <p>对对偶问题，首先是<script type="math/tex">min_{w,b}L(w,b,\alpha)</script>，分别对<script type="math/tex">w</script>和<script type="math/tex">b</script>求导，得：</p> <p>[\frac {\partial L(w,\alpha,b)} {\partial(w)} = w - \sum_{i=1}^N \alpha_iy_ix_i=0]</p> <p>[\frac {\partial L(w,\alpha,b)} {\partial(b)} = \sum_{i=1}^N\alpha_iy_i=0]</p> <p>随后，将<script type="math/tex">w=\sum_{i=1}^N\alpha_iy_ix_i</script>代入<script type="math/tex">L(w,b,\alpha)</script>中，则有：</p> <p>\begin{eqnarray<em>} L(w,\alpha,b)&amp;=&amp;\frac {1}{2} w^Tw - \sum_{i=1}^N\alpha_i[y_i(w^Tx_i+b)-1]\&amp;=&amp;\frac{1}{2}\sum_{i=1}^N\alpha_iy_ix_i^T\sum_{j=1}^N\alpha_jy_jx_j - \sum_{i=1}^N\alpha_i[y_i(w^Tx_i+b)-1]\ &amp;=&amp;\frac {1}{2} \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j - \sum_{i=1}^N\alpha_i\sum_{j=1}^N\alpha_jy_jx_j^Tx_i + \sum_{i=1}^N\alpha_i\&amp;=&amp;-\frac {1}{2} \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j + \sum_{i=1}^N\alpha_i\end{eqnarray</em>}</p> <p>所以：</p> <p>[\begin{cases}-\frac {1}{2} \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j + \sum_{i=1}^N\alpha_i\s.t~~~\sum_{i=1}^N\alpha_iy_i=0 \end{cases}]</p> <p>到这，我们还没有考虑soft margin。实际情况中，总是会存在一定的噪声数据，使得我们的分类超平面被这些噪声数据所误导，从而使得模型的variance增大，所以一般来讲都会采用soft margin构建优化函数，我们以<script type="math/tex">\varepsilon</script>的范围容许一定的误差，即原来的<script type="math/tex">y_i(w^Tx_i+b) \geq 1</script>此时为<script type="math/tex">y_i(w^Tx_i + b) \geq 1 - \varepsilon_i</script>，所以我们的优化目标变为：</p> <p>[\begin{cases}argmin \frac {1}{2} w^Tw + C\sum_{i=1}^N\varepsilon_i \s.t ~~ y_i(w^Tx_i+b) \geq 1 - \varepsilon_i ~~i=1,2,\cdot \cdot ,n \ \sum_{i=1}^N\varepsilon_i \geq C \end{cases}]</p> <p>关于上式，可以看成是利用hinge loss加l2范数正则项的结果，SVM此时的损失函数可以表示为 <script type="math/tex">min_{w,b} \sum_{i=1}^N [1-y_i(w^Tx_i+b)]_{+} + \lambda\ \mid w\ \mid ^2</script>，其中如果<script type="math/tex">z>0</script>，那么<script type="math/tex">z_{+}</script>=z，否则等于0。如果令<script type="math/tex">\varepsilon_i=1-y_i(w^Tx_i+b),\varepsilon_i \geq 0</script>，那么此时最优化问题为 <script type="math/tex">min_{w,b} \sum_{i=1}^N\varepsilon_i + \lambda\ \mid w\ \mid ^2</script>，如果取<script type="math/tex">\lambda=\frac {1}{2C}</script>，那么就和上述优化目标等价，所以可以看出，软间隔实际上是在ERM的基础上加了SRM~</p> <p>之后的推导没有多大差别，只不过在求导的过程中出现了<script type="math/tex">\alpha_i = C - \varepsilon_i</script>，又有前面在对偶转换使用的KKT条件之一<script type="math/tex">\alpha_i(y_i(w^Tx_i+b) - 1) =0</script>可得，在分类超平面上的点，也即满足<script type="math/tex">y_i(w^Tx_i+b)-1 + \varepsilon_i = 0</script>，而那些不在超平面的点，必然有<script type="math/tex">\alpha_i=0</script>，而在超平面上的，则有<script type="math/tex">% <![CDATA[ 0< \alpha_i < C %]]></script>，在超平面之外的则是<script type="math/tex">\alpha_i=C</script>。</p> <ol> <li>nonlinear SVM</li> </ol> <p>非线性情况之下，利用线性曲线去拟合，明显会产生underfitting，但是我们可以通过函数映射的方式，将原来空间中的非线性特征，映射到高维空间中，使得样本可分或者近似可分，实际上是，机器学习中有一种叫做基展开的技术，就是处理这种线性到非线性的特征映射。不过对于SVM中使用这种非线性变化是因为它能够和核函数配合的天衣无缝。</p> <p>这里用一个简单的例子作简要说明。对于<script type="math/tex">x=(x_1,x_2)</script>二维空间的某个点，我们将其映射到三维空间。所利用的映射函数可以为<script type="math/tex">\phi (x_1,x_2) = (x_1^2,x_2^2,2x_1x_2)</script>，那么在三维空间中，样本线性可分的可能性更大，但是计算开销却上升了，因为在转化成对偶问题之后就产生了向量内积运算。对于原始二维空间中的两点<script type="math/tex">p=(\eta_1,\eta_2),q=(\gamma_1,\gamma_2)</script>，在三维空间中的向量内积为<script type="math/tex">% <![CDATA[ <\phi(\eta_1,\eta_2),\phi(\gamma_1,\gamma_2)> = \eta_1^2\gamma_1^2 + \eta_2^2\gamma_2^2 + 4\eta_1\eta_2\gamma_1\gamma_2 %]]></script>，这和<script type="math/tex">\eta_1^2\gamma_1^2 + \eta_2^2\gamma_2^2 + 2\eta_1\eta_2\gamma_1\gamma_2</script>十分相似，而后者却等于<script type="math/tex">% <![CDATA[ <(\eta_1,\eta_2), (\gamma_1, \gamma_2)>^2 %]]></script>，所以只需要令<script type="math/tex">\phi(x_1,x_2) = (x_1^2, x_2^2, \sqrt {2} x_1x_2)</script>，就可以得到<script type="math/tex">% <![CDATA[ <\phi(\eta_1,\eta_2),\phi(\gamma_1,\gamma_2)> %]]></script>=<script type="math/tex">% <![CDATA[ <(\eta_1,\eta_2), (\gamma_1, \gamma_2)>^2 %]]></script>~ 由此可以推广到高维。不难看出在高维空间中的内积可以通过在原始空间内积的平方得到~此时<script type="math/tex">% <![CDATA[ K(p,q) = <\phi(\eta_1,\eta_2), \phi(\gamma_1, \gamma_2)> %]]></script>=<script type="math/tex">% <![CDATA[ <p,q>^2 %]]></script>。对偶转换之后只需要将<script type="math/tex">x_i,x_j</script>的内积运算更换成<script type="math/tex">K(x_i,x_j)</script>，即可处理非线性数据~</p> <ol> <li>SMO</li> </ol> <p>SMO本质上上一种坐标上升优化算法，坐标上升可以理解为在<script type="math/tex">p</script>维向量构成的空间中，每次选择一个维度进行优化，最终能够求得比较合适的解。SMO每次选择两个参数，因为此时待求变量有<script type="math/tex">\sum_{i=1}^N\alpha_iy_i =0</script>的约束。SMO的优化过程如下：</p> <p>选择<script type="math/tex">\alpha_i, \alpha_j</script> 固定其他参数，然后对<script type="math/tex">\alpha_i, \alpha_j</script>进行优化 利用<script type="math/tex">\alpha_i, \alpha_j</script>，对截距进行优化 在了解确切的<script type="math/tex">\alpha_i,\alpha_j</script>贪心选择策略之前，先假定我们已经将<script type="math/tex">\alpha_i,\alpha_j</script>选择妥当，然后直接对<script type="math/tex">\alpha_i,\alpha_j</script>进行优化。根据条件<script type="math/tex">\sum_{k=1}^N\alpha_ky_k=0</script>，令<script type="math/tex">A = y_i\sum_{k!=i,j}^N\alpha_ky_k</script>. 那么<script type="math/tex">\alpha_i,\alpha_j</script>的关系为<script type="math/tex">\alpha_i = A - y_iy_j\alpha_j</script>。此时我们的优化目标[max_{\alpha} L(\alpha) = \sum_{k=1}^N\alpha_k - \frac{1}{2} \sum_{l=1}^N\sum_{k=1}^N\alpha_l\alpha_ky_ly_kK_{l,k}]</p> <p>其中，<script type="math/tex">K_{l,k} = K(x_l, x_j)</script>，令<script type="math/tex">B = \sum_{k!=i,j}\alpha_k, S = y_iy_j, V_i = \sum_{k!=i,j}\alpha_ky_kK_{i,k}, V_j =\sum_{k!=i,j}\alpha_ky_kK_{j,k}</script>，那么有：</p> <p>\begin{eqnarray<em>} L(\alpha) &amp;=&amp; \sum_{k!=i,j}^N\alpha_k + \alpha_i + \alpha_j - \frac {1}{2}[\alpha_i\alpha_i y_iy_iK_{i,i} + \alpha_j\alpha_jy_jy_jK_{j,j} + 2\alpha_i\alpha_jy_iy_jK_{i,j} + 2\sum_{k!=i,j}^N\alpha_i\alpha_ky_iy_kK_{i,k} + 2\sum_{k!=i,j}^N\alpha_j\alpha_ky_jy_kK_{j,k} + \sum_{l!=i,j}^N \sum_{k!=i,j}^N\alpha_l\alpha_ky_ly_kK_{l,k}] \ &amp;=&amp; B + A - S\alpha_j + \alpha_j - \frac{1}{2} [2\alpha_i\alpha_jSK_{i,j} + \alpha_i^2K_{i,i} + \alpha_j^2K_{j,j} + 2\alpha_iy_iV_i + 2\alpha_jy_jV_j + \sum_{l!=i,j}^N \sum_{k!=i,j}^N\alpha_l\alpha_ky_ly_kK_{l,k}] \ &amp;=&amp;-S\alpha_j + \alpha_j - \frac{1}{2}K_{i,i}(A-S\alpha_j)^2 - \frac{1}{2} K_{j,j}\alpha_j^2 - K_{i,j}\alpha_j(A-S\alpha_j)S - \alpha_jy_jV_j - (A-S\alpha_j)y_iV_i + \varepsilon_{constant} \end{eqnarray</em>}</p> <p>其中，<script type="math/tex">\varepsilon_{constant}</script>为一些常量，在求极值点是可以忽略，上式对<script type="math/tex">\alpha_j</script>求导，有：</p> <p>\begin{eqnarray<em>} \frac {\partial_{L(\alpha_j)}} {\partial_{\alpha_j}} &amp;=&amp; -S + 1 + ASK_{i,i} -K_{i,i}\alpha_j - K_{j,j}\alpha_j -ASK_{i,j} + 2K_{i,j}\alpha_j -y_jV_j + Sy_iV_i=0 \ \alpha_j&amp;=&amp; \frac {-S + 1 + AS(K_{i,i}-K_{i,j}) + y_j(V_i-V_j)}{K_{i,i} + K_{j,j} - 2K_{i,j}} \end{eqnarray</em>}</p> <p>又有优化<script type="math/tex">\alpha_i,\alpha_j</script>的时候，其他参数没有被改变。</p> <p>\begin{eqnarray<em>}\alpha_iy_i+\alpha_jy_j&amp;=&amp; -\sum_{k!=i,j}\alpha_k^{old}y_k=\alpha_i^{old}y_i + \alpha_j^{old}y_j \ V_i &amp;=&amp; \sum_{k!=i,j}\alpha_ky_kK_{i,k}=\sum_{k!=i,j}\alpha_k^{old}y_kK_{i,k} = \sum_{k=1}^N\alpha_k^{old}y_kK_{i,k} + b - b - \alpha_i^{old}y_iK_{i,i} - \alpha_j^{old}y_jK_{i,j} \ V_j &amp; = &amp;\sum_{k!=i,j}\alpha_ky_kK_{j,k}=\sum_{k!=i,j}\alpha_k^{old}y_kK_{j,k} = \sum_{k=1}^N\alpha_k^{old}y_kK_{j,k} + b - b - \alpha_j^{old}y_jK_{j,j} - \alpha_i^{old}y_iK_{i,j} \end{eqnarray</em>}</p> <p>且<script type="math/tex">g(x_i) = \sum_{k=1}^N\alpha_k^{old}y_kK_{i,k} + b</script>，<script type="math/tex">g(x_j) = \sum_{k=1}^N\alpha_k^{old}y_kK_{j,k} + b</script>，所以：</p> <p>[V_i - V_j = g(x_i) - g(x_j) - \alpha_i^{old}y_iK_{i,i} + \alpha_j^{old}y_jK_{j,j} - \alpha_j^{old}y_jK_{i,j} + \alpha_i^{old}y_iK_{i,j} ]</p> <p>然后，将A = <script type="math/tex">\alpha_i^{old}+S\alpha_j^{old}</script>，S=<script type="math/tex">y_iy_j</script>代入<script type="math/tex">\alpha_j</script>表达式，可得：</p> <p>\begin{eqnarray<em>}\alpha_j &amp;=&amp; \frac {y_jy_j - y_iy_j +(\alpha_i^{old}+y_iy_j\alpha_j^{old})y_iy_j(K_{i,i}-K_{i,j}) + y_j(g(x_i) - g(x_j) - \alpha_i^{old}y_iK_{i,i} + \alpha_j^{old}y_jK_{j,j} - \alpha_j^{old}y_jK_{i,j} + \alpha_i^{old}y_iK_{i,j})} {K_{i,i} + K_{j,j} - 2K_{i,j}} \ &amp;=&amp; \frac {y_j[y_j-y_i + y_j\alpha_j^{old}(K_{i,i} + K_{j,j} - 2K_{i,j} + g(x_i) - g(x_j) ]} {K_{i,i} + K_{j,j} - 2K_{i,j}}\ &amp;=&amp; \alpha_j^{old} + \frac{y_j[y_j-y_i + g(x_i)-g(x_j)]} {K_{i,i} + K_{j,j} - 2K_{i,j}}\end{eqnarray</em>}</p> <p>然后令<script type="math/tex">E_i = g(x_i) - y_i</script>，<script type="math/tex">\eta = K_{i,i} + K_{j,j} - 2K{i,j}</script>，<script type="math/tex">\alpha_j^{new,unc} = \alpha_j^{old} + \frac {y_j(E_i - E_j)} {\eta}</script>，此时求出的<script type="math/tex">\alpha_j</script>还需要经过边界判定，对<script type="math/tex">\alpha_i,\alpha_j</script>有<script type="math/tex">\alpha_iy_i + \alpha_jy_j = \alpha_i^{old} + \alpha_j^{old}</script>和<script type="math/tex">% <![CDATA[ 0<=\alpha_i<=C %]]></script>，<script type="math/tex">% <![CDATA[ 0<=\alpha_j<=C %]]></script>的条件限制，所以必须对<script type="math/tex">\alpha_i, \alpha_j</script>的上下边界<script type="math/tex">L,H</script>进行确认。</p> <p>如果<script type="math/tex">y_i = y_j</script>，那么<script type="math/tex">L= max(0, \alpha_j^{old} - \alpha_i^{old})</script>，<script type="math/tex">H=min(C,C+\alpha_j^{old} - \alpha_i^{old})</script> 如果<script type="math/tex">y_i!=y_j</script>，那么<script type="math/tex">L=max(0, \alpha_j^{old} + \alpha_i^{old} - C)</script>，<script type="math/tex">H=min(C,\alpha_j^{old} + \alpha_i^{old})</script> 根据上式可以得到<script type="math/tex">\alpha_j^{new}</script>为：</p> <p>[\alpha_j^{new} = \begin{cases} H,~<del>\alpha_j^{new,unc} &gt; H \ \alpha_j^{new,unc},</del>~~~ L&lt;=\alpha_j^{new,unc}&lt;=H \ L,~~~~~\alpha_j^{new,unc} &lt; L \end{cases}]</p> <p>由此可以得到<script type="math/tex">\alpha_i^{new}= \alpha_i^{old} + y_iy_j(\alpha_j^{old} - \alpha_j^{new})</script>。</p> <p>关于<script type="math/tex">alpha_i, \alpha_j</script>的更新策略完成，但是对于上式中的bias也需要进行更新，以保证KKT条件<script type="math/tex">\alpha_j(y_j(\sum_{i=1}^N\alpha_iy_iK_{i,j} + b) - 1) = 0</script>成立。</p> <p>当<script type="math/tex">% <![CDATA[ 0< \alpha_i^{new} < C %]]></script>，<script type="math/tex">y_i(\sum_{k=1}^N\alpha_ky_kK_{i,k}+b) = 1</script> ，所以<script type="math/tex">b_1^{new} = y_i - \sum_{k!=i,j}^N\alpha_ky_kK_{i,k} - \alpha_i^{new}y_iK_{i,i} - \alpha_j^{new}y_jK_{i,j}</script>，又知<script type="math/tex">E_i = g(x_i) - y_i = \sum_{k=1}^N\alpha_ky_kK_{i,k} + b^{old} - y_i</script>，此时除<script type="math/tex">\alpha_i,\alpha_j</script>以外的都不会发生变化，所以<script type="math/tex">E_i = \sum_{k!=i,j}^N\alpha_ky_kK_{k,i} + \alpha_i^{old}y_iKii + \alpha_j^{old}y_jK_{i,j} + b^{old} - y_i</script>，也即 <script type="math/tex">b_i^{new} = -E_i - y_iK_{i,i}(\alpha_i^{new} - \alpha_i^{old}) - y_jK_{i,j}(\alpha_j^{new} - \alpha_j^{old}) + b^{old}</script> 同1，当<script type="math/tex">% <![CDATA[ 0< \alpha_j^{new} < C %]]></script>，则<script type="math/tex">b_2^{new} = -E_j - y_iK_{i,j}(\alpha_i^{new} - \alpha_i^{old}) - y_jK_{j,j}(\alpha_j^{new} - \alpha_j^{old}) + b^{old}</script> 如果<script type="math/tex">\alpha_i^{new}</script>和<script type="math/tex">\alpha_j^{new}</script>都满足条件，则<script type="math/tex">b_1^{new}</script>=<script type="math/tex">b_2^{new}</script> 如果<script type="math/tex">\alpha_i^{new}</script>、<script type="math/tex">\alpha_j^{new}</script>为0或者C，那么取<script type="math/tex">b_1^{new}</script>和<script type="math/tex">b_2^{new}</script>的中值即可 至此，SMO算法结束，不过实际中，需要实时更新<script type="math/tex">E_i</script>，所以在更新完bias之后，再利用已有的信息重新更新<script type="math/tex">E_i</script>即可。</p> <p>最终实现见：SVM@GITHUB</p> <p>参考资料</p> <p>李航 《统计学习方法》</p> <p>http://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html</p> </div> <div id='article_list_p st' style='float:right; width:250px; height:100%; position:absolute; right:10px; top:300px; font-size:12px;'> <h2>文章列表</h2></hr> <a href="/machinelearning/2016/12/02/RNN.html">RNN</a></br> <a href="/2016/09/20/%E8%BD%BB%E9%87%8F%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%9E%B6%E6%9E%84.html">轻量应用服务器架构</a></br> <a href="/machinelearning/2016/08/16/CRF.html">CRF 笔记</a></br> <a href="/machinelearning/2016/08/16/PaperReadingNote.html">论文阅读笔记</a></br> <a href="/machinelearning/2016/04/03/EnsembleLearningMethods.html">Ensemble Learning Methods</a></br> <a href="/naturallanguageprocessing/2016/04/02/pLSA.html">Probability Latent Semantic Analysis</a></br> <a href="/machinelearning/2016/04/02/SVM.html">Support Vector Machine</a></br> <a href="/machinelearning/2016/04/02/SVD.html">SVD</a></br> <a href="/machinelearning/2016/04/02/RandomForests.html">Random Forest</a></br> <a href="/machinelearning/2016/04/02/HMM.html">HMM</a></br> <a href="/machinelearning/2016/04/02/ESL_Notes.html">ESL Notes</a></br> <a href="/algorithm/2016/04/02/DP.html">Dynamic Programming</a></br> <a href="/algorithm/2016/04/02/Bit.html">Bit Manupulation</a></br> </div> </article> <div class="ds-thread" data-thread-key="/machinelearning/2016/04/02/SVM" data-title="Support Vector Machine" data-url="//machinelearning/2016/04/02/SVM.html" style='width:666px;margin:0px auto'></div> <!-- 多说评论框 end --> <!-- 多说公共JS代码 start (一个网页只需插入一次) --> <script type="text/javascript"> var duoshuoQuery = {short_name:"aron"}; (function() { var ds = document.createElement('script'); ds.type = 'text/javascript';ds.async = true; ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js'; ds.charset = 'UTF-8'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds); })(); </script> <!-- 多说公共JS代码 end --> <div class="back"> <a href="/">Back</a> </div> </main> </body> </html>