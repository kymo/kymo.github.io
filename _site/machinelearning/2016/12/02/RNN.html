<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1" /> <title>RNN</title> <meta name="twitter:card" content="summary" /> <meta name="twitter:site" content="@kymo" /> <meta name="twitter:title" content="RNN" /> <meta name="twitter:description" content="递归神经网络的一些小事儿"> <meta name="description" content="递归神经网络的一些小事儿"> <link rel="icon" href="/assets/favicon.png"> <link rel="apple-touch-icon" href="/assets/touch-icon.png"> <link rel="stylesheet" href="//code.cdn.mozilla.net/fonts/fira.css"> <link rel="stylesheet" href="/assets/core.css"> <link rel="canonical" href="/machinelearning/2016/12/02/RNN.html"> <link rel="alternate" type="application/atom+xml" title="Aron blog" href="/feed.xml" /> <!-- mathjax config similar to math.stackexchange --> <script type="text/x-mathjax-config"> MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) }, tex2jax: { inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" }, TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } }, messageStyle: "none" }); </script> <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> </head> <body> <aside class="logo"> <a href="/"> <img src="/public/image/avatar.jpg" class="gravatar"> </a> <span class="logo-prompt">Back to Home</span> </aside> <main> <noscript> <style> article .footnotes { display: block; } </style> </noscript> <article> <div style='float:left;'> <div class="center"> <h1>RNN</h1> <time>December 2, 2016</time> </div> <div class="divider"></div> <h2 id="递归神经网络的一些小事儿">递归神经网络的一些小事儿</h2> <h3 id="1-从神经网络谈起">1. 从神经网络谈起</h3> <p>了解神经网络的都知道，神经网络作为一种非线性模型，在监督学习领域取得了state-of-art的效果，其中反向传播算法的提出居功至伟，到如今仍然是主流的优化神经网络参数的算法. 递归神经网络、卷积神经网络以及深度神经网络作为人工神经网络的”变种”，仍然延续了ANN的诸多特质，如权值连接，激励函数，以神经元为计算单元等，只不过因为应用场景的不同衍生了不同的特性，如：处理变长数据、权值共享等。</p> <p>为了介绍RNN，先简单的介绍ANN. ANN的结构很容易理解，一般是三层结构（输入层-隐含层-输出层）. 隐含层输出<script type="math/tex">o_j</script> 和输出层输出<script type="math/tex">o_k</script>如下。其中<script type="math/tex">net_j</script>为隐含层第<script type="math/tex">j</script>个神经元的输入,<script type="math/tex">u</script>为输入层和隐含层的连接权值矩阵，<script type="math/tex">v</script>为隐含层和输出层之间的连接权值矩阵.</p> <p> $$ \begin{align} o_j &amp; =f(net_j) \\ o_k &amp; =f(net_k) \\ net_j &amp; =\sum_i(x_{i}u_{i,j})+b_j \\ net_k &amp; =\sum_j(o_{j}v_{j,k})+b_k \end{align} $$ </p> <p>定义损失函数为<script type="math/tex">E_p=\frac{1}{2}\sum_k (o_k - d_k)^2</script> ,其中<script type="math/tex">p</script>为样本下标，<script type="math/tex">o^k</script>为第<script type="math/tex">k</script>个输出层神经元的输出,<script type="math/tex">d^k</script>为样本在第$k$个编码值。然后分别对参数<script type="math/tex">v_{j,k}</script>、<script type="math/tex">u_{i,j}</script> 进行求导，可得：</p> <p> $$ \begin{align} \frac{\partial E_p}{\partial v_{j,k}} &amp; = \frac{\partial E_p}{\partial net_k} \frac{\partial net_k}{\partial v_{j,k}} \\ &amp; = \frac{\partial E_p}{\partial net_k}o_j \\ &amp; = \frac{\partial E_p}{\partial o_k}\frac{\partial o_k}{\partial net_k}o_j \\ &amp; = (o_k-d_k)o_k(1-o_k)o_j \end{align} $$ </p> <p> $$ \begin{align} \frac{\partial E_p} {\partial u_{i,j}} &amp; = \frac{\partial E_p} {\partial net_j} \frac{\partial net_j} {\partial u_{i,j}} \\ &amp; =x_i \sum_k \frac{\partial E_p} {\partial net_k} \frac{\partial net_k}{\partial o_j} \frac{\partial o_j}{\partial net_j} \\ &amp; =x_i \sum_k \frac{\partial E_p}{\partial net_k} v_{j,k} o_j(1-o_j) \\ &amp; = x_i o_j(1-o_j) \sum_k \frac{\partial E_p}{\partial net_k} v_{j,k} \end{align} $$ </p> <p>从对$\frac{\partial E_p} {\partial u_{i,j}}$的推导可以得到反向传播的核心思路，令误差项$\beta_k = \frac{\partial E_p} {\partial net_k}$, 则有：</p> <p> $$ \beta_k=o_l(1-o_l)\sum_l\beta_lw_{lk} $$ </p> <p>反向传播的实质是基于梯度下降的优化方法，只不过在优化的过程使用了一种更为优雅的权值更新方式。</p> <h3 id="2-循环神经网络">2. 循环神经网络</h3> <p>传统的神经网络一般都是全连接结构，且非相邻两层之间是没有连接的。一般而言，定长输入的样本很容易通过神经网络来解决，但是类似于NLP中的序列标注这样的非定长输入，前向神经网络却无能为力。</p> <p>于是有人提出了循环神经网络(Recurrent Neural Network)，这是一种无法像前向神经网络一样有可以具象的网络结构的模型，一般认为是网络隐层节点之间有相互作用的连接，其实质可以认为是多个具有相同结构和参数的前向神经网络的stacking, 前向神经网络的数目和输入序列的长度一致，且序列中毗邻的元素对应的前向神经网络的隐层之间有互联结构，其图示( <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">来源</a> )如下. :<img src="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg" alt="" /></p> <p>上图只是一个比较抽象的结构，下面是一个以时间展开的更为具体的结构(<a href="http://www.cnblogs.com/YiXiaoZhou/p/6058890.html">来源</a>)~ :<img src="http://images2015.cnblogs.com/blog/1027162/201611/1027162-20161113162111280-1753976877.png" alt="" /></p> <p>从图中可以看出，每个输出层神经元的输出和前向神经网络</p> </div> <div id='article_list_p st' style='float:right; width:250px; height:100%; position:absolute; right:10px; top:300px; font-size:12px;'> </div> </article> <div class="ds-thread" data-thread-key="/machinelearning/2016/12/02/RNN" data-title="RNN" data-url="//machinelearning/2016/12/02/RNN.html" style='width:666px;margin:0px auto'></div> <!-- 多说评论框 end --> <!-- 多说公共JS代码 start (一个网页只需插入一次) --> <script type="text/javascript"> var duoshuoQuery = {short_name:"aron"}; (function() { var ds = document.createElement('script'); ds.type = 'text/javascript';ds.async = true; ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js'; ds.charset = 'UTF-8'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds); })(); </script> <!-- 多说公共JS代码 end --> <div class="back"> <a href="/">Back</a> </div> </main> </body> </html>