<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1" /> <title>循环神经网络的一些小事儿</title> <meta name="twitter:card" content="summary" /> <meta name="twitter:site" content="@kymo" /> <meta name="twitter:title" content="循环神经网络的一些小事儿" /> <meta name="twitter:description" content="1. 从神经网络谈起"> <meta name="description" content="1. 从神经网络谈起"> <link rel="icon" href="/assets/favicon.png"> <link rel="apple-touch-icon" href="/assets/touch-icon.png"> <link rel="stylesheet" href="//code.cdn.mozilla.net/fonts/fira.css"> <link rel="stylesheet" href="/assets/core.css"> <link rel="canonical" href="/machinelearning/2016/12/02/RNN.html"> <link rel="alternate" type="application/atom+xml" title="Aron blog" href="/feed.xml" /> <!-- mathjax config similar to math.stackexchange --> <script type="text/x-mathjax-config"> MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) }, tex2jax: { inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" }, TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } }, messageStyle: "none" }); </script> <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> </head> <body> <aside class="logo"> <a href="/"> <img src="/public/image/avatar.jpg" class="gravatar"> </a> <span class="logo-prompt">Back to Home</span> </aside> <main> <noscript> <style> article .footnotes { display: block; } </style> </noscript> <article> <div style='float:left;'> <div class="center"> <h1>循环神经网络的一些小事儿</h1> <time>December 2, 2016</time> </div> <div class="divider"></div> <h3 id="1-从神经网络谈起">1. 从神经网络谈起</h3> <p>了解神经网络的都知道，神经网络作为一种非线性模型，在监督学习领域取得了state-of-art的效果，其中反向传播算法的提出居功至伟，到如今仍然是主流的优化神经网络参数的算法. 递归神经网络、卷积神经网络以及深度神经网络作为人工神经网络的”变种”，仍然延续了ANN的诸多特质，如权值连接，激励函数，以神经元为计算单元等，只不过因为应用场景的不同衍生了不同的特性，如：处理变长数据、权值共享等。</p> <p>为了介绍RNN，先简单的介绍ANN. ANN的结构很容易理解，一般是三层结构（输入层-隐含层-输出层）. 隐含层输出<script type="math/tex">o_j</script> 和输出层输出<script type="math/tex">o_k</script>如下。其中<script type="math/tex">net_j</script>为隐含层第<script type="math/tex">j</script>个神经元的输入,<script type="math/tex">u</script>为输入层和隐含层的连接权值矩阵，<script type="math/tex">v</script>为隐含层和输出层之间的连接权值矩阵.</p> <p> $$ \begin{align} o_j &amp; =f(net_j) \\ o_k &amp; =f(net_k) \\ net_j &amp; =\sum_i(x_{i}u_{i,j})+b_j \\ net_k &amp; =\sum_j(o_{j}v_{j,k})+b_k \end{align} $$ </p> <p>定义损失函数为<script type="math/tex">E_p=\frac{1}{2}\sum_k (o_k - d_k)^2</script> ,其中<script type="math/tex">p</script>为样本下标，<script type="math/tex">o^k</script>为第<script type="math/tex">k</script>个输出层神经元的输出,<script type="math/tex">d^k</script>为样本在第$k$个编码值。然后分别对参数<script type="math/tex">v_{j,k}</script>、<script type="math/tex">u_{i,j}</script> 进行求导，可得：</p> <p> $$ \begin{align} \frac{\partial E_p}{\partial v_{j,k}} &amp; = \frac{\partial E_p}{\partial net_k} \frac{\partial net_k}{\partial v_{j,k}} \\ &amp; = \frac{\partial E_p}{\partial net_k}o_j \\ &amp; = \frac{\partial E_p}{\partial o_k}\frac{\partial o_k}{\partial net_k}o_j \\ &amp; = (o_k-d_k)o_k(1-o_k)o_j \end{align} $$ </p> <p> $$ \begin{align} \frac{\partial E_p} {\partial u_{i,j}} &amp; = \frac{\partial E_p} {\partial net_j} \frac{\partial net_j} {\partial u_{i,j}} \\ &amp; =x_i \sum_k \frac{\partial E_p} {\partial net_k} \frac{\partial net_k}{\partial o_j} \frac{\partial o_j}{\partial net_j} \\ &amp; =x_i \sum_k \frac{\partial E_p}{\partial net_k} v_{j,k} o_j(1-o_j) \\ &amp; = x_i o_j(1-o_j) \sum_k \frac{\partial E_p}{\partial net_k} v_{j,k} \end{align} $$ </p> <p>从对$\frac{\partial E_p} {\partial u_{i,j}}$的推导可以得到反向传播的核心思路，令误差项$\beta_k = \frac{\partial E_p} {\partial net_k}$, 则有：</p> <p> $$ \beta_k=o_l(1-o_l)\sum_l\beta_lw_{lk} $$ </p> <p>反向传播的实质是基于梯度下降的优化方法，只不过在优化的过程使用了一种更为优雅的权值更新方式。</p> <h3 id="2-循环神经网络">2. 循环神经网络</h3> <p>传统的神经网络一般都是全连接结构，且非相邻两层之间是没有连接的。一般而言，定长输入的样本很容易通过神经网络来解决，但是类似于NLP中的序列标注这样的非定长输入，前向神经网络却无能为力。</p> <p>于是有人提出了循环神经网络(Recurrent Neural Network)，这是一种无法像前向神经网络一样有可以具象的网络结构的模型，一般认为是网络隐层节点之间有相互作用的连接，其实质可以认为是多个具有相同结构和参数的前向神经网络的stacking, 前向神经网络的数目和输入序列的长度一致，且序列中毗邻的元素对应的前向神经网络的隐层之间有互联结构，其图示( <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">图片来源</a> )如下. <img src="http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg" alt="" /></p> <p>上图只是一个比较抽象的结构，下面是一个以时间展开的更为具体的结构(<a href="http://www.cnblogs.com/YiXiaoZhou/p/6058890.html">图片来源</a>). <img src="http://images2015.cnblogs.com/blog/1027162/201611/1027162-20161113162111280-1753976877.png" alt="" /></p> <p>从图中可以看出，输出层神经元的输入输出和前向神经网络中没有什么差异，仅仅在于隐层除了要接收输入层的输入外，还需要接受来自于自身的输入(可以理解为t时刻的隐层需要接收来自于t-1时刻隐层的输入, 当然这仅限于单向RNN的情况，在双向RNN还需要接受来自t+1时刻的输入).</p> <p>RNN的隐层是控制信息传递的重要单元，不同时刻隐层之间的连接权值决定了过去时刻对当前时刻的影响，所以会存在时间跨度过大而导致这种影响会削弱甚至消失的现象，称之为梯度消失，改进一般都是针对隐层做文章，LSTM(控制输入量，补充新的信息然后输出)，GRU(更新信息然后输出)等都是这类的改进算法.</p> <p>下图为某时刻隐层单元的结构示意图(<a href="http://www.cnblogs.com/YiXiaoZhou/p/6058890.html">图片来源</a>).</p> <p><img src="http://images2015.cnblogs.com/blog/1027162/201611/1027162-20161113162105295-307972897.png" alt="" /></p> <p>虽说处理的是不定长输入数据，但是某个时刻的输入还是定长的。令t时刻:输入$x_t \in R^{xdim}$ ,t隐层输出<script type="math/tex">h_t\in R^{hdim}</script>, 输出层$y_t \in R^{ydim}$, RNN和CNN有着同样的共享权值的属性，输入层到隐层的权值矩阵$V\in R^{xdim\times hdim}$, 隐层到输出层的权值矩阵$W \in R^{hdim\times ydim}$, 不同时刻隐层自连接权值矩阵$U\in R^{hdim\times hdim}$[1]. RNN有着类似于CNN的权值共享特点，所以不同时刻的U,V,W都是相同的，所有整个网络的学习目标就是优化这些参数以及偏置. RNN和普通神经网络一样，也有超过三层的结构，下文的推导仅以三层为例.</p> <p>令隐含层的激励函数$f(x)$, 输出层的激励函数为$g(x)$. 则有：</p> <p> $$ \begin{align} h^t &amp; = f(net_h^t) \\ net_h^t &amp; = x^tV + h^{t-1}U + b_h \\ y^t &amp; = g(net_y^t) \\ net_y^t &amp; = h^tW + b^y \end{align} $$ </p> <p>对于单个样本，定义我们的cost function $E^t = \frac{1}{2}||d^t- y^t ||^2$，则对权值$W_{j,k}$、$V_{i,j}$、$U_{j,r}$的求导分别如下，其中$j$表示隐层单元下标,$k$表示输出层下标,$i$表示输入层下标,$r$表示下一时刻隐含层下标.$net^t_{hj}$表示t时刻隐层第j个神经元的加权输入，$net^t_{yk}$表示t时刻输出层第k个神经元的加权输入。</p> <p> $$ \begin{align} \frac{\partial E} {\partial W_{jk}} &amp; = \frac{\partial E} {\partial net^t_{yk}} \frac{\partial net^t_{yk}} {\partial W_{jk} } \\ &amp; = \frac{\partial E} {\partial net^t_{yk}} h^t_j \\ &amp; = (d_k^t - y_k^t)y^t_k(1-y^t_k)h_j^t \\ \frac{\partial E} {\partial V_{ij}} &amp; = \frac{\partial E} {\partial net^t_{hj}} \frac{\partial net^t_{hj}} {\partial V_{ij}} \\ &amp; = \frac{\partial E} {\partial net^t_{hj}} x^t_i \\ &amp; = (\sum_k \frac{\partial E}{\partial net^t_{yk}} \frac{\partial net^t_{yk}}{\partial h^t_j} \frac{\partial h^t_j}{\partial net^t_{hj}} + \color{red}{ \sum_r \frac{\partial E}{\partial net^{t+1}_{hr}} \frac{\partial net^{t+1}_{hr}}{\partial h^{t}_{j}} \frac{\partial h^{t}_{j}} {\partial net^t_{hj}} })x^t_i \\ &amp; = (\sum_k \frac{\partial E}{\partial net^t_{yk}} W_{jk} + \color{red} { \sum_r \frac{\partial E}{\partial net^{t+1}_{hr}}U_{jr}} ) \frac{\partial h^{t}_{j}} {\partial net^t_{hj}} x^t_i \\ \frac{\partial E} {\partial U_{jr}} &amp; = \frac{\partial E} {\partial net^{t+1}_{hr}} \frac{\partial net^{t+1}_{hr}}{\partial U_{jr}} \\ &amp; = \frac{\partial E}{\partial net^{t+1}_{hr}} h^t_j \\ &amp; = (\sum_k \frac{\partial E} {\partial net^{t+1}_{yk}} \frac {\partial net^{t+1}_{yk}} {\partial h^{t+1}_r} \frac {\partial h^{t+1}_r} {\partial net^{t+1}_{hr}} + \color{red}{\sum_j \frac{\partial E} {\partial net^{t+2}_{hj}} \frac {\partial net^{t+2}_{hj}} {\partial h^{t+1}_r} \frac {\partial h^{t+1}_r} {\partial net^{t+1}_{hr}}}) h^t_j \\ &amp; = (\sum_k \frac{\partial E} {\partial net^{t+1}_{yk}} W_{rk} + \color{red}{\sum_j \frac{\partial E} {\partial net^{t+2}_{hj}} U_{jr}}) \frac {\partial h^{t+1}_r} {\partial net^{t+1}_{hr}} h^t_j \\ \end{align} $$ </p> <p>令$\delta_{y,k}^t$为t时刻输出层y第k个神经元的误差项，令$\delta_{h,j}^t$为t时刻隐含层h第j个神经元的误差项, 则有:</p> <p> $$ \begin{align} \delta_{y,k}^t &amp; = \frac{\partial E} {\partial net_{yk}^t} \\ \delta_{h,j}^t &amp; = \frac{\partial E} {\partial net_{hj}^t} \\ \delta_{y,k}^t &amp; = (d_k^t - y_k^t)y^t_k(1-y^t_k) \\ \delta_{h,j}^t &amp; = (\sum_k \frac{\partial E}{\partial net^t_{yk}} W_{jk} + \sum_r \frac{\partial E}{\partial net^{t+1}_{hr}}U_{jr}) \frac{\partial h^{t}_{j}} {\partial net^t_{hj}} \\ &amp; = (\sum_k \delta_{y,k}^t W_{jk} + \sum_r \delta_{h,j}^{t+1} U_{jr}) \frac{\partial h^{t}_{j}} {\partial net^t_{hj}} \end{align} $$ </p> <p>上述 公式其实在了解反向传播之后就能够很容易推导，对于$W_{jk}$、$U_{jr}$的推导套用反向传播公式即可，而对于$V_{i,j}$需要加上来自于下一时刻的误差，如以上式子中红色部分所示. 对于LSTM，GRU的推导也类似，了解清楚误差传递的来源和去向，就很容易得到对当前参数的推导链式规则了.</p> <h3 id="参考资料">参考资料</h3> <p>[1]<a href="http://www.cnblogs.com/YiXiaoZhou/p/6058890.html">RNN求解过程推导与实现</a></p> </div> <div id='article_list_p st' style='float:right; width:250px; height:100%; position:absolute; right:10px; top:300px; font-size:12px;'> </div> </article> <div class="ds-thread" data-thread-key="/machinelearning/2016/12/02/RNN" data-title="循环神经网络的一些小事儿" data-url="//machinelearning/2016/12/02/RNN.html" style='width:766px;margin:0px auto'></div> <!-- 多说评论框 end --> <!-- 多说公共JS代码 start (一个网页只需插入一次) --> <script type="text/javascript"> var duoshuoQuery = {short_name:"aron"}; (function() { var ds = document.createElement('script'); ds.type = 'text/javascript';ds.async = true; ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js'; ds.charset = 'UTF-8'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds); })(); </script> <!-- 多说公共JS代码 end --> <div class="back"> <a href="/">Back</a> </div> </main> </body> </html>