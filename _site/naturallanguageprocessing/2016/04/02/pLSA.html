<!DOCTYPE html> <html lang="en"> <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1" /> <title>Probability Latent Semantic Analysis</title> <meta name="twitter:card" content="summary" /> <meta name="twitter:site" content="@jekyllrb" /> <meta name="twitter:title" content="Probability Latent Semantic Analysis" /> <meta name="twitter:description" content="pLSA是一种Topic Model，全称概率潜在语义分析，将文本的高维稀疏向量表示成低位维稠密向量。它是一种无监督学习方法，假设整个文档集合是由若干个主题组成，某一篇文章都以一定的概率(p(z|d))属于某一主题，在给定主题的情况下，每个词汇都以一定的概率(p(w|z))产生。pLSA是基于这样一个假设，假设文档和主题的分布以及主题和词汇的分布都是多项式分布，因此一个词汇的生成过程可以表示..."> <meta name="description" content="pLSA是一种Topic Model，全称概率潜在语义分析，将文本的高维稀疏向量表示成低位维稠密向量。它是一种无监督学习方法，假设整个文档集合是由若干个主题组成，某一篇文章都以一定的概率(p(z|d))属于某一主题，在给定主题的情况下，每个词汇都以一定的概率(p(w|z))产生。pLSA是基于这样一个假设，假设文..."> <link rel="icon" href="/assets/favicon.png"> <link rel="apple-touch-icon" href="/assets/touch-icon.png"> <link rel="stylesheet" href="//code.cdn.mozilla.net/fonts/fira.css"> <link rel="stylesheet" href="/assets/core.css"> <link rel="canonical" href="/naturallanguageprocessing/2016/04/02/pLSA.html"> <link rel="alternate" type="application/atom+xml" title="Aron's blog" href="/feed.xml" /> </head> <body> <aside class="logo"> <a href="/"> <img src="/public/image/avatar.jpg" class="gravatar"> </a> <span class="logo-prompt">Back to Home</span> </aside> <main> <noscript> <style> article .footnotes { display: block; } </style> </noscript> <article> <div class="center"> <h1>Probability Latent Semantic Analysis</h1> <time>April 2, 2016</time> </div> <div class="divider"></div> <p>pLSA是一种Topic Model，全称概率潜在语义分析，将文本的高维稀疏向量表示成低位维稠密向量。它是一种无监督学习方法，假设整个文档集合是由若干个主题组成，某一篇文章都以一定的概率(p(z|d))属于某一主题，在给定主题的情况下，每个词汇都以一定的概率(p(w|z))产生。 pLSA是基于这样一个假设，假设文档和主题的分布以及主题和词汇的分布都是多项式分布，因此一个词汇的生成过程可以表示为：  以一定的概率生成一篇文档  在该文档中按照文档主题分布选择一个主题  在该主题下面，按照主题-词汇分布生成一个词汇 可以得到如下模型：(p(w,d)=p(d)p(w|d);~~~~p(w|d)=\sum_{z=1}p(z|d)p(w|z))，其中z为主题，w为词，d为文档。其实这个过程也很容易理解，假设现在我们要写篇paper，在动笔之前，会列好提纲，大概选择几个点(也就是主题)去稀疏，在充实这些主题的过程中，我们会去选择和这个主题相关的词进行修饰，比如写到关于pLSA的文章的时候，我们会选择”建模、概率、主题模型、极大似然“等词去完善这个主题。在得到模型之后，我们就需要进行Model Inference，也就是参数估计了，pLSA作为概率学派的经典之一，一般是采用MLE作为inference的tool。 首先构造极大似然函数：(L(\theta) = \prod_{i=1}^N\prod_{j=1}^Np(d_i,w_j)^{n(d_i,w_j)})，其中(n(d_i,w_j))表示的是词(w_j)在和文档(d_i)的共现频率，由于我们(p(w|z))和(p(z|d))都是多项式分布，</p><pre><code>    接着，是参数估计，一般而言，此时可以选用极大似然估计。极大对数似然函数如下：
</code></pre><p>如果直接求导的话，很明显就得需要求解(n+m)个方程， 明显不可取。 可以采用EM算法，EM算法的基本思想，随机初始化相关参数。随后，在E步，计算潜在变量的后验概率。M步，利用潜在变量的后验概率去更新未知参数。 问题：为什么可以收敛？ 在e步：</p> <p>M步，最大化极大似然期望：</p> <p>又由于此时：</p> <p>所以：</p> <p>利用Jensen不等式，对于凸函数有：</p> <p>可以得到：</p> <p>此时即转化为，对于约束条件： ，求E’(L)的最大值。 很显然可以构造拉格朗日乘法进行求解：</p> <p>分别对 求导可得：</p> <p>联立方程组(1),(2),(3),(4)求解可得：</p> <p>因此，在M步更求新参数，通过不断迭代，如果求得的极大似然趋近收敛，则训练结束。 总结一下： E步：求当前估计的参数条件下的后验概率：p(z|w,d) M步：最大化complete data(加上隐藏变量主题)极大似然估计的期望 E步和M步循环迭代，直至收敛。 实现的源码见：plsa.cpp plsa.h ，由于切词用到了公司的lib库，所以切词的初始化和切词的过程在源码中被删去，可以去中科院nlp主页获取相关模块。</p> </article> <div class="back"> <a href="/">Back</a> </div> </main> </body> </html>