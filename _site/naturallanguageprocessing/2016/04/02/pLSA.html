<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1" /> <title>Probability Latent Semantic Analysis</title> <meta name="twitter:card" content="summary" /> <meta name="twitter:site" content="@kymo" /> <meta name="twitter:title" content="Probability Latent Semantic Analysis" /> <meta name="twitter:description" content="pLSA是一种主题模型(Topic Model)，全称概率潜在语义分析，是一种将文本的高维稀疏向量表示成低位维稠密向量的映射方法。它是一种无监督学习方法，假设整个文档集合是由若干个主题组成，某一篇文章都以一定的概率属于某一主题;在给定主题的情况下，以的概率产生文档的词。pLSA是基于这样一个假设，假设文档和主题的分布以及主题和词汇的分布都是多项式分布，因此一个词汇的生成过程可以表示为："> <meta name="description" content="pLSA是一种主题模型(Topic Model)，全称概率潜在语义分析，是一种将文本的高维稀疏向量表示成低位维稠密向量的映射方法。它是一种无监督学习方法，假设整个文档集合是由若干个主题组成，某一篇文章都以一定的概率属于某一主题;在给定主题的情况下，以的概率产生文档的词。pLSA是基于这样一个假设，假设文档和主题的..."> <link rel="icon" href="/assets/favicon.png"> <link rel="apple-touch-icon" href="/assets/touch-icon.png"> <link rel="stylesheet" href="//code.cdn.mozilla.net/fonts/fira.css"> <link rel="stylesheet" href="/assets/core.css"> <link rel="canonical" href="/naturallanguageprocessing/2016/04/02/pLSA.html"> <link rel="alternate" type="application/atom+xml" title="Aron's blog" href="/feed.xml" /> <!-- mathjax config similar to math.stackexchange --> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ extensions: ["tex2jax.js"], jax: ["input/TeX", "output/HTML-CSS"], tex2jax: { inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$']], processEscapes: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }, messageStyle: "none", "HTML-CSS": { preferredFont: "TeX", availableFonts: ["TeX"] } }); </script> <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> </head> <body> <aside class="logo"> <a href="/"> <img src="/public/image/avatar.jpg" class="gravatar"> </a> <span class="logo-prompt">Back to Home</span> </aside> <main> <noscript> <style> article .footnotes { display: block; } </style> </noscript> <article> <div style='float:left;'> <div class="center"> <h1>Probability Latent Semantic Analysis</h1> <time>April 2, 2016</time> </div> <div class="divider"></div> <p>pLSA是一种主题模型(Topic Model)，全称概率潜在语义分析，是一种将文本的高维稀疏向量表示成低位维稠密向量的映射方法。 它是一种无监督学习方法，假设整个文档集合是由若干个主题组成，某一篇文章都以一定的概率<script type="math/tex">p(z \mid d)</script>属于某一主题;在给定主题的情况下，以<script type="math/tex">p(w \mid z)</script>的概率产生文档的词。 pLSA是基于这样一个假设，假设文档和主题的分布以及主题和词汇的分布都是多项式分布，因此一个词汇的生成过程可以表示为：</p> <ul> <li>以一定的概率生成一篇文档</li> <li>在该文档中按照文档主题分布选择一个主题</li> <li>在该主题下面，按照主题-词汇分布生成一个词汇</li> </ul> <p>可以得到如下模型<script type="math/tex">p(w,d)=p(d)p(w \mid d); p(w \mid d)=\sum_{z=1}p(z \mid d)p(w \mid z)</script>，其中z为主题，w为词，d为文档。 其实这个过程也很容易理解，假设现在我们要写篇paper，在动笔之前，会列好提纲，大概选择几个点(也就是主题)。 在完善这些主题的过程中，我们会在积累的词典中去选择和这个主题相适应的词进行修饰。 比如写到关于pLSA的文章的时候，我们会选择”建模、概率、主题模型、极大似然“等词去完善这个主题。 在得到模型之后，我们就需要进行Model Inference，也就是参数估计了，pLSA作为概率图模型，一般是采用MLE作为inference的tool。 首先构造极大似然函数：<script type="math/tex">L(\theta) = \prod_{i=1}^N\prod_{j=1}^Np(d_i,w_j)^{n(d_i,w_j)}</script>，其中<script type="math/tex">n(d_i,w_j)</script>表示的是词<script type="math/tex">w_j</script>在和文档<script type="math/tex">d_i</script>的共现频率，由于我们<script type="math/tex">p(w \mid z)</script>和<script type="math/tex">p(z \mid d)</script>都是多项式分布，<br /> 接着，是参数估计，一般而言，此时可以选用极大似然估计。极大对数似然函数如下：</p> <script type="math/tex; mode=display">L(\theta) = \prod_{i=1}^N\prod_{j=1}^Np(d_i,w_j)^{n(d_i,w_j)}</script> <script type="math/tex; mode=display">ln(L(\theta)) = \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)logp(d_i,w_j) \\ = \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)log(p(d_i)\sum_{k=1}^Kp(z_k \mid d_i)p(w_j \mid z_k)) \\ = \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)log(p(d_i))+\sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)log(\sum_{k=1}^Kp(z_k\mid d_i)p(w_j \mid z_k))</script> <p>如果直接求导的话，很明显就得需要求解(n+m)个方程，几乎是不可能完成的任务。 一般采用最大期望算法(Maximum Expection， ME)，EM算法的基本思想很直观，即随机初始化相关参数；随后，在E步，计算潜在变量的后验概率；M步， 利用潜在变量的后验概率去更新未知参数。循环迭代直至达到最大迭代次数算法退出。</p> <h3 id="section">问题：为什么可以收敛？</h3> <p>在e步</p> <script type="math/tex; mode=display">p(z_k \mid w_j, d_i) = \frac{p(z_k \mid w_j)p(w_j \mid z_k)} {\sum_{k=1}^Kp(z_k \mid w_j)p(w_j\mid z_k)}</script> <p>M步，最大化极大似然</p> <script type="math/tex; mode=display">L(\theta) = \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)log(\sum_{k=1}^Kp(z_k\mid d_i)p(w_j \mid z_k))</script> <p>又由于此时：</p> <script type="math/tex; mode=display">\sum_{k=1}^K p(z_k|d_i)p(w_j|z_k) > \sum_{k=1}^K p(z_k \mid d_i) p(z_k \mid d_i )p(w_j\mid z_k); \sum_{k=1}^Kp(z_k \mid d_i,w_j) = 1</script> <p>所以：</p> <script type="math/tex; mode=display">L(\theta) > \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)log( \sum_{k=1}^K p(z_k \mid d_i) p(z_k \mid d_i )p(w_j\mid z_k) )</script> <p>利用Jensen不等式，对于凸函数<script type="math/tex">f(x)</script>有：<script type="math/tex">f(\sum_{i=1}^Nw_ix_i)>=\sum_{i=1}^Nw_if(x_i)</script>可以得到：</p> <script type="math/tex; mode=display">L(\theta) > \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)\sum_{k=1}^K p(z_k \mid d_i) log(p(z_k \mid d_i )p(w_j\mid z_k) ) = L'(\theta)</script> <p>此时即转化为，对于约束条件：<script type="math/tex">\sum_{k=1}^Kp(z_k|d_i)=1[1],\sum_{j=1}^Mp(w_j|z_k)=1[2]</script> ，求<script type="math/tex">L'(\theta)</script>的最大值。 对于这种带等式约束的优化问题，很显然可以构造拉格朗日乘法进行求解：</p> <script type="math/tex; mode=display">H = L'(\theta) + \sum_{i=1}^N\alpha_i(\sum_{k=1}^Kp(z_k|d_i)-1)+\sum_{k=1}^K\beta_k(\sum_{j=1}^Mp(w_j|z_k)-1)</script> <p>分别对 <script type="math/tex">p(z_k\mid d_i),p(w_j\mid z_k)</script>求导可得：</p> <script type="math/tex; mode=display">=\sum_{j=1}^Mn(d_i,w_j)p(z_k|d_i,w_j)-\alpha_ip(z_k|d_i) = 0 [3] \\ = \sum_{i=1}^Nn(d_i,w_j)p(z_k|d_i,w_j)-\beta_kp(w_j|z_k) = 0 [4]</script> <p>联立方程组[1],[2],[3],[4]求解可得：</p> <script type="math/tex; mode=display">p(z_k|d_i) = \frac {\sum_{i=1}^N n(d_i,w_j)p(z_k|d_i,w_j)}{\sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)p(z_k|d_i,w_j)}</script> <script type="math/tex; mode=display">p(w_j|z_k) = \frac {\sum_{j=1}^Mn(d_i,w_j)p(z_k|d_i,w_j)}{\sum_{j=1}^M\sum_{k=1}^Kn(d_i,w_j)p(z_k|d_i,w_j)} \\ = \frac{\sum_{j=1}^Mn(d_i,w_j)p(z_k|d_i,w_j)}{n(d_i)}</script> <p>因此，在M步更求新参数，通过不断迭代，如果求得的极大似然趋近收敛，则训练结束。</p> <p>总结一下：</p> <ul> <li>E步：求当前估计的参数条件下的后验概率：p(z \mid w,d)</li> <li>M步：最大化complete data(加上隐藏变量主题)极大似然估计的期望</li> <li>E步和M步循环迭代，直至收敛。</li> </ul> <p>实现的源码见：plsa.cpp plsa.h ，由于切词用到了公司的lib库，所以切词的初始化和切词的过程在源码中被删去，可以去中科院nlp主页获取相关模块。</p> </div> <div id='article_list_p st' style='float:right; width:250px; height:100%; position:absolute; right:10px; top:300px; font-size:12px;'> <h2>文章列表</h2></hr> <a href="/machinelearning/2016/12/02/RNN.html">RNN</a></br> <a href="/2016/09/20/%E8%BD%BB%E9%87%8F%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%9E%B6%E6%9E%84.html">轻量应用服务器架构</a></br> <a href="/machinelearning/2016/08/16/CRF.html">CRF 笔记</a></br> <a href="/machinelearning/2016/08/16/PaperReadingNote.html">论文阅读笔记</a></br> <a href="/machinelearning/2016/04/03/EnsembleLearningMethods.html">Ensemble Learning Methods</a></br> <a href="/naturallanguageprocessing/2016/04/02/pLSA.html">Probability Latent Semantic Analysis</a></br> <a href="/machinelearning/2016/04/02/SVM.html">Support Vector Machine</a></br> <a href="/machinelearning/2016/04/02/SVD.html">SVD</a></br> <a href="/machinelearning/2016/04/02/RandomForests.html">Random Forest</a></br> <a href="/machinelearning/2016/04/02/HMM.html">HMM</a></br> <a href="/machinelearning/2016/04/02/ESL_Notes.html">ESL Notes</a></br> <a href="/algorithm/2016/04/02/DP.html">Dynamic Programming</a></br> <a href="/algorithm/2016/04/02/Bit.html">Bit Manupulation</a></br> </div> </article> <div class="ds-thread" data-thread-key="/naturallanguageprocessing/2016/04/02/pLSA" data-title="Probability Latent Semantic Analysis" data-url="//naturallanguageprocessing/2016/04/02/pLSA.html" style='width:666px;margin:0px auto'></div> <!-- 多说评论框 end --> <!-- 多说公共JS代码 start (一个网页只需插入一次) --> <script type="text/javascript"> var duoshuoQuery = {short_name:"aron"}; (function() { var ds = document.createElement('script'); ds.type = 'text/javascript';ds.async = true; ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js'; ds.charset = 'UTF-8'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds); })(); </script> <!-- 多说公共JS代码 end --> <div class="back"> <a href="/">Back</a> </div> </main> </body> </html>