<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aron blog</title>
    <description>nihao
</description>
    <link>http://kymo.github.io/</link>
    <atom:link href="http://kymo.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 24 Apr 2017 23:35:17 +0800</pubDate>
    <lastBuildDate>Mon, 24 Apr 2017 23:35:17 +0800</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>标注的一些小事儿</title>
        <description>&lt;p&gt;标注是个很笼统但是应用面很广的概念，词性标注、实体识别、分词、新词发现、专名识别等都属于标注这一NLP细分方向之中，各大互联网公司在该技术方向上的投入和积淀甚多。当然，经过那么多年的发展，这一方向已经有了较为成熟和稳定的解决方案. 传统的方法以HMM和CRF为首，通过将标注问题抽象为图模型，然后利用马尔可夫性质或者&lt;/p&gt;
</description>
        <pubDate>Thu, 02 Feb 2017 01:11:03 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2017/02/02/%E6%A0%87%E6%B3%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E4%BA%8B%E5%84%BF.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2017/02/02/%E6%A0%87%E6%B3%A8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E4%BA%8B%E5%84%BF.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>循环神经网络的一些小事儿</title>
        <description>&lt;h3 id=&quot;1-从神经网络谈起&quot;&gt;1. 从神经网络谈起&lt;/h3&gt;

&lt;p&gt;了解神经网络的都知道，神经网络作为一种非线性模型，在监督学习领域取得了state-of-art的效果，其中反向传播算法的提出居功至伟，到如今仍然是主流的优化神经网络参数的算法. 递归神经网络、卷积神经网络以及深度神经网络作为人工神经网络的”变种”，仍然延续了ANN的诸多特质，如权值连接，激励函数，以神经元为计算单元等，只不过因为应用场景的不同衍生了不同的特性，如：处理变长数据、权值共享等。&lt;/p&gt;

&lt;p&gt;为了介绍RNN，先简单的介绍ANN. ANN的结构很容易理解，一般是三层结构（输入层-隐含层-输出层）. 隐含层输出&lt;script type=&quot;math/tex&quot;&gt;o_j&lt;/script&gt; 和输出层输出&lt;script type=&quot;math/tex&quot;&gt;o_k&lt;/script&gt;如下。其中&lt;script type=&quot;math/tex&quot;&gt;net_j&lt;/script&gt;为隐含层第&lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;个神经元的输入,&lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt;为输入层和隐含层的连接权值矩阵，&lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;为隐含层和输出层之间的连接权值矩阵.&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
o_j &amp;amp; =f(net_j) \\
o_k &amp;amp; =f(net_k) \\ 
net_j &amp;amp; =\sum_i(x_{i}u_{i,j})+b_j \\
net_k &amp;amp; =\sum_j(o_{j}v_{j,k})+b_k
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;定义损失函数为&lt;script type=&quot;math/tex&quot;&gt;E_p=\frac{1}{2}\sum_k (o_k - d_k)^2&lt;/script&gt; ,其中&lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;为样本下标，&lt;script type=&quot;math/tex&quot;&gt;o^k&lt;/script&gt;为第&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;个输出层神经元的输出,&lt;script type=&quot;math/tex&quot;&gt;d^k&lt;/script&gt;为样本在第$k$个编码值。然后分别对参数&lt;script type=&quot;math/tex&quot;&gt;v_{j,k}&lt;/script&gt;、&lt;script type=&quot;math/tex&quot;&gt;u_{i,j}&lt;/script&gt; 进行求导，可得：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
\frac{\partial E_p}{\partial v_{j,k}} &amp;amp; = \frac{\partial E_p}{\partial net_k} \frac{\partial net_k}{\partial v_{j,k}} \\
&amp;amp; = \frac{\partial E_p}{\partial net_k}o_j \\
&amp;amp; = \frac{\partial E_p}{\partial o_k}\frac{\partial o_k}{\partial net_k}o_j \\
&amp;amp; = (o_k-d_k)o_k(1-o_k)o_j
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;
$$
\begin{align}
\frac{\partial E_p} {\partial u_{i,j}} &amp;amp; = \frac{\partial E_p} {\partial net_j} \frac{\partial net_j} {\partial u_{i,j}} \\
&amp;amp; =x_i \sum_k \frac{\partial E_p} {\partial net_k} \frac{\partial net_k}{\partial o_j} \frac{\partial o_j}{\partial net_j}  \\
&amp;amp; =x_i \sum_k \frac{\partial E_p}{\partial net_k} v_{j,k} o_j(1-o_j) \\
&amp;amp; = x_i o_j(1-o_j) \sum_k \frac{\partial E_p}{\partial net_k} v_{j,k} 
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;从对$\frac{\partial E_p} {\partial u_{i,j}}$的推导可以得到反向传播的核心思路，令误差项$\beta_k = \frac{\partial E_p} {\partial net_k}$, 则有：&lt;/p&gt;

&lt;p&gt;
$$
\beta_k=o_l(1-o_l)\sum_l\beta_lw_{lk}
$$
&lt;/p&gt;

&lt;p&gt;反向传播的实质是基于梯度下降的优化方法，只不过在优化的过程使用了一种更为优雅的权值更新方式。&lt;/p&gt;

&lt;h3 id=&quot;2-循环神经网络&quot;&gt;2. 循环神经网络&lt;/h3&gt;

&lt;p&gt;传统的神经网络一般都是全连接结构，且非相邻两层之间是没有连接的。一般而言，定长输入的样本很容易通过神经网络来解决，但是类似于NLP中的序列标注这样的非定长输入，前向神经网络却无能为力。&lt;/p&gt;

&lt;p&gt;于是有人提出了循环神经网络(Recurrent Neural Network)，这是一种无法像前向神经网络一样有可以具象的网络结构的模型，一般认为是网络隐层节点之间有相互作用的连接，其实质可以认为是多个具有相同结构和参数的前向神经网络的stacking, 前向神经网络的数目和输入序列的长度一致，且序列中毗邻的元素对应的前向神经网络的隐层之间有互联结构，其图示( &lt;a href=&quot;http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/&quot;&gt;图片来源&lt;/a&gt; )如下.
&lt;img src=&quot;http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图只是一个比较抽象的结构，下面是一个以时间展开的更为具体的结构(&lt;a href=&quot;http://www.cnblogs.com/YiXiaoZhou/p/6058890.html&quot;&gt;图片来源&lt;/a&gt;).
&lt;img src=&quot;http://images2015.cnblogs.com/blog/1027162/201611/1027162-20161113162111280-1753976877.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从图中可以看出，输出层神经元的输入输出和前向神经网络中没有什么差异，仅仅在于隐层除了要接收输入层的输入外，还需要接受来自于自身的输入(可以理解为t时刻的隐层需要接收来自于t-1时刻隐层的输入, 当然这仅限于单向RNN的情况，在双向RNN还需要接受来自t+1时刻的输入).&lt;/p&gt;

&lt;p&gt;RNN的隐层是控制信息传递的重要单元，不同时刻隐层之间的连接权值决定了过去时刻对当前时刻的影响，所以会存在时间跨度过大而导致这种影响会削弱甚至消失的现象，称之为梯度消失，改进一般都是针对隐层做文章，LSTM(控制输入量，补充新的信息然后输出)，GRU(更新信息然后输出)等都是这类的改进算法.&lt;/p&gt;

&lt;p&gt;下图为某时刻隐层单元的结构示意图(&lt;a href=&quot;http://www.cnblogs.com/YiXiaoZhou/p/6058890.html&quot;&gt;图片来源&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://images2015.cnblogs.com/blog/1027162/201611/1027162-20161113162105295-307972897.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;虽说处理的是不定长输入数据，但是某个时刻的输入还是定长的。令t时刻:输入$x_t \in R^{xdim}$ ,t隐层输出&lt;script type=&quot;math/tex&quot;&gt;h_t\in R^{hdim}&lt;/script&gt;, 输出层$y_t \in R^{ydim}$, RNN和CNN有着同样的共享权值的属性，输入层到隐层的权值矩阵$V\in R^{xdim\times hdim}$, 隐层到输出层的权值矩阵$W \in R^{hdim\times ydim}$, 不同时刻隐层自连接权值矩阵$U\in R^{hdim\times hdim}$[1]. RNN有着类似于CNN的权值共享特点，所以不同时刻的U,V,W都是相同的，所有整个网络的学习目标就是优化这些参数以及偏置. RNN和普通神经网络一样，也有超过三层的结构，下文的推导仅以三层为例.&lt;/p&gt;

&lt;p&gt;令隐含层的激励函数$f(x)$, 输出层的激励函数为$g(x)$. 则有：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
h^t &amp;amp; = f(net_h^t) \\
net_h^t &amp;amp; = x^tV + h^{t-1}U + b_h \\
y^t &amp;amp; = g(net_y^t) \\
net_y^t &amp;amp; = h^tW + b^y
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;对于单个样本，定义我们的cost function $E^t = \frac{1}{2}||d^t- y^t ||^2$，则对权值$W_{j,k}$、$V_{i,j}$、$U_{j,r}$的求导分别如下，其中$j$表示隐层单元下标,$k$表示输出层下标,$i$表示输入层下标,$r$表示下一时刻隐含层下标.$net^t_{hj}$表示t时刻隐层第j个神经元的加权输入，$net^t_{yk}$表示t时刻输出层第k个神经元的加权输入。&lt;/p&gt;

&lt;p&gt;
$$
\begin{align}
\frac{\partial E} {\partial W_{jk}} &amp;amp; = \frac{\partial E} {\partial net^t_{yk}} \frac{\partial net^t_{yk}} {\partial W_{jk} } \\
&amp;amp; =  \frac{\partial E} {\partial net^t_{yk}} h^t_j \\
&amp;amp; = (d_k^t - y_k^t)y^t_k(1-y^t_k)h_j^t \\

\frac{\partial E} {\partial V_{ij}} &amp;amp; = \frac{\partial E} {\partial net^t_{hj}} \frac{\partial net^t_{hj}} {\partial V_{ij}} \\
&amp;amp; = \frac{\partial E} {\partial net^t_{hj}} x^t_i \\
&amp;amp; = (\sum_k \frac{\partial E}{\partial net^t_{yk}} \frac{\partial net^t_{yk}}{\partial h^t_j} \frac{\partial h^t_j}{\partial net^t_{hj}} +
\color{red}{ \sum_r \frac{\partial E}{\partial net^{t+1}_{hr}} \frac{\partial net^{t+1}_{hr}}{\partial h^{t}_{j}} \frac{\partial h^{t}_{j}} {\partial net^t_{hj}} })x^t_i \\ 
&amp;amp; = (\sum_k \frac{\partial E}{\partial net^t_{yk}} W_{jk} + \color{red} { \sum_r \frac{\partial E}{\partial net^{t+1}_{hr}}U_{jr}} ) \frac{\partial h^{t}_{j}} {\partial net^t_{hj}} x^t_i \\

\frac{\partial E} {\partial U_{jr}} &amp;amp; =  \frac{\partial E} {\partial net^{t+1}_{hr}} \frac{\partial net^{t+1}_{hr}}{\partial U_{jr}} \\
&amp;amp; = \frac{\partial E}{\partial net^{t+1}_{hr}} h^t_j \\
&amp;amp; = (\sum_k \frac{\partial E} {\partial net^{t+1}_{yk}} \frac {\partial net^{t+1}_{yk}} {\partial h^{t+1}_r}  \frac {\partial h^{t+1}_r} {\partial net^{t+1}_{hr}} + \color{red}{\sum_j \frac{\partial E} {\partial net^{t+2}_{hj}} \frac {\partial net^{t+2}_{hj}} {\partial h^{t+1}_r}  \frac {\partial h^{t+1}_r} {\partial net^{t+1}_{hr}}})  h^t_j \\
&amp;amp; = (\sum_k \frac{\partial E} {\partial net^{t+1}_{yk}} W_{rk} + \color{red}{\sum_j \frac{\partial E} {\partial net^{t+2}_{hj}} U_{jr}})  \frac {\partial h^{t+1}_r} {\partial net^{t+1}_{hr}} h^t_j \\
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;令$\delta_{y,k}^t$为t时刻输出层y第k个神经元的误差项，令$\delta_{h,j}^t$为t时刻隐含层h第j个神经元的误差项, 则有:&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
\delta_{y,k}^t &amp;amp; = \frac{\partial E} {\partial net_{yk}^t} \\
\delta_{h,j}^t &amp;amp; = \frac{\partial E} {\partial net_{hj}^t} \\
\delta_{y,k}^t &amp;amp; = (d_k^t - y_k^t)y^t_k(1-y^t_k) \\
\delta_{h,j}^t &amp;amp; = (\sum_k \frac{\partial E}{\partial net^t_{yk}} W_{jk} + \sum_r \frac{\partial E}{\partial net^{t+1}_{hr}}U_{jr}) \frac{\partial h^{t}_{j}} {\partial net^t_{hj}} \\
&amp;amp; = (\sum_k \delta_{y,k}^t W_{jk} + \sum_r \delta_{h,j}^{t+1} U_{jr}) \frac{\partial h^{t}_{j}} {\partial net^t_{hj}} 
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;上述 公式其实在了解反向传播之后就能够很容易推导，对于$W_{jk}$、$U_{jr}$的推导套用反向传播公式即可，而对于$V_{i,j}$需要加上来自于下一时刻的误差，如以上式子中红色部分所示. 对于LSTM，GRU的推导也类似，了解清楚误差传递的来源和去向，就很容易得到对当前参数的推导链式规则了.&lt;/p&gt;

&lt;h3 id=&quot;参考资料&quot;&gt;参考资料&lt;/h3&gt;
&lt;p&gt;[1]&lt;a href=&quot;http://www.cnblogs.com/YiXiaoZhou/p/6058890.html&quot;&gt;RNN求解过程推导与实现&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Fri, 02 Dec 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/12/02/RNN.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/12/02/RNN.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>Cv</title>
        <description>
&lt;p&gt;% LaTeX file for resume 
% This file uses the resume document class (res.cls)&lt;/p&gt;

&lt;p&gt;\documentclass[margin]{res} 
% the margin option causes section titles to appear to the left of body text 
\textwidth=5.2in % increase textwidth to get smaller right margin
%\usepackage{helvetica} % uses helvetica postscript font (download helvetica.sty)
%\usepackage{newcent}   % uses new century schoolbook postscript font&lt;/p&gt;

&lt;p&gt;\begin{document}&lt;/p&gt;

&lt;p&gt;\name{王晓峰  \[14pt] } % the \[12pt] adds a blank line after name&lt;/p&gt;

&lt;p&gt;\address{\ (+86)15071013512 \ kymowind@gmail.com }
\address{\ github.com/kymo \ www.idiotaron.org }&lt;/p&gt;

&lt;p&gt;\begin{resume}&lt;/p&gt;

&lt;p&gt;\section{Objective} 
算法工程师&lt;/p&gt;

&lt;p&gt;\section{Education} 
M.S. in Software Engineering, WuHan University \hfill 2013.09-2016.07
\begin{itemize} 
\item Concentrations on Machine Learning \&amp;amp; Optimization Algorithms
\end{itemize} 
B.S. in Computer Science \&amp;amp; Technology, WuHan Univerity \hfill 2009.09-2013.07
\begin{itemize} \itemsep -2pt 
\item Excellent Student Scholarship 2010\&amp;amp; 2011\&amp;amp; 2012(top 10\%)
\end{itemize}&lt;/p&gt;

&lt;p&gt;\section{Internship}&lt;/p&gt;

&lt;p&gt;{\bf Algorithm Engineer,} Alibaba Inc, Hangzhou \hfill  2015.04-2015.06&lt;/p&gt;

&lt;p&gt;\begin{itemize} \itemsep -2pt % reduce space between items
\item Query Recommendation for search drop down list, already on A/B Test 
\item Query Quality Model Investigation
\end{itemize}&lt;/p&gt;

&lt;p&gt;{\bf Algorithm Engineer,} Baidu Inc, Beijing \hfill 2014.07-2014.11
 \begin{itemize} \itemsep -2pt  % reduce space between items
 \item Created a new searching strategy by mining ngram pattern from online query set (good Performance on A/B Test)
 \item Maintained Text Miner module and added pLSA model into it
 \end{itemize}&lt;/p&gt;

&lt;p&gt;{\bf Web Backend Engineer,} Jiaming Inc, Wuhan \hfill 2012.09-2012.12
 \begin{itemize} \itemsep -2pt  % reduce space between items
 \item Developed the information stream system and user management system
 \item Maintain the web server and help design the Ajax interface
 \end{itemize}&lt;/p&gt;

&lt;p&gt;\section{Project}&lt;/p&gt;

&lt;p&gt;{\bf SUML,} Machine Learning Library written in C++  \hfill 2015
\begin{itemize} 
\item Implemented SVM, Random Forest, GBDT, Logistic Regression and Back Propagation Neural Network and the multi-thread version of GBDT
\end{itemize}&lt;/p&gt;

&lt;p&gt;{\bf HMMSEG,} Chinese Word Segmentation Module written in C++  \hfill 2014 
\begin{itemize} \itemsep -2pt
\item Stored the dictionary with Trie, and process the maximum match algorithm among the tree and then use HMM Model to choose the best one
\item Implemented the whole project and good performance on long text
\end{itemize}&lt;/p&gt;

&lt;p&gt;{\bf QiShu,} A Novel Searching Engine written in Python \hfill 2013
\begin{itemize} \itemsep -2pt
\item Leveraged Django, MySQL, Bootstrap and Memcached
\item Designed and implemented Searching Engine ,Crawler Engine, Back-End logic and the Front-End page
\end{itemize}&lt;/p&gt;

&lt;p&gt;{\bf GeBi,} A Social Network Website written in Python \hfill 2012
\begin{itemize} \itemsep -2pt
\item Leveraged Django, MySQL, Bootstrap and Memcached
\item Help designed and implemented the Back-End logic
\end{itemize}&lt;/p&gt;

&lt;p&gt;\section{Publish}
GPU-based Neural Network learning algorithm based on improved Differential Evolution with Elite Strategy&lt;/p&gt;

&lt;p&gt;% Tabulate Computer Skills; p{3in} defines paragraph 3 inches wide
\section{Skills}&lt;/p&gt;

&lt;p&gt;Familiar with Python \&amp;amp; C++ \&amp;amp; Linux Shell, Map-Reduce, CUDA and SQL &lt;br /&gt;
Familiar with basic data mining algorithm, and good sense of feature engineering &lt;br /&gt;
Familiar with Data Structure \&amp;amp; Multi-thread Programming \&amp;amp; Network Programming&lt;/p&gt;

&lt;p&gt;\end{resume}&lt;/p&gt;

&lt;p&gt;\end{document}&lt;/p&gt;

</description>
        <pubDate>Mon, 12 Sep 2016 00:00:00 +0800</pubDate>
        <link>http://kymo.github.io/2016/09/12/cv.html</link>
        <guid isPermaLink="true">http://kymo.github.io/2016/09/12/cv.html</guid>
        
        
      </item>
    
      <item>
        <title>CRF 笔记</title>
        <description>&lt;h1 id=&quot;crf-笔记&quot;&gt;CRF 笔记&lt;/h1&gt;

&lt;h5 id=&quot;dgm转换成ugm&quot;&gt;DGM转换成UGM&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;节点之间连接&lt;/li&gt;
  &lt;li&gt;节点的双亲连接，有节点都指向同一个节点，说明两者之间有关联&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;两者之间还是有关联的&lt;/p&gt;

</description>
        <pubDate>Tue, 16 Aug 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/08/16/CRF.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/08/16/CRF.html</guid>
        
        <category>CRF</category>
        
        <category>NLP</category>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>论文阅读笔记</title>
        <description>&lt;h3 id=&quot;learning-n-best-correction-models-from-implicit-user-feedback-in-a-muti-modal-local-search-application&quot;&gt;Learning N-best Correction Models from Implicit User Feedback in a Muti-Modal Local Search Application&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;通过用户的反馈对语音识别的结果进行纠错，通过点击数据构造result confusion matrix,然后利用该矩阵对结果进行重排&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通常而言，用户的点击数据可以用来对已有的声学模型或者语言模型进行概率，当然也可以利用这些数据对语言识别的结果进行纠正，本文作者想要构建的就是基于用户数据的纠错模型。&lt;/p&gt;

&lt;p&gt;本文的工作和主流的改写流程差不多，主要包含了两个方面：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;语音识别系统首先会给出n-best候选，不同于一般的候选，这里会增加一部分通过点击数据统计出来的额外的候选。&lt;/li&gt;
  &lt;li&gt;接着进行ReScore，构建更加精确的n-best list
对于第一步，需要从user click data中统计出result confusion matrix,该矩阵揭示了当展现某一个query的时候，用户点击其他query的频次，比如对于识别系统识别出来的n-best list:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;+++ Sterling  Stirling  Burlington  Cooling&lt;/p&gt;

&lt;p&gt;通过点击数据可以构造如下的result confusion matrix:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Bar&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Bowling&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Burgerking&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;…&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Burlington&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Sterling&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Burlington&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;…&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cooling&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;…&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Sterling&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;…&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Stirling&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;…&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;得到这个result confusion matrix之后，就可以将原有的n-best list进行扩充(在矩阵中共现过便可加入），得到如下结果：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Sterling  Striling  Burlington  Cooling  Bar  Bowling   Burgetking&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对于第二步，需要利用矩阵的统计频次.$Score(word_i)=\sum_{i=0}C(word_i, otherword)$，其中$word_i$表示第$i$个n-best list的元素，$otherword$表示用户在展现$word_i$之后点击的$word$.&lt;/p&gt;
</description>
        <pubDate>Tue, 16 Aug 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/08/16/PaperReadingNote.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/08/16/PaperReadingNote.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>sub: 轻量应用服务器</title>
        <description>&lt;p&gt;sub是正在开发的一种轻量的应用服务器, 项目地址: &lt;a href=&quot;https://github.com/kymo/sub&quot;&gt;sub&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;sub&quot;&gt;SUB&lt;/h1&gt;

&lt;h2 id=&quot;socket&quot;&gt;SOCKET&lt;/h2&gt;

&lt;h3 id=&quot;设置为非阻塞模式可以设置selecteoll-io模型将&quot;&gt;设置为非阻塞模式，可以设置SELECT/EOLL IO模型.将&lt;/h3&gt;
&lt;p&gt;Read, Write, Accept作为Reactor函数传递给IO模型&lt;/p&gt;

&lt;h3 id=&quot;有新的连接发过来数据利用解析任务解析该任务然&quot;&gt;有新的连接发过来数据，利用解析任务解析该任务，然&lt;/h3&gt;
&lt;p&gt;后将解析出来的数据按照类型生成新的对应任务丢到队
列中. 任务处理完成之后，设置该FD为写，并将写的容
一并返回.&lt;/p&gt;

&lt;h2 id=&quot;io&quot;&gt;IO&lt;/h2&gt;

&lt;h3 id=&quot;select&quot;&gt;Select&lt;/h3&gt;

&lt;h4 id=&quot;使用read_set和write_set处理连接和读写并接受&quot;&gt;使用read_set和write_set处理连接和读写，并接受&lt;/h4&gt;
&lt;p&gt;server 传递过来的回调函数指针作为事件处理方法&lt;/p&gt;

&lt;h3 id=&quot;epoll&quot;&gt;Epoll&lt;/h3&gt;

&lt;h4 id=&quot;todo&quot;&gt;TODO&lt;/h4&gt;

&lt;h2 id=&quot;taskmgr&quot;&gt;TaskMgr&lt;/h2&gt;

&lt;h3 id=&quot;task&quot;&gt;task&lt;/h3&gt;

&lt;h4 id=&quot;对任务的封装单个任务会有自己运行的回掉函数以&quot;&gt;对任务的封装，单个任务会有自己运行的回掉函数，以&lt;/h4&gt;
&lt;p&gt;及运行时的参数空间,运行run完成之后，这个run会将
数据写回缓冲区. 然后调用call_back函数，将结果写
会fd,不同的任务会继承该task, 计算任务可以由用户指
定运行的回调函数.&lt;/p&gt;

&lt;h3 id=&quot;task_handler&quot;&gt;task_handler&lt;/h3&gt;

&lt;h4 id=&quot;作为线程池的thead_handler每次都会从任务队列&quot;&gt;作为线程池的thead_handler，每次都会从任务队列&lt;/h4&gt;
&lt;p&gt;中取一个任务进行运行&lt;/p&gt;

&lt;h2 id=&quot;threadmgr&quot;&gt;ThreadMgr&lt;/h2&gt;

&lt;h3 id=&quot;thread&quot;&gt;thread&lt;/h3&gt;

&lt;h4 id=&quot;对线程的封装线程运行函数由thread_handler指定&quot;&gt;对线程的封装，线程运行函数由thread_handler指定&lt;/h4&gt;

&lt;h3 id=&quot;thread_handler&quot;&gt;thread_handler&lt;/h3&gt;

&lt;h4 id=&quot;线程真正的实现逻辑不同类型的线程有不同的handl&quot;&gt;线程真正的实现逻辑，不同类型的线程有不同的handl&lt;/h4&gt;
&lt;p&gt;er去处理，对于线程池中的handler会不断的从taskM
gr中取task进行处理&lt;/p&gt;

&lt;h3 id=&quot;thread_pool&quot;&gt;thread_pool&lt;/h3&gt;

&lt;h4 id=&quot;线程池模型根据配置中的线程数启动每个线程分配一&quot;&gt;线程池模型,根据配置中的线程数启动每个线程分配一&lt;/h4&gt;
&lt;p&gt;个handler&lt;/p&gt;

&lt;h2 id=&quot;pluginmgr&quot;&gt;PluginMgr&lt;/h2&gt;

&lt;h2 id=&quot;dictmgr&quot;&gt;DictMgr&lt;/h2&gt;

&lt;h2 id=&quot;configmgr&quot;&gt;ConfigMgr&lt;/h2&gt;

&lt;h2 id=&quot;memorymgr&quot;&gt;MemoryMgr&lt;/h2&gt;
</description>
        <pubDate>Wed, 20 Apr 2016 23:11:02 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/20/SUB.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/20/SUB.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>卷积神经网络的一些小事儿</title>
        <description>&lt;h3 id=&quot;1-从手写体图片识别说起&quot;&gt;1. 从手写体图片识别说起&lt;/h3&gt;

&lt;p&gt;手写体识别一般是神经网络入门必备的demo，在传统的神经网络进行手写体识别的时候，需要更多的人工对特征进行处理，一般是如下的步骤:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1.将原始图片的像素值栅格化为一维向量,如原始图片为8*8, 则栅格化之后的向量为164; &lt;br /&gt;
2.将栅格化的向量作为神经网络的输入，手写体对应的数字编码之后作为神经网络的输出;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;然后在MNIST数据集上也能有96%以上的正确率，但是MNIST数据集给的都是28*28大小的数据，如果图片的像素规模很大呢？这里可能就会考虑两个方案：要么就是直接将全部的像素作为神经网络的输入；要么就是人工去对像素进行采样降维。那么可行性又如何？对于第一种，假设隐层神经元的数量为1000，那么输入层-隐含层权值矩阵就达到了$10^9$的规模，一般的机器根本训不动，更别提用梯度下降这种可能会陷入局部最优的算法了(参数多，解空间大)；对于第二种可能就是无尽的特征提取工作了。&lt;/p&gt;

&lt;p&gt;谈到第二点，深度学习某种意义可以认为是对特征的学习. 领域知识在传统机器学习中十分重要，不管是CTR预估亦或是信用评级、LTR等，都需要在相关领域有较多的经验，往往某几个特征的发现能够给整个系统带来质的飞跃，当然这也使得传统机器学习的解释性更好。&lt;/p&gt;

&lt;h3 id=&quot;2-你的眼睛是怎么看图片的&quot;&gt;2. 你的眼睛是怎么看图片的？&lt;/h3&gt;

&lt;p&gt;通常而言，我们在观察事物的时候，都是会将注意力集中到某一块部分，可能一块还足以让我们判断这个东西的类型，但是多观察几个局部，就能大概知道所观察的东西是个啥了。这个给我们的启发是对于输入的1000*1000像素的图片，我们并不需要所有像素的信息，提取局部的特征也能够给我们勾勒出该图片所代表的事物的特征。这个在卷积神经网络中对应的就是卷积核。另外，对图片进行采样模糊等变换，某种程度上也不会改变图片中事物的主要特征，这个在卷积神经网络中对应的就是pooling，也就是向下采样.&lt;/p&gt;

&lt;h3 id=&quot;3-卷积神经网络&quot;&gt;3. 卷积神经网络&lt;/h3&gt;

&lt;p&gt;那么卷积神经网络是如何处理类似于图片这种结构的输入呢？卷积神经网络中采用了两种十分有效的思路：局部感知和权值共享；前面提到如果直接将图像的像素作为神经网络的输入，需要学习的参数较多。表观上图像所代表的信息通过局部也能够表达，对于神经网络而言，虽然单个神经元只感受到了局部，但是整体的特征已经被整个模型所接收，最后的效果也不会差到哪里去，而随之带来的就是学习参数数目的下降。比如原始图像为1000*1000，隐层为1000000个节点，每个节点只与输入图像的5*5的区域相连，此时需要学习的参数为2.5*10^7，是原先的四万分之一.&lt;/p&gt;

&lt;p&gt;但是这些参数的学习仍然是一个不小的挑战，但是如果隐层节点与输入层区域相连的权值都一样呢，那样子需要学习的参数仅仅为25个，而这个25个参数一般也被称之为卷积核，也有叫滤波器的，总而言之就是对局部特征进行某种线性变换的参数！如此而来，用较小的参数规模实现了对图像局部特征的学习！单个的卷积核造成的问题就是特征提取不够充分，所以一般采用的都是多个卷积核，每种卷积核所体现的就是图像局部不同的特征！卷积过后，如果直接用这些特征进行分类，一是维度过大导致训练不便，二是有可能导致过拟合，所以一般在卷积之后会加一个对卷积层特征的聚合过程，也即Pooling层。&lt;/p&gt;

&lt;p&gt;表达卷积神经网络一般是卷积层与pooling层交替组合，构成深度神经网络，多层卷积能够组合局部特征从而表征全局，像下图中经典的LeNet-5网络结构，就是由3个卷积层和两个pooling层以及一个全连接层组成。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7pn4yt.com1.z0.glb.clouddn.com/blog-lenet.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;卷积层和pooling层可以认为是对特征的学习过程，然后将学习到的特征再输入到一个分类器中进行训练. 在卷积层，是利用卷积核对上一层的特征图进行卷积操作，然后将卷积结果重新输出到新的特征图中，在LeNet5中的C1层中，利用的就是6个5*5的卷积核对输入图像进行卷积，然后得到6个特征图，每个特征图的大小是28*28.&lt;/p&gt;

&lt;p&gt;这里需要注意的是，卷积层的特征图并不仅仅是来自于前一层的一个特征图，有可能是前一层的多个特征图和多个卷积核进行联合操作之后得到，如LeNet-5中Pooling层S2到卷积层C3就是按照如下的排列进行连接的, 其中C3的第一个特征图来自于S2中的0,1,2号3个特征图，最后一个特征图与S2中的所有特征图相连，此时需要60个卷积核. 加上偏置，需要学习的参数为60*5*5+16. &lt;br /&gt;
&lt;img src=&quot;http://images2015.cnblogs.com/blog/743682/201604/743682-20160421101636460-1080820356.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;31-卷积层&quot;&gt;3.1 卷积层&lt;/h4&gt;
&lt;p&gt;令$X_j^l$ 为第$l$层(卷积层)第j个特征图，当$l=1$时即为原始图像特征，$M_j^l$为与第$l$层(卷积层)第$j$个特征图相连的前一层特征图集合的某个子集，那么第l层的第j个特征图的计算：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X_j^l = f(\sum_{i\in M_j^l}K^l_{i,j}*X_i^{l-1} + b_j^l)&lt;/script&gt;

&lt;p&gt;其中，&lt;script type=&quot;math/tex&quot;&gt;*&lt;/script&gt;为卷积运算，$b_j^l$为一维的实数，假设图片像素矩阵$P$，卷积核为$K$, 如下:&lt;/p&gt;

&lt;p&gt;
$$
{\begin{matrix}
P=\begin{bmatrix}
0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 2\\
     1 &amp;amp; 0 &amp;amp; 3 &amp;amp; 3 \\
     3 &amp;amp; 4 &amp;amp; 3 &amp;amp; 0 \\
     4 &amp;amp; 2 &amp;amp; 4 &amp;amp; 4 \\
     
\end{bmatrix} &amp;amp;   K = \begin{bmatrix}
2 &amp;amp; 1 \\ 
1 &amp;amp; -1
\end{bmatrix}
\end{matrix}}
$$
&lt;/p&gt;
&lt;p&gt;那么对图像进行卷积之后的结果$F=K*P$为：&lt;/p&gt;
&lt;p&gt;
$$
F=\begin{bmatrix}
1 &amp;amp; 7 &amp;amp; 11 \\
     10 &amp;amp; 13 &amp;amp; 3 \\
     9 &amp;amp; 9 &amp;amp; 9 \\
\end{bmatrix} 
$$
&lt;/p&gt;
&lt;p&gt;此时需要注意卷积不是直接算矩阵点乘，而是将卷积矩阵旋转180度之后再算点乘。并且F中的元素与P中的元素有如下的对应关系，其中u,v为输出特征图的下标，卷积核大小为d*d，$K_{d-h,d-j}$为卷积核参数$K_{h,j}$旋转180度之后对应的值，$P_{u+h,v+j}$为与输入特征图进行卷积的位置的像素值。&lt;/p&gt;

&lt;p&gt;
$$
F_{u,v} = \sum_{h}\sum_{j} K_{d-h,d-j}*P_{u+h,v+j}
$$
&lt;/p&gt;

&lt;p&gt;卷积核的大小d一般选奇数，卷积时候的步长(一般为1)也可以根据具体的场景进行调整，当然如果为了保证卷积之后维度不变，也可以进行边缘扩展，在原始特征图的边缘补充新的像素值。&lt;/p&gt;

&lt;h4 id=&quot;32-pooling层&quot;&gt;3.2 Pooling层&lt;/h4&gt;
&lt;p&gt;Pooling层主要的作用是降噪降维，缓解过拟合的尴尬。为什么这么说呢？因为就典型的max pooling（取最大）或者averaging pooling(取平均)操作而言，它能够使得模型关注于局部某一部分，而不是具体的像素，模型的鲁棒性和泛化能力得到了一定程度的保障。&lt;/p&gt;

&lt;p&gt;Pooling层的特征图与上一层的卷积层一一对应，只不过特征图的维度变小。令$X_j^l$为Pooling层的第j个特征图，$down()$为向下采样。则有：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X_j^l = f(\beta_j^l \cdot down(X_j^{l-1}) + b_j^l)&lt;/script&gt;

&lt;p&gt;对于Pooling层的输入特征图，其每一个采样局部共享同一个采样尺度和偏置，也即$\beta_j^ll$和$b_j^l$都是一个实数. Pooling层之后，维度一般会减半。假设学习到的采样尺度为0.25, 偏执为0.1，采样窗口为2*2，激励函数f才用的sigmoid函数，则在Pooling层由输入特征图到输出特征图的过程如下：&lt;/p&gt;

&lt;p&gt;
$$
{\begin{matrix}
X_j^{l-1}=\begin{bmatrix}
0 &amp;amp; 2 &amp;amp; 4 &amp;amp; 2\\
     2 &amp;amp; 0 &amp;amp; 3 &amp;amp; 3 \\
     3 &amp;amp; 4 &amp;amp; 3 &amp;amp; 0 \\
     4 &amp;amp; 2 &amp;amp; 4 &amp;amp; 4 \\
     
\end{bmatrix} 
&amp;amp;   
X_j^l = \begin{bmatrix}
f(1.1) &amp;amp; f(3.1) \\ 
f(3.35) &amp;amp; f(2.85)
\end{bmatrix}
\end{matrix}}
$$
&lt;/p&gt;

&lt;h4 id=&quot;33-全连接层&quot;&gt;3.3 全连接层&lt;/h4&gt;

&lt;p&gt;在通过卷积层和Pooling层可以认为是对特征的学习，在这个交替的层次之后，原始的数据被转换到新的特征空间中，然后通过全连接层将特征空间和样本空间进行关联。在通常情况下，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽。&lt;/p&gt;

&lt;h4 id=&quot;34-反向传播推导&quot;&gt;3.4 反向传播推导&lt;/h4&gt;

&lt;p&gt;CNN的推导和基本的全连接神经网络无异，没什么特别的地方，关键在于两点:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;1.卷积层到Pooling层，前向计算的时候由于下采样导致为维度降低，在误差反向传导时，需要对下一层误差进行反向采样 &lt;br /&gt;
2.Pooling层到卷积层，误差项中需要弄清楚和当前卷积核进行卷积的patch.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;令$\delta_j^l$ 为第$l$层第$j$个特征图$X_j^l$对应的误差项，其维度和$X_j^l$一致，$(\delta_j^l)_{u,v}$ 为$X_j^l$ 下标为u,v对应的误差项，即有：&lt;/p&gt;

&lt;p&gt;
$$
\begin{align}
\delta_j^l &amp;amp; = \frac {\partial E}{\partial net_j^l} ; (\delta_j^l)_{u,v} = \frac {\partial E}{\partial ((net_j^l)_{u,v})} \\

net_j^l &amp;amp;= \left\{
\begin{aligned}
&amp;amp; \sum_{i\in M_j^l}K^l_{i,j}*X_i^{l-1} + b_j^l \ \ \ l层为卷积层 \\
&amp;amp; \beta_j^l \cdot down(X_j^{l-1}) + b_j^l \ \ \ l层为Pooling层\\
\end{aligned}
\right.
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;当$l$层为卷积层时&lt;/strong&gt;，需要学习的参数为卷积核和偏执，个数为$\sum_{j}{||M_j||} + N^l$，其中$||M_j||$为与卷积层第j个特征图相连的前层特征图的某个子集，$N^l$为第l层卷积层的特征图数目。&lt;/p&gt;

&lt;p&gt;每一个输出特征图对应一个为偏执$b_j^l$, 且$b_j^l$为标量，其与每一个$(\delta_j^l)_{u,v}$ 都有关联，所以对偏执的求导为：&lt;/p&gt;

&lt;p&gt;
$$
\begin{align}
\frac {\partial E}{\partial b_j^l} &amp;amp; = \sum_u\sum_v \frac {\partial E}{\partial ((net_j^l)_{u,v})} \frac {\partial ((net_j^l)_{u,v})}{\partial b_j^l} =  \sum_u\sum_v (\delta_j^l)_{u,v}
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;每一个输出特征图由若干个卷积核与输入特征图卷积得来，对于卷积核$K_{i,j}^l$而言，其求导的结果为一个$d*d$的矩阵，对于其中的某个值$(K_{i,j})_{r,s}$求导为, 其中r,s为卷积核中的下标，大小在[0,d-1]范围内，u,v为输出特征图的下标：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
\frac {\partial E}{\partial ((K_{i,j}^l)_{r,s})} &amp;amp; = \sum_u\sum_v \frac {\partial E}{\partial ((net_j^l)_{u,v})} \frac {\partial ((net_j^l)_{u,v})}{\partial ((K_{i,j}^l)_{r,s})} \\ 
&amp;amp; = \sum_u\sum_v (\delta_j^l)_{u,v}\frac {\partial ((net_j^l)_{u,v})}{\partial ((K_{i,j}^l)_{r,s})} \\
&amp;amp; = \sum_u\sum_v (\delta_j^l)_{u,v}\frac {\partial ((X_i^{l-1} * K_{i,j}^l)_{u,v})}{\partial ((K_{i,j}^l)_{r,s})}
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;此时又有：&lt;/p&gt;
&lt;p&gt;
$$
(X_i^{l-1} * K_{i,j}^l)_{u,v} = \sum_{r}\sum_{s} (K_{i,j}^l)_{r,s}(X_i^{l-1})_{u+r,v+s}
$$
&lt;/p&gt;
&lt;p&gt;所以：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
\frac {\partial E}{\partial ((K_{i,j}^l)_{r,s})} &amp;amp; = \sum_u\sum_v (\delta_j^l)_{u,v} (X_i^{l-1})_{u+r,v+s}
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;也即：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
\frac {\partial E}{\partial K_{i,j}^l} &amp;amp; = \sum_u\sum_v (\delta_j^l)_{u,v} (PT_i^{l-1})_{u,v}
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;其中，$PT_i^{l-1}$为$X_i^{l-1}$中与卷积核进行卷积的部分，可以认为是一个4维的矩阵，$(PT_i^{l-1})_{u,v}$对应于$X_i^{l-1}$中$row\in [u,u+d-1] \ \ col\in [v,v+d-1]$范围内的部分.&lt;/p&gt;

&lt;p&gt;卷积层的误差项来自于下一层与其相连的Pooling层，此时有:&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
\left\{
\begin{aligned}
net_j^{l+1} &amp;amp; = \beta_j^{l+1} \circ down(X_j^{l}) + b_j^{l+1} \\
X_j^{l} &amp;amp; = f(net_j^l) \\
\end{aligned}
\right.
\end{align}
$$
&lt;/p&gt;
&lt;p&gt;所以：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
\frac {\partial E}{\partial net_j^l} &amp;amp; = \frac {\partial E}{\partial net_j^{l+1}} \frac {\partial E}{\partial net_j^{l+1}}
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;当$l$层为Pooling层时&lt;/strong&gt;，需要学习的参数为偏执和采样尺度，每一个特征图对应两个参数。对偏执的求导与卷积层无异，对采样尺度$\beta_j^l$的求导如下：&lt;/p&gt;

&lt;p&gt;
$$
\begin{align}
\frac {\partial E}{\partial \beta_j^l} &amp;amp; = \sum_u\sum_v \frac {\partial E}{\partial ((net_j^l)_{u,v})} \frac {\partial ((net_j^l)_{u,v})}{\partial \beta_j^l} \\
&amp;amp; =  \sum_u\sum_v (\delta_j^l)_{u,v} down(X_j^{l-1})_{u,v} \\
&amp;amp; = \sum_u\sum_v (\delta_j^l \circ down(X_j^{l-1}))_{u,v}
\end{align}
$$
&lt;/p&gt;

&lt;h3 id=&quot;35-应用&quot;&gt;3.5 应用&lt;/h3&gt;

&lt;h3 id=&quot;参考文献&quot;&gt;参考文献&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/xuanyuansen/article/details/41800721&quot;&gt;卷积神经网络（一）：LeNet5的基本结构&lt;/a&gt; &lt;br /&gt;
&lt;a href=&quot;http://blog.csdn.net/kaido0/article/details/53161684&quot;&gt;理解结构，LeNet5介绍&lt;/a&gt; &lt;br /&gt;
&lt;a href=&quot;http://dataunion.org/11692.html&quot;&gt;技术向：一文读懂卷积神经网络CNN&lt;/a&gt;
&lt;a href=&quot;https://www.zhihu.com/question/41037974&quot;&gt;全连接层的作用是什么？&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 20 Apr 2016 23:11:02 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/20/CNN.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/20/CNN.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>Ensemble Learning Methods</title>
        <description>&lt;p&gt;集成学习的关键是弱学习器的组合，目标是为了提升模型的性能，一般用在Model Selection，此时降低了选择弱名的可能性。当然集成学习也广泛的应用于结果置信度检验、特征选择、数据融合、增量学习等方面，取得了良好的效果。&lt;/p&gt;

&lt;p&gt;一般而言，对于给定的任务，比如分类，首先在处理完特征之后，我们需要考虑的是选择何种模型来对这些样本进行训练。一般我们没有样本数据产生过程的先验知识，只能通过手动的选择调参，才有可能得到一个较好拟合已知样本的模型。集成学习给了一个非常完美的模型选择的解决方案，通过训练多个子模型，组成committee，最后综合决策，达到了自动选择模型的效果。&lt;/p&gt;

&lt;p&gt;一般模型的性能某种程度取决于数据集的大小，也即观测样本的覆盖度。数据集过大对于一般模型可能会导致训练过拟合或者训练性能瓶颈，此时则可以切分数据集，在子数据集上单独训练模型，然后按照某种组合策略构建committee。数据集过小对于一般模型则会欠拟合，此时可以使用bootstrap等抽样方法，在每一份抽样样本中单独训练模型，组成committee。&lt;/p&gt;

&lt;p&gt;一般而言，样本的分类边界十分复杂，如果仅仅使用一般的模型，则很难学习出能够较好拟合样本的分类边界。
但是如果使用集成学习框架，通过学习多个子模型，则能够很好的学习出该分类边界。&lt;/p&gt;

&lt;p&gt;另外，如果我们收集到的数据来源很多，导致数据的类型、维度不同，如果将所有来源的数据都放在同一个向量中，可能会降低模型的学习能力。
但是假如我们在不同来源的数据中根据该数据特性或者其他先验知识单独训练模型，则效果一般都会好很多。
今天看了一篇关于新浪微博垃圾用户检测的论文，研究现状里就提到了采用这种思想的方法，通过观测，发现垃圾用户一般有三种行为：广告、重复转发和恶意关注，然后根据这三种类型去抽取特征，得到三种类型的特征向量之后，然后分别在三种特征上进行子模型训练，最后效果也还不错。&lt;/p&gt;

&lt;p&gt;此外，集成学习框架还可以用来对结果进行评估，可以根据committee中的子模型的vote去计算。&lt;/p&gt;

&lt;p&gt;有人总结了使用ensemble learning的三大原因：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;统计学，已知样本无法完成的表达原始数据的生成分布，通过bootstrap等方法，可以尽量的拟合原始分布训练模型。&lt;/li&gt;
  &lt;li&gt;可计算，体现于模型选择，通过模型融合的解决方案将缺乏模型先验知识的人从单模型调优中解放出来。&lt;/li&gt;
  &lt;li&gt;任意拟合，单模型无法拟合出复杂分类边界，则模型融合则很好的解决了这一问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;参考文献 &lt;a href=&quot;http://www.scholarpedia.org/article/Ensemble_learning&quot;&gt;http://www.scholarpedia.org/article/Ensemble_learning&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 03 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/03/EnsembleLearningMethods.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/03/EnsembleLearningMethods.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>Probability Latent Semantic Analysis</title>
        <description>&lt;h3 id=&quot;1-建模思想&quot;&gt;1. 建模思想&lt;/h3&gt;
&lt;p&gt;pLSA是一种主题模型(Topic Model)，全称概率潜在语义分析，是一种将文本的高维稀疏向量表示成低位维稠密向量的映射方法。
它是一种无监督的建模方法，用以表征某doc的主题分布. 其假设文档上的主题分布以及某个主题的词分布服从多项式分布。某一篇文章都以一定的概率&lt;script type=&quot;math/tex&quot;&gt;p(z \mid d)&lt;/script&gt;属于某一主题;在给定主题的情况下，以&lt;script type=&quot;math/tex&quot;&gt;p(w \mid z)&lt;/script&gt;的概率产生文档的词，即：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;以一定的概率生成一篇文档&lt;/li&gt;
  &lt;li&gt;在该文档中按照文档主题分布选择一个主题&lt;/li&gt;
  &lt;li&gt;在该主题下面，按照主题-词汇分布生成一个词汇&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;pLSA是一个生成模型，构建的是联合概率分布，即&lt;script type=&quot;math/tex&quot;&gt;p(w,d)=p(d)\sum_{z=1}p(z \mid d)p(w \mid z)&lt;/script&gt;，其中z为主题，w为词，d为文档。
其实整个过程也很容易理解，假设现在我们要写篇paper，在动笔之前，会列好提纲，大概选择几个点(也就是主题)。在完善这些主题的过程中，我们会在积累的词典中去选择和这个主题相适应的词进行修饰。比如写到关于pLSA的文章的时候，我们会选择”建模、概率、主题模型、极大似然“等词去完善这个主题。&lt;/p&gt;

&lt;h3 id=&quot;2-参数估计&quot;&gt;2. 参数估计&lt;/h3&gt;

&lt;p&gt;在得到模型之后，我们就需要进行模型的参数估计了。由于我们&lt;script type=&quot;math/tex&quot;&gt;p(w \mid z)&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;p(z \mid d)&lt;/script&gt;都是多项式分布，构造的极大似然函数如下，其中&lt;script type=&quot;math/tex&quot;&gt;n(d_i,w_j)&lt;/script&gt;表示的是词&lt;script type=&quot;math/tex&quot;&gt;w_j&lt;/script&gt;在和文档&lt;script type=&quot;math/tex&quot;&gt;d_i&lt;/script&gt;的共现频率，$N$表示的是文档数，$M$表示的词的数目，$K$是主题的数目：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align} 
L(\theta) &amp;amp; = \prod_{i=1}^N\prod_{j=1}^Mp(d_i,w_j)^{n(d_i,w_j)} \\ 
ln(L(\theta)) &amp;amp; = \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)logp(d_i,w_j) \\
&amp;amp; =  \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)log[p(d_i)\sum_{k=1}^Kp(z_k \mid d_i)p(w_j \mid z_k)] \\
&amp;amp; = \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)log(p(d_i))+\sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)log(\sum_{k=1}^Kp(z_k\mid d_i)p(w_j \mid z_k))
\end{align}
$$
&lt;/p&gt;
&lt;p&gt;如果直接求导的话，很明显就得需要求解(n+m)个方程，几乎是不可能完成的任务。
对于这种带有隐变量的极大似然估计一般采用最大期望算法(Maximum Expection， ME)，EM算法的基本思想很直观，通过优化极大似然估计的下界，对原始问题进行求解。一般包含两个步骤，在E步，计算隐含变量的后验概率；M步，利用隐变量的后验概率去更新未知参数。循环迭代直至达到收敛退出条件。&lt;/p&gt;

&lt;h3 id=&quot;3-em算法&quot;&gt;3. EM算法&lt;/h3&gt;
&lt;p&gt;一般而言，EM算法一般都是对含有隐变量的模型进行求解，一般我们把观测变量和隐变量一起称之为完全数据Complete Data, 但是我们是无法直接对Complete Data进行极大似然估计的，因为隐变量是无法观测的嘛！但是，在对模型进行初步推断之后，我们还是可以得到隐变量的后验分布$P(z|x,\theta)$的。令$p(x;\theta)$为观测变量$x$的似然估计，其对数似然函数如下：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
L(\theta) &amp;amp; = log(p(x;\theta)) \\
&amp;amp; = log(\sum_z{p(x,z;\theta)}) \\
&amp;amp; = log(\sum_zQ(z)\frac{p(x,z;\theta)}{Q(z)})
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;由Jenkens不等式$f(\sum_i {\lambda_i x_i}) &amp;gt;= \sum_i \lambda_i f(x_i), \sum_i \lambda_i = 1 $, 并且$\sum_z Q(z) = 1$， 可得:&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
L(\theta) &amp;amp; = log(\sum_zQ(z)\frac{p(x,z;\theta)}{Q(z)}) \\
&amp;amp; &amp;gt;= \sum_zQ(z) log(\frac{p(x,z;\theta)}{Q(z)}) 
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;此时等号当且仅当$\frac{p(x,z;\theta)}{Q(z)}$为常数时成立，即：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
\frac{p(x,z;\theta)}{Q(z)} &amp;amp; = C \\
\sum_z Q(z) &amp;amp; = 1 \\
Q(z) &amp;amp; =  \frac{p(x,z;\theta)}{\sum_z {p(x,z;\theta)}} \\
&amp;amp; = p(z|x;\theta)
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;由此，在给定$\theta^t$之后，可以保证Jensens不等式的等号成立，也即：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
L(\theta^t) &amp;amp; = \sum_zQ(z) log(\frac{p(x,z;\theta^t)}{Q(z)}) 
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;由此，将优化原始极大似然估计的问题转化为优化其下界函数$B(\theta)$：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
B(\theta) = \sum_zQ(z) log(\frac{p(x,z;\theta)}{Q(z)}) \propto \sum_zQ(z) log(p(x,z;\theta))
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;那么优化下界为什么能够保证收敛呢？对于第t+1次迭代，$B(\theta^{t+1}) &amp;gt;= B(\theta^{t})$，且有：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
L(\theta^{t+1}) &amp;amp; &amp;gt;= B(\theta^{t+1}) \\
&amp;amp; &amp;gt;= B(\theta^{t}) = \sum_zQ(z) log(\frac{p(x,z;\theta^{t})}{Q(z)}) \\
&amp;amp; &amp;gt;= L(\theta^{t})
\end{align}
$$
&lt;/p&gt;
&lt;p&gt;所以能够保证在不断优化$B(\theta)$的过程中，原有的极大似然估计也得到优化. 其实EM算法之所以交期望最大化算法，其关键也在于对$B(\theta)$的优化。前面提到了无法直接对Complete Data进行似然估计，观察$B(\theta)$我们发现，当$Q(z)=p(z|x;\theta)$时，$B(\theta)$可以看做是Complete Data在隐变量后验分布上的期望，也即$B(\theta) = E_{z|x,\theta^*}(ln(p(x,z,\theta)))$, 也有写成$B(\theta) = E_{z}(ln(p(x,z,\theta|x,\theta^*)))$, 此时$\theta^*$表示的是上一次迭代学习的模型。&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;之前一直困惑既然E步只是计算$p(z&lt;/td&gt;
      &lt;td&gt;x;\theta)$，何有期望之说。在看了PRML上对EM的解释之后，恍然大悟。在E步我们通过计算$Q(z)$得到隐变量的后验分布，然后在M步，对Complete Data在该后验分布上的期望进行极大化！(ps: 还是应该多看书，网上的博客参考就行，包括我自己的总结，也权当自己日后再次回顾时对当时自己的自嘲吧~)，当然EM算法的优化目标是KL散度，也即complete data的分布和隐变量后验分布的相似度.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;4-plsa-参数优化&quot;&gt;4. plsa 参数优化&lt;/h3&gt;
&lt;p&gt;在e步，求解隐变量的后验分布，从而得到complete data的期望&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
p(z_k|w_j,d_i) &amp;amp; = \frac{p(z_k \mid w_j)p(w_j \mid z_k)} {\sum_{k=1}^Kp(z_k \mid w_j)p(w_j\mid z_k)} \\
E_{z_k|w_j,d_i} log( p(z_{k}|d_i) p(z_{k}|w_j) ) &amp;amp; = \sum_k p(z_k|w_j,d_i) log ( p(z_{k}|d_i) p(z_{k}|w_j) )
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;M步，最大化complete data对数似然函数的期望$E_{z_k|w_j,d_i} log( p(z_{k}|d_i) p(z_{k}|w_j) )$ ，此时是一个多元函数求极值的问题，且有$\sum_{k=1}^Kp(z_k|d_i)=1[1],\sum_{j=1}^Mp(w_j|z_k)=1[2]$的等式约束，可以用拉格朗日方法进行求解，构造拉格朗日函数为：&lt;/p&gt;
&lt;p&gt;
$$
H = L'(\theta) + \sum_{i=1}^N\alpha_i(\sum_{k=1}^Kp(z_k|d_i)-1)+\sum_{k=1}^K\beta_k(\sum_{j=1}^Mp(w_j|z_k)-1)
$$
&lt;/p&gt;
&lt;p&gt;分别对 $p(z_k\mid d_i),p(w_j\mid z_k)$求导可得：&lt;/p&gt;
&lt;p&gt;
$$ 
\begin{align}
\frac {\partial H } {\partial p(z_k|d_i)} &amp;amp; =\sum_{j=1}^Mn(d_i,w_j)p(z_k|d_i,w_j)-\alpha_ip(z_k|d_i) = 0      [3] \\
\frac {\partial H } {\partial p(w_j|z_k)} &amp;amp;= \sum_{i=1}^Nn(d_i,w_j)p(z_k|d_i,w_j)-\beta_kp(w_j|z_k) = 0     [4]
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;联立方程组[1],[2],[3],[4]求解可得：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
p(z_k|d_i) &amp;amp; = \frac {\sum_{i=1}^N n(d_i,w_j)p(z_k|d_i,w_j)}{\sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)p(z_k|d_i,w_j)} \\
p(w_j|z_k) &amp;amp; = \frac {\sum_{j=1}^Mn(d_i,w_j)p(z_k|d_i,w_j)}{\sum_{j=1}^M\sum_{k=1}^Kn(d_i,w_j)p(z_k|d_i,w_j)} \\
&amp;amp; = \frac{\sum_{j=1}^Mn(d_i,w_j)p(z_k|d_i,w_j)}{n(d_i)} 
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;因此，在M步更求新参数，通过不断迭代，如果求得的极大似然趋近收敛，则训练结束。Plsa跟lsa之间也有某种关系，&lt;/p&gt;

&lt;p&gt;总结一下：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;E步：求当前估计的参数条件下的后验概率：&lt;/li&gt;
  &lt;li&gt;M步：最大化complete data(加上隐藏变量主题)极大似然估计的期望&lt;/li&gt;
  &lt;li&gt;E步和M步循环迭代，直至收敛。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实现的源码见：&lt;a href=&quot;https://github.com/kymo/plsa&quot;&gt;plsa&lt;/a&gt; ，由于切词用到了公司的lib库，所以切词的初始化和切词的过程在源码中被删去，可以去中科院nlp主页获取相关模块。&lt;/p&gt;

</description>
        <pubDate>Sat, 02 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/naturallanguageprocessing/2016/04/02/pLSA.html</link>
        <guid isPermaLink="true">http://kymo.github.io/naturallanguageprocessing/2016/04/02/pLSA.html</guid>
        
        
        <category>NaturalLanguageProcessing</category>
        
      </item>
    
      <item>
        <title>Support Vector Machine</title>
        <description>&lt;p&gt;Support Vector Machine ，支持向量机，通常用来进行classification，但是也有做regression。SVM在面对非线性问题上具有独特的优势。本文从linear和nonlinear两种情况下对SVM的建模过程、优化目标的求解推导过程以及优化算法SMO进行阐述。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a^2 + b^2 = c^2&lt;/script&gt;

&lt;h3 id=&quot;1-linear-svm&quot;&gt;1. linear SVM&lt;/h3&gt;

&lt;p&gt;在分类任务中，样本label为&lt;script type=&quot;math/tex&quot;&gt;{-1,1}&lt;/script&gt;，关于从sign distance转换到geometry distance的过程其实很容易理解，
sign distance可以衡量某个样本被分类的置信，如果sign distance越大，那么该样本被分为该类别的可信度就更大；
而geometry distance可以理解为样本距离超平面&lt;script type=&quot;math/tex&quot;&gt;Y = w^TX + b&lt;/script&gt;的距离，是sign distance归一化的结果，
求解目标为&lt;script type=&quot;math/tex&quot;&gt;argmax(\frac { \mid w^Tx + b \mid } { \mid  \mid w \mid  \mid })&lt;/script&gt;，并且需要满足约束：&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i + b) \geq 1&lt;/script&gt;，为了求解方便，可以不加证明的令&lt;script type=&quot;math/tex&quot;&gt;\mid w^Tx+b \mid =1&lt;/script&gt;，形式化如下：&lt;/p&gt;

&lt;p&gt;
$$
\begin{cases} 
argmax \frac{1}{ \mid  \mid w \mid  \mid } \\ 
y_i(w^Tx_i+b) \geq 1   ~~~ i=1,2,\cdot \cdot ,n 
\end{cases} 
$$
&lt;/p&gt;
&lt;p&gt;而此时最大化 &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{ \mid  \mid w \mid  \mid }&lt;/script&gt;等价于最小化 &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; ，并且最小化和最小化&lt;script type=&quot;math/tex&quot;&gt;\frac {1}{2} w^Tw&lt;/script&gt;等价，&lt;/p&gt;

&lt;p&gt;所以1.1可以变为：&lt;/p&gt;
&lt;p&gt;
$$
\begin{cases}
argmin \frac {1}{2} w^Tw \\
y_i(w^Tx_i+b) \geq 1 ~~~ i=1,2,\cdot \cdot ,n 
\end{cases}
$$
&lt;/p&gt;
&lt;p&gt;由此可以得到不等式约束问题，原始问题通过分析不难发现，求解十分困难，不过对于PSO(粒子群算法)而言，往往可以求得比较好的解。不过在碰到这种带不等式约束的问题的时候，我们可以通过拉格朗日对偶性质将原始问题转化为对偶问题，在最大熵模型中也有类似的处理过程。首先构造拉格朗日函数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(w,b,\alpha) = \frac {1} {2} w^Tw - \sum_{i=1}^N a_i[y_i(w^Tx_i + b) - 1]&lt;/script&gt;

&lt;p&gt;首先我们可以得到这个函数的等价形式，令&lt;script type=&quot;math/tex&quot;&gt;\theta = argmax_\alpha L(w,b,\alpha)&lt;/script&gt;，那么：&lt;/p&gt;

&lt;p&gt;
$$
\theta=
\begin{cases}\frac {1} {2} w^Twy_i(w^Tx_i+b) \geq 1 \\
\infty &amp;lt; y_i(w^Tx_i+b) &amp;lt; 1
\end{cases}
$$
&lt;/p&gt;

&lt;p&gt;可见，&lt;script type=&quot;math/tex&quot;&gt;min\theta​&lt;/script&gt;和原始优化目标&lt;script type=&quot;math/tex&quot;&gt;argmin \frac {1}{2}w^Tw​&lt;/script&gt;等价。令&lt;script type=&quot;math/tex&quot;&gt;p=min\theta​&lt;/script&gt;，其对偶形式&lt;script type=&quot;math/tex&quot;&gt;d=max_{\alpha}min_{w,b}L(w,\alpha)​&lt;/script&gt;，那么必然会有&lt;script type=&quot;math/tex&quot;&gt;p \geq d​&lt;/script&gt;。此时令&lt;script type=&quot;math/tex&quot;&gt;g=argmin_{w} L(w,b,\alpha)​&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;g \leq L(w^*,b,\alpha) \leq \frac {1}{2} {w^*}^Tw^* = p​&lt;/script&gt;，所以&lt;script type=&quot;math/tex&quot;&gt;p \geq d​&lt;/script&gt;。所以此时该算法满足弱对偶，我们可以通过求解该弱对偶问题去近似求解原始问题，在EM中就是不断优化极大似然下界~ 并且可以知道的是，不管原始问题是何种优化，对偶问题都会是凸优化，也即都会存在极值。不过在SVM中，我们是可以把弱对偶加强，变成strong duality，也即&lt;script type=&quot;math/tex&quot;&gt;p = d​&lt;/script&gt;，优化对偶问题等价于对原始问题的求解。那么怎么判断该对偶问题是强对偶问题呢？KKT条件。如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}  \bigtriangledown L(w,b,\alpha) = 0 \\ \alpha_i(y_i(w^Tx_i+b) - 1) = 0 \\ \alpha_i \geq 0 \end{cases}&lt;/script&gt;

&lt;p&gt;很明显，此时KKT条件成立，所以满足强对偶。其中&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i+b)-1) = 0&lt;/script&gt;但此时KKT只是必要条件，不过由于我们的原始问题是凸优化，所以KKT便是充要条件了。&lt;/p&gt;

&lt;p&gt;对对偶问题，首先是&lt;script type=&quot;math/tex&quot;&gt;min_{w,b}L(w,b,\alpha)&lt;/script&gt;，分别对&lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;求导，得：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac {\partial L(w,\alpha,b)} {\partial(w)} = w - \sum_{i=1}^N \alpha_iy_ix_i=0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac {\partial L(w,\alpha,b)} {\partial(b)} = \sum_{i=1}^N\alpha_iy_i=0&lt;/script&gt;

&lt;p&gt;随后，将&lt;script type=&quot;math/tex&quot;&gt;w=\sum_{i=1}^N\alpha_iy_ix_i&lt;/script&gt;代入&lt;script type=&quot;math/tex&quot;&gt;L(w,b,\alpha)&lt;/script&gt;中，则有：&lt;/p&gt;
&lt;p&gt;
$$
\begin{eqnarray*}
L(w,\alpha,b)&amp;amp;=&amp;amp;\frac {1}{2} w^Tw - \sum_{i=1}^N\alpha_i[y_i(w^Tx_i+b)-1]\\&amp;amp;=&amp;amp;\frac{1}{2}\sum_{i=1}^N\alpha_iy_ix_i^T\sum_{j=1}^N\alpha_jy_jx_j - \sum_{i=1}^N\alpha_i[y_i(w^Tx_i+b)-1]\\ &amp;amp;=&amp;amp;\frac {1}{2} \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j - \sum_{i=1}^N\alpha_i\sum_{j=1}^N\alpha_jy_jx_j^Tx_i + \sum_{i=1}^N\alpha_i\\&amp;amp;=&amp;amp;-\frac {1}{2} \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j + \sum_{i=1}^N\alpha_i\end{eqnarray*}
$$
&lt;/p&gt;

&lt;p&gt;所以：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}-\frac {1}{2} \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j + \sum_{i=1}^N\alpha_i\\s.t~~~\sum_{i=1}^N\alpha_iy_i=0 
\end{cases}&lt;/script&gt;

&lt;p&gt;到这，我们还没有考虑soft margin。实际情况中，总是会存在一定的噪声数据，使得我们的分类超平面被这些噪声数据所误导，从而使得模型的variance增大，所以一般来讲都会采用soft margin构建优化函数，我们以&lt;script type=&quot;math/tex&quot;&gt;\varepsilon&lt;/script&gt;的范围容许一定的误差，即原来的&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i+b) \geq 1&lt;/script&gt;此时为&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i + b) \geq 1 - \varepsilon_i&lt;/script&gt;，所以我们的优化目标变为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}argmin \frac {1}{2} w^Tw + C\sum_{i=1}^N\varepsilon_i \\s.t ~~ y_i(w^Tx_i+b) \geq 1 - \varepsilon_i ~~i=1,2,\cdot \cdot ,n \\ \sum_{i=1}^N\varepsilon_i \geq C \end{cases}&lt;/script&gt;

&lt;p&gt;关于上式，可以看成是利用hinge loss加l2范数正则项的结果，SVM此时的损失函数可以表示为 &lt;script type=&quot;math/tex&quot;&gt;min_{w,b} \sum_{i=1}^N [1-y_i(w^Tx_i+b)]_{+} + \lambda\ \mid w\ \mid ^2&lt;/script&gt;，其中如果&lt;script type=&quot;math/tex&quot;&gt;z&gt;0&lt;/script&gt;，那么&lt;script type=&quot;math/tex&quot;&gt;z_{+}&lt;/script&gt;=z，否则等于0。如果令&lt;script type=&quot;math/tex&quot;&gt;\varepsilon_i=1-y_i(w^Tx_i+b),\varepsilon_i \geq 0&lt;/script&gt;，那么此时最优化问题为 &lt;script type=&quot;math/tex&quot;&gt;min_{w,b} \sum_{i=1}^N\varepsilon_i + \lambda\ \mid w\ \mid ^2&lt;/script&gt;，如果取&lt;script type=&quot;math/tex&quot;&gt;\lambda=\frac {1}{2C}&lt;/script&gt;，那么就和上述优化目标等价，所以可以看出，软间隔实际上是在ERM的基础上加了SRM~&lt;/p&gt;

&lt;p&gt;之后的推导没有多大差别，只不过在求导的过程中出现了&lt;script type=&quot;math/tex&quot;&gt;\alpha_i = C - \varepsilon_i&lt;/script&gt;，又有前面在对偶转换使用的KKT条件之一&lt;script type=&quot;math/tex&quot;&gt;\alpha_i(y_i(w^Tx_i+b) - 1) =0&lt;/script&gt;可得，在分类超平面上的点，也即满足&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i+b)-1 + \varepsilon_i = 0&lt;/script&gt;，而那些不在超平面的点，必然有&lt;script type=&quot;math/tex&quot;&gt;\alpha_i=0&lt;/script&gt;，而在超平面上的，则有&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt; \alpha_i &lt; C %]]&gt;&lt;/script&gt;，在超平面之外的则是&lt;script type=&quot;math/tex&quot;&gt;\alpha_i=C&lt;/script&gt;。&lt;/p&gt;

&lt;h3 id=&quot;2-nonlinear-svm&quot;&gt;2. nonlinear SVM&lt;/h3&gt;

&lt;p&gt;非线性情况之下，利用线性曲线去拟合，明显会产生underfitting，但是我们可以通过函数映射的方式，将原来空间中的非线性特征，映射到高维空间中，使得样本可分或者近似可分，实际上是，机器学习中有一种叫做基展开的技术，就是处理这种线性到非线性的特征映射。不过对于SVM中使用这种非线性变化是因为它能够和核函数配合的天衣无缝。&lt;/p&gt;

&lt;p&gt;这里用一个简单的例子作简要说明。对于&lt;script type=&quot;math/tex&quot;&gt;x=(x_1,x_2)&lt;/script&gt;二维空间的某个点，我们将其映射到三维空间。所利用的映射函数可以为&lt;script type=&quot;math/tex&quot;&gt;\phi (x_1,x_2) = (x_1^2,x_2^2,2x_1x_2)&lt;/script&gt;，那么在三维空间中，样本线性可分的可能性更大，但是计算开销却上升了，因为在转化成对偶问题之后就产生了向量内积运算。对于原始二维空间中的两点&lt;script type=&quot;math/tex&quot;&gt;p=(\eta_1,\eta_2),q=(\gamma_1,\gamma_2)&lt;/script&gt;，在三维空间中的向量内积为&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;\phi(\eta_1,\eta_2),\phi(\gamma_1,\gamma_2)&gt; = \eta_1^2\gamma_1^2 + \eta_2^2\gamma_2^2 + 4\eta_1\eta_2\gamma_1\gamma_2 %]]&gt;&lt;/script&gt;，这和&lt;script type=&quot;math/tex&quot;&gt;\eta_1^2\gamma_1^2 + \eta_2^2\gamma_2^2 + 2\eta_1\eta_2\gamma_1\gamma_2&lt;/script&gt;十分相似，而后者却等于&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;(\eta_1,\eta_2), (\gamma_1, \gamma_2)&gt;^2 %]]&gt;&lt;/script&gt;，所以只需要令&lt;script type=&quot;math/tex&quot;&gt;\phi(x_1,x_2) = (x_1^2, x_2^2, \sqrt {2} x_1x_2)&lt;/script&gt;，就可以得到&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;\phi(\eta_1,\eta_2),\phi(\gamma_1,\gamma_2)&gt; %]]&gt;&lt;/script&gt;=&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;(\eta_1,\eta_2), (\gamma_1, \gamma_2)&gt;^2 %]]&gt;&lt;/script&gt;~ 由此可以推广到高维。不难看出在高维空间中的内积可以通过在原始空间内积的平方得到~此时&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
K(p,q) = &lt;\phi(\eta_1,\eta_2), \phi(\gamma_1, \gamma_2)&gt; %]]&gt;&lt;/script&gt;=&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;p,q&gt;^2 %]]&gt;&lt;/script&gt;。对偶转换之后只需要将&lt;script type=&quot;math/tex&quot;&gt;x_i,x_j&lt;/script&gt;的内积运算更换成&lt;script type=&quot;math/tex&quot;&gt;K(x_i,x_j)&lt;/script&gt;，即可处理非线性数据~&lt;/p&gt;

&lt;h3 id=&quot;3-smo&quot;&gt;3. SMO&lt;/h3&gt;

&lt;p&gt;SMO本质上上一种坐标上升优化算法，坐标上升可以理解为在&lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;维向量构成的空间中，每次选择一个维度进行优化，最终能够求得比较合适的解。SMO每次选择两个参数，因为此时待求变量有&lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^N\alpha_iy_i =0&lt;/script&gt;的约束。SMO的优化过程如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;选择&lt;script type=&quot;math/tex&quot;&gt;\alpha_i, \alpha_j&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;固定其他参数，然后对&lt;script type=&quot;math/tex&quot;&gt;\alpha_i, \alpha_j&lt;/script&gt;进行优化&lt;/li&gt;
  &lt;li&gt;利用&lt;script type=&quot;math/tex&quot;&gt;\alpha_i, \alpha_j&lt;/script&gt;，对截距进行优化&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在了解确切的&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;贪心选择策略之前，先假定我们已经将&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;选择妥当，然后直接对&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;进行优化。根据条件&lt;script type=&quot;math/tex&quot;&gt;\sum_{k=1}^N\alpha_ky_k=0&lt;/script&gt;，令&lt;script type=&quot;math/tex&quot;&gt;A = y_i\sum_{k!=i,j}^N\alpha_ky_k&lt;/script&gt;. 那么&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;的关系为&lt;script type=&quot;math/tex&quot;&gt;\alpha_i = A - y_iy_j\alpha_j&lt;/script&gt;。此时我们的优化目标为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;max_{\alpha} L(\alpha) = \sum_{k=1}^N\alpha_k - \frac{1}{2} \sum_{l=1}^N\sum_{k=1}^N\alpha_l\alpha_ky_ly_kK_{l,k}&lt;/script&gt;

&lt;p&gt;其中，&lt;script type=&quot;math/tex&quot;&gt;K_{l,k} = K(x_l, x_j)&lt;/script&gt;，令&lt;script type=&quot;math/tex&quot;&gt;B = \sum_{k!=i,j}\alpha_k, S = y_iy_j, V_i = \sum_{k!=i,j}\alpha_ky_kK_{i,k}, V_j =\sum_{k!=i,j}\alpha_ky_kK_{j,k}&lt;/script&gt;，那么有：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align} L(\alpha) &amp;amp; = \sum_{k!=i,j}^N\alpha_k + \alpha_i + \alpha_j \\
&amp;amp; - \frac {1}{2}[\alpha_i\alpha_i y_iy_iK_{i,i} + \alpha_j\alpha_jy_jy_jK_{j,j} + 2\alpha_i\alpha_jy_iy_jK_{i,j} + \\
&amp;amp; 2\sum_{k!=i,j}^N\alpha_i\alpha_ky_iy_kK_{i,k} + 2\sum_{k!=i,j}^N\alpha_j\alpha_ky_jy_kK_{j,k} + \sum_{l!=i,j}^N \sum_{k!=i,j}^N\alpha_l\alpha_ky_ly_kK_{l,k}]\end{align}
$$
&lt;/p&gt;

&lt;p&gt;
$$
\begin{align}
L(\alpha) &amp;amp; = B + A - S\alpha_j + \alpha_j - \frac{1}{2} [2\alpha_i\alpha_jSK_{i,j} + \alpha_i^2K_{i,i} + \alpha_j^2K_{j,j} \\
&amp;amp; + 2\alpha_iy_iV_i + 2\alpha_jy_jV_j +  \sum_{l!=i,j}^N \sum_{k!=i,j}^N\alpha_l\alpha_ky_ly_kK_{l,k}]    \\
&amp;amp; =-S\alpha_j + \alpha_j  - \frac{1}{2}K_{i,i}(A-S\alpha_j)^2 - \frac{1}{2} K_{j,j}\alpha_j^2 - K_{i,j}\alpha_j(A-S\alpha_j)S \\
&amp;amp; - \alpha_jy_jV_j - (A-S\alpha_j)y_iV_i + \varepsilon_{constant} \end{align}
$$
&lt;/p&gt;

&lt;p&gt;其中，&lt;script type=&quot;math/tex&quot;&gt;\varepsilon_{constant}&lt;/script&gt;为一些常量，在求极值点是可以忽略，上式对&lt;script type=&quot;math/tex&quot;&gt;\alpha_j&lt;/script&gt;求导，有：&lt;/p&gt;
&lt;p&gt;
$$
\begin{eqnarray*} \frac {\partial_{L(\alpha_j)}} {\partial_{\alpha_j}} &amp;amp;=&amp;amp; -S + 1 + ASK_{i,i} -K_{i,i}\alpha_j - K_{j,j}\alpha_j -ASK_{i,j} + 2K_{i,j}\alpha_j -y_jV_j + Sy_iV_i=0 \\ \alpha_j&amp;amp;=&amp;amp; \frac {-S + 1 + AS(K_{i,i}-K_{i,j}) + y_j(V_i-V_j)}{K_{i,i} + K_{j,j} - 2K_{i,j}} \end{eqnarray*}
$$
&lt;/p&gt;

&lt;p&gt;在优化&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;的时候，其他参数没有被改变。&lt;/p&gt;
&lt;p&gt;
$$
\begin{eqnarray*}
\alpha_iy_i+\alpha_jy_j&amp;amp;=&amp;amp; -\sum_{k!=i,j}\alpha_k^{old}y_k=\alpha_i^{old}y_i + \alpha_j^{old}y_j \\ 
V_i &amp;amp;=&amp;amp; \sum_{k!=i,j}\alpha_ky_kK_{i,k} \\
&amp;amp;=&amp;amp;\sum_{k!=i,j}\alpha_k^{old}y_kK_{i,k} \\
&amp;amp;=&amp;amp; \sum_{k=1}^N\alpha_k^{old}y_kK_{i,k} + b - b - \alpha_i^{old}y_iK_{i,i} - \alpha_j^{old}y_jK_{i,j} \\
V_j &amp;amp;=&amp;amp;\sum_{k!=i,j}\alpha_ky_kK_{j,k} \\
&amp;amp;=&amp;amp;\sum_{k!=i,j}\alpha_k^{old}y_kK_{j,k} \\
&amp;amp;=&amp;amp; \sum_{k=1}^N\alpha_k^{old}y_kK_{j,k} + b - b - \alpha_j^{old}y_jK_{j,j} - \alpha_i^{old}y_iK_{i,j}  
\end{eqnarray*}
$$
&lt;/p&gt;
&lt;p&gt;且&lt;script type=&quot;math/tex&quot;&gt;g(x_i) = \sum_{k=1}^N\alpha_k^{old}y_kK_{i,k} + b&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;g(x_j) = \sum_{k=1}^N\alpha_k^{old}y_kK_{j,k} + b&lt;/script&gt;，所以：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V_i - V_j = g(x_i) - g(x_j) - \alpha_i^{old}y_iK_{i,i} + \alpha_j^{old}y_jK_{j,j} - \alpha_j^{old}y_jK_{i,j} + \alpha_i^{old}y_iK_{i,j}&lt;/script&gt;

&lt;p&gt;然后，将A = &lt;script type=&quot;math/tex&quot;&gt;\alpha_i^{old}+S\alpha_j^{old}&lt;/script&gt;，S=&lt;script type=&quot;math/tex&quot;&gt;y_iy_j&lt;/script&gt;代入&lt;script type=&quot;math/tex&quot;&gt;\alpha_j&lt;/script&gt;表达式，可得：&lt;/p&gt;

&lt;p&gt;
$$
\begin{eqnarray*}\alpha_j &amp;amp;=&amp;amp; \frac {\{y_jy_j - y_iy_j +(\alpha_i^{old}+y_iy_j\alpha_j^{old})y_iy_j(K_{i,i}-K_{i,j}) \\ 
+ y_j(g(x_i) - g(x_j) - \alpha_i^{old}y_iK_{i,i} + \alpha_j^{old}y_jK_{j,j} - \alpha_j^{old}y_jK_{i,j} + \alpha_i^{old}y_iK_{i,j})\}} {K_{i,i} + K_{j,j} - 2K_{i,j}} \\ &amp;amp;=&amp;amp; \frac {y_j[y_j-y_i + y_j\alpha_j^{old}(K_{i,i} + K_{j,j} - 2K_{i,j} + g(x_i) - g(x_j) ]} {K_{i,i} + K_{j,j} - 2K_{i,j}} \\ &amp;amp;=&amp;amp; \alpha_j^{old} + \frac{y_j[y_j-y_i + g(x_i)-g(x_j)]} {K_{i,i} + K_{j,j} - 2K_{i,j}}\end{eqnarray*}
$$
&lt;/p&gt;

&lt;p&gt;然后令&lt;script type=&quot;math/tex&quot;&gt;E_i = g(x_i) - y_i&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;\eta = K_{i,i} + K_{j,j} - 2K{i,j}&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;\alpha_j^{new,unc} = \alpha_j^{old} + \frac {y_j(E_i - E_j)} {\eta}&lt;/script&gt;，此时求出的&lt;script type=&quot;math/tex&quot;&gt;\alpha_j&lt;/script&gt;还需要经过边界判定，对&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;有&lt;script type=&quot;math/tex&quot;&gt;\alpha_iy_i + \alpha_jy_j = \alpha_i^{old} + \alpha_j^{old}&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt;=\alpha_i&lt;=C %]]&gt;&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt;=\alpha_j&lt;=C %]]&gt;&lt;/script&gt;的条件限制，所以必须对&lt;script type=&quot;math/tex&quot;&gt;\alpha_i, \alpha_j&lt;/script&gt;的上下边界&lt;script type=&quot;math/tex&quot;&gt;L,H&lt;/script&gt;进行确认。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;如果&lt;script type=&quot;math/tex&quot;&gt;y_i = y_j&lt;/script&gt;，那么&lt;script type=&quot;math/tex&quot;&gt;L= max(0, \alpha_j^{old} - \alpha_i^{old})&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;H=min(C,C+\alpha_j^{old} - \alpha_i^{old})&lt;/script&gt; &lt;br /&gt;
如果&lt;script type=&quot;math/tex&quot;&gt;y_i!=y_j&lt;/script&gt;，那么&lt;script type=&quot;math/tex&quot;&gt;L=max(0, \alpha_j^{old} + \alpha_i^{old} - C)&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;H=min(C,\alpha_j^{old} + \alpha_i^{old})&lt;/script&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;根据上式可以得到&lt;script type=&quot;math/tex&quot;&gt;\alpha_j^{new}&lt;/script&gt;为：
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
alpha_j^{new} = \begin{cases} H,~~~\alpha_j^{new,unc} &gt; H \\ \alpha_j^{new,unc},~~~~~ L&lt;=\alpha_j^{new,unc}&lt;=H \\ L,~~~~~\alpha_j^{new,unc} &lt; L \end{cases} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;由此可以得到&lt;script type=&quot;math/tex&quot;&gt;\alpha_i^{new}= \alpha_i^{old} + y_iy_j(\alpha_j^{old} - \alpha_j^{new})&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;关于&lt;script type=&quot;math/tex&quot;&gt;alpha_i, \alpha_j&lt;/script&gt;的更新策略完成，但是对于上式中的bias也需要进行更新，以保证KKT条件&lt;script type=&quot;math/tex&quot;&gt;\alpha_j(y_j(\sum_{i=1}^N\alpha_iy_iK_{i,j} + b) - 1) = 0&lt;/script&gt;成立。&lt;/p&gt;

&lt;p&gt;当&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt; \alpha_i^{new} &lt; C %]]&gt;&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;y_i(\sum_{k=1}^N\alpha_ky_kK_{i,k}+b) = 1&lt;/script&gt; ，所以&lt;script type=&quot;math/tex&quot;&gt;b_1^{new} = y_i - \sum_{k!=i,j}^N\alpha_ky_kK_{i,k} - \alpha_i^{new}y_iK_{i,i} - \alpha_j^{new}y_jK_{i,j}&lt;/script&gt;，又知&lt;script type=&quot;math/tex&quot;&gt;E_i = g(x_i) - y_i = \sum_{k=1}^N\alpha_ky_kK_{i,k} + b^{old} - y_i&lt;/script&gt;，此时除&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;以外的都不会发生变化，所以&lt;script type=&quot;math/tex&quot;&gt;E_i = \sum_{k!=i,j}^N\alpha_ky_kK_{k,i} + \alpha_i^{old}y_iKii + \alpha_j^{old}y_jK_{i,j} + b^{old} - y_i&lt;/script&gt;，也即 &lt;script type=&quot;math/tex&quot;&gt;b_i^{new} = -E_i - y_iK_{i,i}(\alpha_i^{new} - \alpha_i^{old}) - y_jK_{i,j}(\alpha_j^{new} - \alpha_j^{old}) + b^{old}&lt;/script&gt;
同1，当&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt; \alpha_j^{new} &lt; C %]]&gt;&lt;/script&gt;，则&lt;script type=&quot;math/tex&quot;&gt;b_2^{new} = -E_j - y_iK_{i,j}(\alpha_i^{new} - \alpha_i^{old}) - y_jK_{j,j}(\alpha_j^{new} - \alpha_j^{old}) + b^{old}&lt;/script&gt;
如果&lt;script type=&quot;math/tex&quot;&gt;\alpha_i^{new}&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;\alpha_j^{new}&lt;/script&gt;都满足条件，则&lt;script type=&quot;math/tex&quot;&gt;b_1^{new}&lt;/script&gt;=&lt;script type=&quot;math/tex&quot;&gt;b_2^{new}&lt;/script&gt;
如果&lt;script type=&quot;math/tex&quot;&gt;\alpha_i^{new}&lt;/script&gt;、&lt;script type=&quot;math/tex&quot;&gt;\alpha_j^{new}&lt;/script&gt;为0或者C，那么取&lt;script type=&quot;math/tex&quot;&gt;b_1^{new}&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;b_2^{new}&lt;/script&gt;的中值即可
至此，SMO算法结束，不过实际中，需要实时更新&lt;script type=&quot;math/tex&quot;&gt;E_i&lt;/script&gt;，所以在更新完bias之后，再利用已有的信息重新更新&lt;script type=&quot;math/tex&quot;&gt;E_i&lt;/script&gt;即可。&lt;/p&gt;

&lt;p&gt;最终实现见：&lt;a href=&quot;https://github.com/kymo/SUML/tree/master/SVM&quot;&gt;svm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;参考资料&lt;/p&gt;

&lt;p&gt;李航 《统计学习方法》&lt;/p&gt;

&lt;p&gt;http://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html&lt;/p&gt;

</description>
        <pubDate>Sat, 02 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/02/SVM.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/02/SVM.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
  </channel>
</rss>
