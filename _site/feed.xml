<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aron blog</title>
    <description>nihao
</description>
    <link>http://kymo.github.io/</link>
    <atom:link href="http://kymo.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 16 Apr 2017 22:49:36 +0800</pubDate>
    <lastBuildDate>Sun, 16 Apr 2017 22:49:36 +0800</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>循环神经网络的一些小事儿</title>
        <description>&lt;h3 id=&quot;1-从神经网络谈起&quot;&gt;1. 从神经网络谈起&lt;/h3&gt;

&lt;p&gt;了解神经网络的都知道，神经网络作为一种非线性模型，在监督学习领域取得了state-of-art的效果，其中反向传播算法的提出居功至伟，到如今仍然是主流的优化神经网络参数的算法. 递归神经网络、卷积神经网络以及深度神经网络作为人工神经网络的”变种”，仍然延续了ANN的诸多特质，如权值连接，激励函数，以神经元为计算单元等，只不过因为应用场景的不同衍生了不同的特性，如：处理变长数据、权值共享等。&lt;/p&gt;

&lt;p&gt;为了介绍RNN，先简单的介绍ANN. ANN的结构很容易理解，一般是三层结构（输入层-隐含层-输出层）. 隐含层输出&lt;script type=&quot;math/tex&quot;&gt;o_j&lt;/script&gt; 和输出层输出&lt;script type=&quot;math/tex&quot;&gt;o_k&lt;/script&gt;如下。其中&lt;script type=&quot;math/tex&quot;&gt;net_j&lt;/script&gt;为隐含层第&lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;个神经元的输入,&lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt;为输入层和隐含层的连接权值矩阵，&lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;为隐含层和输出层之间的连接权值矩阵.&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
o_j &amp;amp; =f(net_j) \\
o_k &amp;amp; =f(net_k) \\ 
net_j &amp;amp; =\sum_i(x_{i}u_{i,j})+b_j \\
net_k &amp;amp; =\sum_j(o_{j}v_{j,k})+b_k
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;定义损失函数为&lt;script type=&quot;math/tex&quot;&gt;E_p=\frac{1}{2}\sum_k (o_k - d_k)^2&lt;/script&gt; ,其中&lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;为样本下标，&lt;script type=&quot;math/tex&quot;&gt;o^k&lt;/script&gt;为第&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;个输出层神经元的输出,&lt;script type=&quot;math/tex&quot;&gt;d^k&lt;/script&gt;为样本在第$k$个编码值。然后分别对参数&lt;script type=&quot;math/tex&quot;&gt;v_{j,k}&lt;/script&gt;、&lt;script type=&quot;math/tex&quot;&gt;u_{i,j}&lt;/script&gt; 进行求导，可得：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
\frac{\partial E_p}{\partial v_{j,k}} &amp;amp; = \frac{\partial E_p}{\partial net_k} \frac{\partial net_k}{\partial v_{j,k}} \\
&amp;amp; = \frac{\partial E_p}{\partial net_k}o_j \\
&amp;amp; = \frac{\partial E_p}{\partial o_k}\frac{\partial o_k}{\partial net_k}o_j \\
&amp;amp; = (o_k-d_k)o_k(1-o_k)o_j
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;
$$
\begin{align}
\frac{\partial E_p} {\partial u_{i,j}} &amp;amp; = \frac{\partial E_p} {\partial net_j} \frac{\partial net_j} {\partial u_{i,j}} \\
&amp;amp; =x_i \sum_k \frac{\partial E_p} {\partial net_k} \frac{\partial net_k}{\partial o_j} \frac{\partial o_j}{\partial net_j}  \\
&amp;amp; =x_i \sum_k \frac{\partial E_p}{\partial net_k} v_{j,k} o_j(1-o_j) \\
&amp;amp; = x_i o_j(1-o_j) \sum_k \frac{\partial E_p}{\partial net_k} v_{j,k} 
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;从对$\frac{\partial E_p} {\partial u_{i,j}}$的推导可以得到反向传播的核心思路，令误差项$\beta_k = \frac{\partial E_p} {\partial net_k}$, 则有：&lt;/p&gt;

&lt;p&gt;
$$
\beta_k=o_l(1-o_l)\sum_l\beta_lw_{lk}
$$
&lt;/p&gt;

&lt;p&gt;反向传播的实质是基于梯度下降的优化方法，只不过在优化的过程使用了一种更为优雅的权值更新方式。&lt;/p&gt;

&lt;h3 id=&quot;2-循环神经网络&quot;&gt;2. 循环神经网络&lt;/h3&gt;

&lt;p&gt;传统的神经网络一般都是全连接结构，且非相邻两层之间是没有连接的。一般而言，定长输入的样本很容易通过神经网络来解决，但是类似于NLP中的序列标注这样的非定长输入，前向神经网络却无能为力。&lt;/p&gt;

&lt;p&gt;于是有人提出了循环神经网络(Recurrent Neural Network)，这是一种无法像前向神经网络一样有可以具象的网络结构的模型，一般认为是网络隐层节点之间有相互作用的连接，其实质可以认为是多个具有相同结构和参数的前向神经网络的stacking, 前向神经网络的数目和输入序列的长度一致，且序列中毗邻的元素对应的前向神经网络的隐层之间有互联结构，其图示( &lt;a href=&quot;http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/&quot;&gt;图片来源&lt;/a&gt; )如下.
&lt;img src=&quot;http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图只是一个比较抽象的结构，下面是一个以时间展开的更为具体的结构(&lt;a href=&quot;http://www.cnblogs.com/YiXiaoZhou/p/6058890.html&quot;&gt;图片来源&lt;/a&gt;).
&lt;img src=&quot;http://images2015.cnblogs.com/blog/1027162/201611/1027162-20161113162111280-1753976877.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从图中可以看出，输出层神经元的输入输出和前向神经网络中没有什么差异，仅仅在于隐层除了要接收输入层的输入外，还需要接受来自于自身的输入(可以理解为t时刻的隐层需要接收来自于t-1时刻隐层的输入, 当然这仅限于单向RNN的情况，在双向RNN还需要接受来自t+1时刻的输入).&lt;/p&gt;

&lt;p&gt;RNN的隐层是控制信息传递的重要单元，不同时刻隐层之间的连接权值决定了过去时刻对当前时刻的影响，所以会存在时间跨度过大而导致这种影响会削弱甚至消失的现象，称之为梯度消失，改进一般都是针对隐层做文章，LSTM(控制输入量，补充新的信息然后输出)，GRU(更新信息然后输出)等都是这类的改进算法.&lt;/p&gt;

&lt;p&gt;下图为某时刻隐层单元的结构示意图(&lt;a href=&quot;http://www.cnblogs.com/YiXiaoZhou/p/6058890.html&quot;&gt;图片来源&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://images2015.cnblogs.com/blog/1027162/201611/1027162-20161113162105295-307972897.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;虽说处理的是不定长输入数据，但是某个时刻的输入还是定长的。令t时刻:输入$x_t \in R^{xdim}$ ,t隐层输出&lt;script type=&quot;math/tex&quot;&gt;h_t\in R^{hdim}&lt;/script&gt;, 输出层$y_t \in R^{ydim}$, RNN和CNN有着同样的共享权值的属性，输入层到隐层的权值矩阵$V\in R^{xdim\times hdim}$, 隐层到输出层的权值矩阵$W \in R^{hdim\times ydim}$, 不同时刻隐层自连接权值矩阵$U\in R^{hdim\times hdim}$[1]. RNN有着类似于CNN的权值共享特点，所以不同时刻的U,V,W都是相同的，所有整个网络的学习目标就是优化这些参数以及偏置. RNN和普通神经网络一样，也有超过三层的结构，下文的推导仅以三层为例.&lt;/p&gt;

&lt;p&gt;令隐含层的激励函数$f(x)$, 输出层的激励函数为$g(x)$. 则有：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
h^t &amp;amp; = f(net_h^t) \\
net_h^t &amp;amp; = x^tV + h^{t-1}U + b_h \\
y^t &amp;amp; = g(net_y^t) \\
net_y^t &amp;amp; = h^tW + b^y
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;对于单个样本，定义我们的cost function $E^t = \frac{1}{2}||d^t- y^t ||^2$，则对权值$W_{j,k}$、$V_{i,j}$、$U_{j,r}$的求导分别如下，其中$j$表示隐层单元下标,$k$表示输出层下标,$i$表示输入层下标,$r$表示下一时刻隐含层下标.$net^t_{hj}$表示t时刻隐层第j个神经元的加权输入，$net^t_{yk}$表示t时刻输出层第k个神经元的加权输入。&lt;/p&gt;

&lt;p&gt;
$$
\begin{align}
\frac{\partial E} {\partial W_{jk}} &amp;amp; = \frac{\partial E} {\partial net^t_{yk}} \frac{\partial net^t_{yk}} {\partial W_{jk} } \\
&amp;amp; =  \frac{\partial E} {\partial net^t_{yk}} h^t_j \\
&amp;amp; = (d_k^t - y_k^t)y^t_k(1-y^t_k)h_j^t \\

\frac{\partial E} {\partial V_{ij}} &amp;amp; = \frac{\partial E} {\partial net^t_{hj}} \frac{\partial net^t_{hj}} {\partial V_{ij}} \\
&amp;amp; = \frac{\partial E} {\partial net^t_{hj}} x^t_i \\
&amp;amp; = (\sum_k \frac{\partial E}{\partial net^t_{yk}} \frac{\partial net^t_{yk}}{\partial h^t_j} \frac{\partial h^t_j}{\partial net^t_{hj}} +
\color{red}{ \sum_r \frac{\partial E}{\partial net^{t+1}_{hr}} \frac{\partial net^{t+1}_{hr}}{\partial h^{t}_{j}} \frac{\partial h^{t}_{j}} {\partial net^t_{hj}} })x^t_i \\ 
&amp;amp; = (\sum_k \frac{\partial E}{\partial net^t_{yk}} W_{jk} + \color{red} { \sum_r \frac{\partial E}{\partial net^{t+1}_{hr}}U_{jr}} ) \frac{\partial h^{t}_{j}} {\partial net^t_{hj}} x^t_i \\

\frac{\partial E} {\partial U_{jr}} &amp;amp; =  \frac{\partial E} {\partial net^{t+1}_{hr}} \frac{\partial net^{t+1}_{hr}}{\partial U_{jr}} \\
&amp;amp; = \frac{\partial E}{\partial net^{t+1}_{hr}} h^t_j \\
&amp;amp; = (\sum_k \frac{\partial E} {\partial net^{t+1}_{yk}} \frac {\partial net^{t+1}_{yk}} {\partial h^{t+1}_r}  \frac {\partial h^{t+1}_r} {\partial net^{t+1}_{hr}} + \color{red}{\sum_j \frac{\partial E} {\partial net^{t+2}_{hj}} \frac {\partial net^{t+2}_{hj}} {\partial h^{t+1}_r}  \frac {\partial h^{t+1}_r} {\partial net^{t+1}_{hr}}})  h^t_j \\
&amp;amp; = (\sum_k \frac{\partial E} {\partial net^{t+1}_{yk}} W_{rk} + \color{red}{\sum_j \frac{\partial E} {\partial net^{t+2}_{hj}} U_{jr}})  \frac {\partial h^{t+1}_r} {\partial net^{t+1}_{hr}} h^t_j \\
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;令$\delta_{y,k}^t$为t时刻输出层y第k个神经元的误差项，令$\delta_{h,j}^t$为t时刻隐含层h第j个神经元的误差项, 则有:&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
\delta_{y,k}^t &amp;amp; = \frac{\partial E} {\partial net_{yk}^t} \\
\delta_{h,j}^t &amp;amp; = \frac{\partial E} {\partial net_{hj}^t} \\
\delta_{y,k}^t &amp;amp; = (d_k^t - y_k^t)y^t_k(1-y^t_k) \\
\delta_{h,j}^t &amp;amp; = (\sum_k \frac{\partial E}{\partial net^t_{yk}} W_{jk} + \sum_r \frac{\partial E}{\partial net^{t+1}_{hr}}U_{jr}) \frac{\partial h^{t}_{j}} {\partial net^t_{hj}} \\
&amp;amp; = (\sum_k \delta_{y,k}^t W_{jk} + \sum_r \delta_{h,j}^{t+1} U_{jr}) \frac{\partial h^{t}_{j}} {\partial net^t_{hj}} 
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;上述 公式其实在了解反向传播之后就能够很容易推导，对于$W_{jk}$、$U_{jr}$的推导套用反向传播公式即可，而对于$V_{i,j}$需要加上来自于下一时刻的误差，如以上式子中红色部分所示. 对于LSTM，GRU的推导也类似，了解清楚误差传递的来源和去向，就很容易得到对当前参数的推导链式规则了.&lt;/p&gt;

&lt;h3 id=&quot;参考资料&quot;&gt;参考资料&lt;/h3&gt;
&lt;p&gt;[1]&lt;a href=&quot;http://www.cnblogs.com/YiXiaoZhou/p/6058890.html&quot;&gt;RNN求解过程推导与实现&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Fri, 02 Dec 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/12/02/RNN.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/12/02/RNN.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>CRF 笔记</title>
        <description>&lt;h1 id=&quot;crf-笔记&quot;&gt;CRF 笔记&lt;/h1&gt;

&lt;h5 id=&quot;dgm转换成ugm&quot;&gt;DGM转换成UGM&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;节点之间连接&lt;/li&gt;
  &lt;li&gt;节点的双亲连接，有节点都指向同一个节点，说明两者之间有关联&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;两者之间还是有关联的&lt;/p&gt;

</description>
        <pubDate>Tue, 16 Aug 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/08/16/CRF.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/08/16/CRF.html</guid>
        
        <category>CRF</category>
        
        <category>NLP</category>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>论文阅读笔记</title>
        <description>&lt;h3 id=&quot;learning-n-best-correction-models-from-implicit-user-feedback-in-a-muti-modal-local-search-application&quot;&gt;Learning N-best Correction Models from Implicit User Feedback in a Muti-Modal Local Search Application&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;通过用户的反馈对语音识别的结果进行纠错，通过点击数据构造result confusion matrix,然后利用该矩阵对结果进行重排&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通常而言，用户的点击数据可以用来对已有的声学模型或者语言模型进行概率，当然也可以利用这些数据对语言识别的结果进行纠正，本文作者想要构建的就是基于用户数据的纠错模型。&lt;/p&gt;

&lt;p&gt;本文的工作和主流的改写流程差不多，主要包含了两个方面：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;语音识别系统首先会给出n-best候选，不同于一般的候选，这里会增加一部分通过点击数据统计出来的额外的候选。&lt;/li&gt;
  &lt;li&gt;接着进行ReScore，构建更加精确的n-best list
对于第一步，需要从user click data中统计出result confusion matrix,该矩阵揭示了当展现某一个query的时候，用户点击其他query的频次，比如对于识别系统识别出来的n-best list:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;+++ Sterling  Stirling  Burlington  Cooling&lt;/p&gt;

&lt;p&gt;通过点击数据可以构造如下的result confusion matrix:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Bar&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Bowling&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Burgerking&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;…&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Burlington&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Sterling&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Burlington&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;…&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cooling&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;…&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Sterling&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;…&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Stirling&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;…&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;得到这个result confusion matrix之后，就可以将原有的n-best list进行扩充(在矩阵中共现过便可加入），得到如下结果：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Sterling  Striling  Burlington  Cooling  Bar  Bowling   Burgetking&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对于第二步，需要利用矩阵的统计频次.$Score(word_i)=\sum_{i=0}C(word_i, otherword)$，其中$word_i$表示第$i$个n-best list的元素，$otherword$表示用户在展现$word_i$之后点击的$word$.&lt;/p&gt;
</description>
        <pubDate>Tue, 16 Aug 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/08/16/PaperReadingNote.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/08/16/PaperReadingNote.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>sub: 轻量应用服务器</title>
        <description>&lt;p&gt;sub是正在开发的一种轻量的应用服务器, 项目地址: &lt;a href=&quot;https://github.com/kymo/sub&quot;&gt;sub&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;sub&quot;&gt;SUB&lt;/h1&gt;

&lt;h2 id=&quot;socket&quot;&gt;SOCKET&lt;/h2&gt;

&lt;h3 id=&quot;设置为非阻塞模式可以设置selecteoll-io模型将&quot;&gt;设置为非阻塞模式，可以设置SELECT/EOLL IO模型.将&lt;/h3&gt;
&lt;p&gt;Read, Write, Accept作为Reactor函数传递给IO模型&lt;/p&gt;

&lt;h3 id=&quot;有新的连接发过来数据利用解析任务解析该任务然&quot;&gt;有新的连接发过来数据，利用解析任务解析该任务，然&lt;/h3&gt;
&lt;p&gt;后将解析出来的数据按照类型生成新的对应任务丢到队
列中. 任务处理完成之后，设置该FD为写，并将写的容
一并返回.&lt;/p&gt;

&lt;h2 id=&quot;io&quot;&gt;IO&lt;/h2&gt;

&lt;h3 id=&quot;select&quot;&gt;Select&lt;/h3&gt;

&lt;h4 id=&quot;使用read_set和write_set处理连接和读写并接受&quot;&gt;使用read_set和write_set处理连接和读写，并接受&lt;/h4&gt;
&lt;p&gt;server 传递过来的回调函数指针作为事件处理方法&lt;/p&gt;

&lt;h3 id=&quot;epoll&quot;&gt;Epoll&lt;/h3&gt;

&lt;h4 id=&quot;todo&quot;&gt;TODO&lt;/h4&gt;

&lt;h2 id=&quot;taskmgr&quot;&gt;TaskMgr&lt;/h2&gt;

&lt;h3 id=&quot;task&quot;&gt;task&lt;/h3&gt;

&lt;h4 id=&quot;对任务的封装单个任务会有自己运行的回掉函数以&quot;&gt;对任务的封装，单个任务会有自己运行的回掉函数，以&lt;/h4&gt;
&lt;p&gt;及运行时的参数空间,运行run完成之后，这个run会将
数据写回缓冲区. 然后调用call_back函数，将结果写
会fd,不同的任务会继承该task, 计算任务可以由用户指
定运行的回调函数.&lt;/p&gt;

&lt;h3 id=&quot;task_handler&quot;&gt;task_handler&lt;/h3&gt;

&lt;h4 id=&quot;作为线程池的thead_handler每次都会从任务队列&quot;&gt;作为线程池的thead_handler，每次都会从任务队列&lt;/h4&gt;
&lt;p&gt;中取一个任务进行运行&lt;/p&gt;

&lt;h2 id=&quot;threadmgr&quot;&gt;ThreadMgr&lt;/h2&gt;

&lt;h3 id=&quot;thread&quot;&gt;thread&lt;/h3&gt;

&lt;h4 id=&quot;对线程的封装线程运行函数由thread_handler指定&quot;&gt;对线程的封装，线程运行函数由thread_handler指定&lt;/h4&gt;

&lt;h3 id=&quot;thread_handler&quot;&gt;thread_handler&lt;/h3&gt;

&lt;h4 id=&quot;线程真正的实现逻辑不同类型的线程有不同的handl&quot;&gt;线程真正的实现逻辑，不同类型的线程有不同的handl&lt;/h4&gt;
&lt;p&gt;er去处理，对于线程池中的handler会不断的从taskM
gr中取task进行处理&lt;/p&gt;

&lt;h3 id=&quot;thread_pool&quot;&gt;thread_pool&lt;/h3&gt;

&lt;h4 id=&quot;线程池模型根据配置中的线程数启动每个线程分配一&quot;&gt;线程池模型,根据配置中的线程数启动每个线程分配一&lt;/h4&gt;
&lt;p&gt;个handler&lt;/p&gt;

&lt;h2 id=&quot;pluginmgr&quot;&gt;PluginMgr&lt;/h2&gt;

&lt;h2 id=&quot;dictmgr&quot;&gt;DictMgr&lt;/h2&gt;

&lt;h2 id=&quot;configmgr&quot;&gt;ConfigMgr&lt;/h2&gt;

&lt;h2 id=&quot;memorymgr&quot;&gt;MemoryMgr&lt;/h2&gt;
</description>
        <pubDate>Wed, 20 Apr 2016 23:11:02 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/20/SUB.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/20/SUB.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>卷积神经网络的一些小事儿</title>
        <description>&lt;h3 id=&quot;1-从手写体图片识别说起&quot;&gt;1. 从手写体图片识别说起&lt;/h3&gt;

&lt;p&gt;手写体识别一般是神经网络入门必备的demo，在传统的神经网络进行手写体识别的时候，需要更多的人工对特征进行处理，一般是如下的步骤:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1.将原始图片的像素值栅格化为一维向量,如原始图片为8*8, 则栅格化之后的向量为164; &lt;br /&gt;
2.将栅格化的向量作为神经网络的输入，手写体对应的数字编码之后作为神经网络的输出;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;然后在MNIST数据集上也能有96%以上的正确率，但是MNIST数据集给的都是28*28大小的数据，如果图片的像素规模很大呢？这里可能就会考虑两个方案：要么就是直接将全部的像素作为神经网络的输入；要么就是人工去对像素进行采样降维。那么可行性又如何？对于第一种，假设隐层神经元的数量为1000，那么输入层-隐含层权值矩阵就达到了$10^9$的规模，一般的机器根本训不动，更别提用梯度下降这种可能会陷入局部最优的算法了(参数多，解空间大)；对于第二种可能就是无尽的特征提取工作了。&lt;/p&gt;

&lt;p&gt;谈到第二点，深度学习某种意义可以认为是对特征的学习. 领域知识在传统机器学习中十分重要，不管是CTR预估亦或是信用评级、LTR等，都需要在相关领域有较多的经验，往往某几个特征的发现能够给整个系统带来质的飞跃，当然这也使得传统机器学习的解释性更好。&lt;/p&gt;

&lt;h3 id=&quot;2-你的眼睛是怎么看图片的&quot;&gt;2. 你的眼睛是怎么看图片的？&lt;/h3&gt;

&lt;p&gt;通常而言，我们在观察事物的时候，都是会将注意力集中到某一块部分，可能一块还足以让我们判断这个东西的类型，但是多观察几个局部，就能大概知道所观察的东西是个啥了。这个给我们的启发是对于输入的1000*1000像素的图片，我们并不需要所有像素的信息，提取局部的特征也能够给我们勾勒出该图片所代表的事物的特征。这个在卷积神经网络中对应的就是卷积核。另外，对图片进行采样模糊等变换，某种程度上也不会改变图片中事物的主要特征，这个在卷积神经网络中对应的就是pooling，也就是向下采样.&lt;/p&gt;

&lt;h3 id=&quot;3-卷积神经网络&quot;&gt;3. 卷积神经网络&lt;/h3&gt;

&lt;p&gt;那么卷积神经网络是如何处理类似于图片这种结构的输入呢？卷积神经网络中采用了两种十分有效的思路：局部感知和权值共享；前面提到如果直接将图像的像素作为神经网络的输入，需要学习的参数较多。表观上图像所代表的信息通过局部也能够表达，对于神经网络而言，虽然单个神经元只感受到了局部，但是整体的特征已经被整个模型所接收，最后的效果也不会差到哪里去，而随之带来的就是学习参数数目的下降。比如原始图像为1000*1000，隐层为1000000个节点，每个节点只与输入图像的5*5的区域相连，此时需要学习的参数为2.5*10^7，是原先的四万分之一.&lt;/p&gt;

&lt;p&gt;但是这些参数的学习仍然是一个不小的挑战，但是如果隐层节点与输入层区域相连的权值都一样呢，那样子需要学习的参数仅仅为25个，而这个25个参数一般也被称之为卷积核，也有叫滤波器的，总而言之就是对局部特征进行某种线性变换的参数！如此而来，用较小的参数规模实现了对图像局部特征的学习！单个的卷积核造成的问题就是特征提取不够充分，所以一般采用的都是多个卷积核，每种卷积核所体现的就是图像局部不同的特征！卷积过后，如果直接用这些特征进行分类，一是维度过大导致训练不便，二是有可能导致过拟合，所以一般在卷积之后会加一个对卷积层特征的聚合过程，也即Pooling层。&lt;/p&gt;

&lt;p&gt;表达卷积神经网络一般是卷积层与pooling层交替组合，构成深度神经网络，多层卷积能够组合局部特征从而表征全局，像下图中经典的LeNet-5网络结构，就是由3个卷积层和两个pooling层以及一个全连接层组成。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7pn4yt.com1.z0.glb.clouddn.com/blog-lenet.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;卷积层和pooling层可以认为是对特征的学习过程，然后将学习到的特征再输入到一个分类器中进行训练. 在卷积层，是利用卷积核对上一层的特征图进行卷积操作，然后将卷积结果重新输出到新的特征图中，在LeNet5中的C1层中，利用的就是6个5*5的卷积核对输入图像进行卷积，然后得到6个特征图，每个特征图的大小是28*28.&lt;/p&gt;

&lt;p&gt;这里需要注意的是，卷积层的特征图并不仅仅是来自于前一层的一个特征图，有可能是前一层的多个特征图和多个卷积核进行联合操作之后得到，如LeNet-5中Pooling层S2到卷积层C3就是按照如下的排列进行连接的, 其中C3的第一个特征图来自于S2中的0,1,2号3个特征图，最后一个特征图与S2中的所有特征图相连，此时需要60个卷积核. 加上偏置，需要学习的参数为60*5*5+16. &lt;br /&gt;
&lt;img src=&quot;http://images2015.cnblogs.com/blog/743682/201604/743682-20160421101636460-1080820356.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;31-卷积层&quot;&gt;3.1 卷积层&lt;/h4&gt;
&lt;p&gt;令$X_j^l$ 为第$l$层(卷积层)第j个特征图，当$l=1$时即为原始图像特征，$M_j^l$为与第$l$层(卷积层)第$j$个特征图相连的前一层特征图集合的某个子集，那么第l层的第j个特征图的计算：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X_j^l = f(\sum_{i\in M_j^l}K^l_{i,j}*X_i^{l-1} + b_j^l)&lt;/script&gt;

&lt;p&gt;其中，&lt;script type=&quot;math/tex&quot;&gt;*&lt;/script&gt;为卷积运算，$b_j^l$为一维的实数，假设图片像素矩阵$P$，卷积核为$K$, 如下:&lt;/p&gt;

&lt;p&gt;
$$
{\begin{matrix}
P=\begin{bmatrix}
0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 2\\
     1 &amp;amp; 0 &amp;amp; 3 &amp;amp; 3 \\
     3 &amp;amp; 4 &amp;amp; 3 &amp;amp; 0 \\
     4 &amp;amp; 2 &amp;amp; 4 &amp;amp; 4 \\
     
\end{bmatrix} &amp;amp;   K = \begin{bmatrix}
2 &amp;amp; 1 \\ 
1 &amp;amp; -1
\end{bmatrix}
\end{matrix}}
$$
&lt;/p&gt;
&lt;p&gt;那么对图像进行卷积之后的结果$F=K*P$为：&lt;/p&gt;
&lt;p&gt;
$$
F=\begin{bmatrix}
1 &amp;amp; 7 &amp;amp; 11 \\
     10 &amp;amp; 13 &amp;amp; 3 \\
     9 &amp;amp; 9 &amp;amp; 9 \\
\end{bmatrix} 
$$
&lt;/p&gt;
&lt;p&gt;此时需要注意卷积不是直接算矩阵点乘，而是将卷积矩阵旋转180度之后再算点乘。并且F中的元素与P中的元素有如下的对应关系，其中u,v为输出特征图的下标，卷积核大小为d*d，$K_{d-h,d-j}$为卷积核参数$K_{h,j}$旋转180度之后对应的值，$P_{u+h,v+j}$为与输入特征图进行卷积的位置的像素值。&lt;/p&gt;

&lt;p&gt;
$$
F_{u,v} = \sum_{h}\sum_{j} K_{d-h,d-j}*P_{u+h,v+j}
$$
&lt;/p&gt;

&lt;p&gt;卷积核的大小d一般选奇数，卷积时候的步长(一般为1)也可以根据具体的场景进行调整，当然如果为了保证卷积之后维度不变，也可以进行边缘扩展，在原始特征图的边缘补充新的像素值。&lt;/p&gt;

&lt;h4 id=&quot;32-pooling层&quot;&gt;3.2 Pooling层&lt;/h4&gt;
&lt;p&gt;Pooling层主要的作用是降噪降维，缓解过拟合的尴尬。为什么这么说呢？因为就典型的max pooling（取最大）或者averaging pooling(取平均)操作而言，它能够使得模型关注于局部某一部分，而不是具体的像素，模型的鲁棒性和泛化能力得到了一定程度的保障。&lt;/p&gt;

&lt;p&gt;Pooling层的特征图与上一层的卷积层一一对应，只不过特征图的维度变小。令$X_j^l$为Pooling层的第j个特征图，$down()$为向下采样。则有：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X_j^l = f(\beta_j^l \cdot down(X_j^{l-1}) + b_j^l)&lt;/script&gt;

&lt;p&gt;对于Pooling层的输入特征图，其每一个采样局部共享同一个采样尺度和偏置，也即$\beta_j^ll$和$b_j^l$都是一个实数. Pooling层之后，维度一般会减半。假设学习到的采样尺度为0.25, 偏执为0.1，采样窗口为2*2，激励函数f才用的sigmoid函数，则在Pooling层由输入特征图到输出特征图的过程如下：&lt;/p&gt;

&lt;p&gt;
$$
{\begin{matrix}
X_j^{l-1}=\begin{bmatrix}
0 &amp;amp; 2 &amp;amp; 4 &amp;amp; 2\\
     2 &amp;amp; 0 &amp;amp; 3 &amp;amp; 3 \\
     3 &amp;amp; 4 &amp;amp; 3 &amp;amp; 0 \\
     4 &amp;amp; 2 &amp;amp; 4 &amp;amp; 4 \\
     
\end{bmatrix} 
&amp;amp;   
X_j^l = \begin{bmatrix}
f(1.1) &amp;amp; f(3.1) \\ 
f(3.35) &amp;amp; f(2.85)
\end{bmatrix}
\end{matrix}}
$$
&lt;/p&gt;

&lt;h4 id=&quot;33-全连接层&quot;&gt;3.3 全连接层&lt;/h4&gt;

&lt;p&gt;在通过卷积层和Pooling层可以认为是对特征的学习，在这个交替的层次之后，原始的数据被转换到新的特征空间中，然后通过全连接层将特征空间和样本空间进行关联。在通常情况下，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽。&lt;/p&gt;

&lt;h4 id=&quot;34-反向传播推导&quot;&gt;3.4 反向传播推导&lt;/h4&gt;

&lt;p&gt;CNN的推导和基本的全连接神经网络无异，没什么特别的地方，关键在于两点:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;1.卷积层到Pooling层，前向计算的时候由于下采样导致为维度降低，在误差反向传导时，需要对下一层误差进行反向采样 &lt;br /&gt;
2.Pooling层到卷积层，误差项中需要弄清楚和当前卷积核进行卷积的patch.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;令$\delta_j^l$ 为第$l$层第$j$个特征图$X_j^l$对应的误差项，其维度和$X_j^l$一致，$(\delta_j^l)_{u,v}$ 为$X_j^l$ 下标为u,v对应的误差项，即有：&lt;/p&gt;

&lt;p&gt;
$$
\begin{align}
\delta_j^l &amp;amp; = \frac {\partial E}{\partial net_j^l} ; (\delta_j^l)_{u,v} = \frac {\partial E}{\partial ((net_j^l)_{u,v})} \\

net_j^l &amp;amp;= \left\{
\begin{aligned}
&amp;amp; \sum_{i\in M_j^l}K^l_{i,j}*X_i^{l-1} + b_j^l \ \ \ l层为卷积层 \\
&amp;amp; \beta_j^l \cdot down(X_j^{l-1}) + b_j^l \ \ \ l层为Pooling层\\
\end{aligned}
\right.
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;当$l$层为卷积层时&lt;/strong&gt;，需要学习的参数为卷积核和偏执，个数为$\sum_{j}{||M_j||} + N^l$，其中$||M_j||$为与卷积层第j个特征图相连的前层特征图的某个子集，$N^l$为第l层卷积层的特征图数目。&lt;/p&gt;

&lt;p&gt;每一个输出特征图对应一个为偏执$b_j^l$, 且$b_j^l$为标量，其与每一个$(\delta_j^l)_{u,v}$ 都有关联，所以对偏执的求导为：&lt;/p&gt;

&lt;p&gt;
$$
\begin{align}
\frac {\partial E}{\partial b_j^l} &amp;amp; = \sum_u\sum_v \frac {\partial E}{\partial ((net_j^l)_{u,v})} \frac {\partial ((net_j^l)_{u,v})}{\partial b_j^l} =  \sum_u\sum_v (\delta_j^l)_{u,v}
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;每一个输出特征图由若干个卷积核与输入特征图卷积得来，对于卷积核$K_{i,j}^l$而言，其求导的结果为一个$d*d$的矩阵，对于其中的某个值$(K_{i,j})_{r,s}$求导为, 其中r,s为卷积核中的下标，大小在[0,d-1]范围内，u,v为输出特征图的下标：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
\frac {\partial E}{\partial ((K_{i,j}^l)_{r,s})} &amp;amp; = \sum_u\sum_v \frac {\partial E}{\partial ((net_j^l)_{u,v})} \frac {\partial ((net_j^l)_{u,v})}{\partial ((K_{i,j}^l)_{r,s})} \\ 
&amp;amp; = \sum_u\sum_v (\delta_j^l)_{u,v}\frac {\partial ((net_j^l)_{u,v})}{\partial ((K_{i,j}^l)_{r,s})} \\
&amp;amp; = \sum_u\sum_v (\delta_j^l)_{u,v}\frac {\partial ((X_i^{l-1} * K_{i,j}^l)_{u,v})}{\partial ((K_{i,j}^l)_{r,s})}
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;此时又有：&lt;/p&gt;
&lt;p&gt;
$$
(X_i^{l-1} * K_{i,j}^l)_{u,v} = \sum_{r}\sum_{s} (K_{i,j}^l)_{r,s}(X_i^{l-1})_{u+r,v+s}
$$
&lt;/p&gt;
&lt;p&gt;所以：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
\frac {\partial E}{\partial ((K_{i,j}^l)_{r,s})} &amp;amp; = \sum_u\sum_v (\delta_j^l)_{u,v} (X_i^{l-1})_{u+r,v+s}
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;也即：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
\frac {\partial E}{\partial K_{i,j}^l} &amp;amp; = \sum_u\sum_v (\delta_j^l)_{u,v} (PT_i^{l-1})_{u,v}
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;其中，$PT_i^{l-1}$为$X_i^{l-1}$中与卷积核进行卷积的部分，可以认为是一个4维的矩阵，$(PT_i^{l-1})_{u,v}$对应于$X_i^{l-1}$中$row\in [u,u+d-1] \ \ col\in [v,v+d-1]$范围内的部分.&lt;/p&gt;

&lt;p&gt;卷积层的误差项来自于下一层与其相连的Pooling层，此时有:&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
\left\{
\begin{aligned}
net_j^{l+1} &amp;amp; = \beta_j^{l+1} \circ down(X_j^{l}) + b_j^{l+1} \\
X_j^{l} &amp;amp; = f(net_j^l) \\
\end{aligned}
\right.
\end{align}
$$
&lt;/p&gt;
&lt;p&gt;所以：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
\frac {\partial E}{\partial net_j^l} &amp;amp; = \frac {\partial E}{\partial net_j^{l+1}} \frac {\partial E}{\partial net_j^{l+1}}
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;当$l$层为Pooling层时&lt;/strong&gt;，需要学习的参数为偏执和采样尺度，每一个特征图对应两个参数。对偏执的求导与卷积层无异，对采样尺度$\beta_j^l$的求导如下：&lt;/p&gt;

&lt;p&gt;
$$
\begin{align}
\frac {\partial E}{\partial \beta_j^l} &amp;amp; = \sum_u\sum_v \frac {\partial E}{\partial ((net_j^l)_{u,v})} \frac {\partial ((net_j^l)_{u,v})}{\partial \beta_j^l} \\
&amp;amp; =  \sum_u\sum_v (\delta_j^l)_{u,v} down(X_j^{l-1})_{u,v} \\
&amp;amp; = \sum_u\sum_v (\delta_j^l \circ down(X_j^{l-1}))_{u,v}
\end{align}
$$
&lt;/p&gt;

&lt;h3 id=&quot;35-应用&quot;&gt;3.5 应用&lt;/h3&gt;

&lt;h3 id=&quot;参考文献&quot;&gt;参考文献&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/xuanyuansen/article/details/41800721&quot;&gt;卷积神经网络（一）：LeNet5的基本结构&lt;/a&gt; &lt;br /&gt;
&lt;a href=&quot;http://blog.csdn.net/kaido0/article/details/53161684&quot;&gt;理解结构，LeNet5介绍&lt;/a&gt; &lt;br /&gt;
&lt;a href=&quot;http://dataunion.org/11692.html&quot;&gt;技术向：一文读懂卷积神经网络CNN&lt;/a&gt;
&lt;a href=&quot;https://www.zhihu.com/question/41037974&quot;&gt;全连接层的作用是什么？&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 20 Apr 2016 23:11:02 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/20/CNN.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/20/CNN.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>Ensemble Learning Methods</title>
        <description>&lt;p&gt;集成学习的关键是弱学习器的组合，目标是为了提升模型的性能，一般用在Model Selection，此时降低了选择弱名的可能性。当然集成学习也广泛的应用于结果置信度检验、特征选择、数据融合、增量学习等方面，取得了良好的效果。&lt;/p&gt;

&lt;p&gt;一般而言，对于给定的任务，比如分类，首先在处理完特征之后，我们需要考虑的是选择何种模型来对这些样本进行训练。一般我们没有样本数据产生过程的先验知识，只能通过手动的选择调参，才有可能得到一个较好拟合已知样本的模型。集成学习给了一个非常完美的模型选择的解决方案，通过训练多个子模型，组成committee，最后综合决策，达到了自动选择模型的效果。&lt;/p&gt;

&lt;p&gt;一般模型的性能某种程度取决于数据集的大小，也即观测样本的覆盖度。数据集过大对于一般模型可能会导致训练过拟合或者训练性能瓶颈，此时则可以切分数据集，在子数据集上单独训练模型，然后按照某种组合策略构建committee。数据集过小对于一般模型则会欠拟合，此时可以使用bootstrap等抽样方法，在每一份抽样样本中单独训练模型，组成committee。&lt;/p&gt;

&lt;p&gt;一般而言，样本的分类边界十分复杂，如果仅仅使用一般的模型，则很难学习出能够较好拟合样本的分类边界。
但是如果使用集成学习框架，通过学习多个子模型，则能够很好的学习出该分类边界。&lt;/p&gt;

&lt;p&gt;另外，如果我们收集到的数据来源很多，导致数据的类型、维度不同，如果将所有来源的数据都放在同一个向量中，可能会降低模型的学习能力。
但是假如我们在不同来源的数据中根据该数据特性或者其他先验知识单独训练模型，则效果一般都会好很多。
今天看了一篇关于新浪微博垃圾用户检测的论文，研究现状里就提到了采用这种思想的方法，通过观测，发现垃圾用户一般有三种行为：广告、重复转发和恶意关注，然后根据这三种类型去抽取特征，得到三种类型的特征向量之后，然后分别在三种特征上进行子模型训练，最后效果也还不错。&lt;/p&gt;

&lt;p&gt;此外，集成学习框架还可以用来对结果进行评估，可以根据committee中的子模型的vote去计算。&lt;/p&gt;

&lt;p&gt;有人总结了使用ensemble learning的三大原因：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;统计学，已知样本无法完成的表达原始数据的生成分布，通过bootstrap等方法，可以尽量的拟合原始分布训练模型。&lt;/li&gt;
  &lt;li&gt;可计算，体现于模型选择，通过模型融合的解决方案将缺乏模型先验知识的人从单模型调优中解放出来。&lt;/li&gt;
  &lt;li&gt;任意拟合，单模型无法拟合出复杂分类边界，则模型融合则很好的解决了这一问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;参考文献 &lt;a href=&quot;http://www.scholarpedia.org/article/Ensemble_learning&quot;&gt;http://www.scholarpedia.org/article/Ensemble_learning&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 03 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/03/EnsembleLearningMethods.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/03/EnsembleLearningMethods.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>Probability Latent Semantic Analysis</title>
        <description>&lt;p&gt;pLSA是一种主题模型(Topic Model)，全称概率潜在语义分析，是一种将文本的高维稀疏向量表示成低位维稠密向量的映射方法。
它是一种无监督学习方法，假设整个文档集合是由若干个主题组成，某一篇文章都以一定的概率&lt;script type=&quot;math/tex&quot;&gt;p(z \mid d)​&lt;/script&gt;属于某一主题;在给定主题的情况下，以&lt;script type=&quot;math/tex&quot;&gt;p(w \mid z)​&lt;/script&gt;的概率产生文档的词。
pLSA是基于这样一个假设，假设文档和主题的分布以及主题和词汇的分布都是多项式分布，因此一个词汇的生成过程可以表示为：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;以一定的概率生成一篇文档&lt;/li&gt;
  &lt;li&gt;在该文档中按照文档主题分布选择一个主题&lt;/li&gt;
  &lt;li&gt;在该主题下面，按照主题-词汇分布生成一个词汇&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可以得到如下模型&lt;script type=&quot;math/tex&quot;&gt;p(w,d)=p(d)p(w \mid d); p(w \mid d)=\sum_{z=1}p(z \mid d)p(w \mid z)&lt;/script&gt;，其中z为主题，w为词，d为文档。
其实这个过程也很容易理解，假设现在我们要写篇paper，在动笔之前，会列好提纲，大概选择几个点(也就是主题)。
在完善这些主题的过程中，我们会在积累的词典中去选择和这个主题相适应的词进行修饰。
比如写到关于pLSA的文章的时候，我们会选择”建模、概率、主题模型、极大似然“等词去完善这个主题。
在得到模型之后，我们就需要进行Model Inference，也就是参数估计了，pLSA作为概率图模型，一般是采用MLE作为inference的tool。
首先构造极大似然函数：&lt;script type=&quot;math/tex&quot;&gt;L(\theta) = \prod_{i=1}^N\prod_{j=1}^Np(d_i,w_j)^{n(d_i,w_j)}&lt;/script&gt;，其中&lt;script type=&quot;math/tex&quot;&gt;n(d_i,w_j)&lt;/script&gt;表示的是词&lt;script type=&quot;math/tex&quot;&gt;w_j&lt;/script&gt;在和文档&lt;script type=&quot;math/tex&quot;&gt;d_i&lt;/script&gt;的共现频率，由于我们&lt;script type=&quot;math/tex&quot;&gt;p(w \mid z)&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;p(z \mid d)&lt;/script&gt;都是多项式分布，&lt;br /&gt;
接着，是参数估计，一般而言，此时可以选用极大似然估计。极大对数似然函数如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\theta) = \prod_{i=1}^N\prod_{j=1}^Np(d_i,w_j)^{n(d_i,w_j)}&lt;/script&gt;

&lt;p&gt;
$$
\begin{align} ln(L(\theta)) = \\
&amp;amp; = \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)logp(d_i,w_j) \\
&amp;amp; =  \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)log(p(d_i)\sum_{k=1}^Kp(z_k \mid d_i)p(w_j \mid z_k)) \\
&amp;amp; = \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)log(p(d_i))+\sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)log(\sum_{k=1}^Kp(z_k\mid d_i)p(w_j \mid z_k))
\end{align}
$$
&lt;p&gt;
如果直接求导的话，很明显就得需要求解(n+m)个方程，几乎是不可能完成的任务。
一般采用最大期望算法(Maximum Expection， ME)，EM算法的基本思想很直观，即随机初始化相关参数；随后，在E步，计算潜在变量的后验概率；M步，
利用潜在变量的后验概率去更新未知参数。循环迭代直至达到最大迭代次数算法退出。

### 问题：为什么可以收敛？

在e步
&lt;p&gt;
$$ p(z_k \mid w_j, d_i) = \frac{p(z_k \mid w_j)p(w_j \mid z_k)} {\sum_{k=1}^Kp(z_k \mid w_j)p(w_j\mid z_k)} $$
 &lt;/p&gt;
M步，最大化极大似然

$$ L(\theta) = \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)log(\sum_{k=1}^Kp(z_k\mid d_i)p(w_j \mid z_k)) $$


又由于此时：

$$ \sum_{k=1}^K p(z_k|d_i)p(w_j|z_k) &amp;gt; \sum_{k=1}^K p(z_k \mid d_i) p(z_k \mid d_i )p(w_j\mid z_k); \sum_{k=1}^Kp(z_k \mid d_i,w_j) = 1 $$

所以：

$$ L(\theta) &amp;gt; \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)log( \sum_{k=1}^K p(z_k \mid d_i) p(z_k \mid d_i )p(w_j\mid z_k) ) $$


利用Jensen不等式，对于凸函数$$f(x)$$有：$$ f(\sum_{i=1}^Nw_ix_i)&amp;gt;=\sum_{i=1}^Nw_if(x_i) $$可以得到：

$$ L(\theta) &amp;gt; \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)\sum_{k=1}^K p(z_k \mid d_i) log(p(z_k \mid d_i )p(w_j\mid z_k) ) = L'(\theta)$$

此时即转化为，对于约束条件：$$\sum_{k=1}^Kp(z_k|d_i)=1[1],\sum_{j=1}^Mp(w_j|z_k)=1[2]$$ ，求$$L'(\theta)$$的最大值。
对于这种带等式约束的优化问题，很显然可以构造拉格朗日乘法进行求解：

$$H = L'(\theta) + \sum_{i=1}^N\alpha_i(\sum_{k=1}^Kp(z_k|d_i)-1)+\sum_{k=1}^K\beta_k(\sum_{j=1}^Mp(w_j|z_k)-1)$$

分别对 $$p(z_k\mid d_i),p(w_j\mid z_k)$$求导可得：

$$ =\sum_{j=1}^Mn(d_i,w_j)p(z_k|d_i,w_j)-\alpha_ip(z_k|d_i) = 0      [3] \\
	= \sum_{i=1}^Nn(d_i,w_j)p(z_k|d_i,w_j)-\beta_kp(w_j|z_k) = 0     [4]
$$
联立方程组[1],[2],[3],[4]求解可得：

$$ p(z_k|d_i) = \frac {\sum_{i=1}^N n(d_i,w_j)p(z_k|d_i,w_j)}{\sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)p(z_k|d_i,w_j)} $$

$$ p(w_j|z_k) = \frac {\sum_{j=1}^Mn(d_i,w_j)p(z_k|d_i,w_j)}{\sum_{j=1}^M\sum_{k=1}^Kn(d_i,w_j)p(z_k|d_i,w_j)} \\
 = \frac{\sum_{j=1}^Mn(d_i,w_j)p(z_k|d_i,w_j)}{n(d_i)} 
$$

因此，在M步更求新参数，通过不断迭代，如果求得的极大似然趋近收敛，则训练结束。

总结一下：

- E步：求当前估计的参数条件下的后验概率：p(z \mid w,d)
- M步：最大化complete data(加上隐藏变量主题)极大似然估计的期望
- E步和M步循环迭代，直至收敛。

实现的源码见：plsa.cpp plsa.h ，由于切词用到了公司的lib库，所以切词的初始化和切词的过程在源码中被删去，可以去中科院nlp主页获取相关模块。



[jekyll]: http://jekyllrb.com
[jekyll-gh]: https://github.com/jekyll/jekyll
[jekyll-help]: https://github.com/jekyll/jekyll-help

&lt;/p&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/naturallanguageprocessing/2016/04/02/pLSA.html</link>
        <guid isPermaLink="true">http://kymo.github.io/naturallanguageprocessing/2016/04/02/pLSA.html</guid>
        
        
        <category>NaturalLanguageProcessing</category>
        
      </item>
    
      <item>
        <title>Support Vector Machine</title>
        <description>&lt;p&gt;Support Vector Machine ，支持向量机，通常用来进行classification，但是也有做regression。SVM在面对非线性问题上具有独特的优势。本文从linear和nonlinear两种情况下对SVM的建模过程、优化目标的求解推导过程以及优化算法SMO进行阐述。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a^2 + b^2 = c^2&lt;/script&gt;

&lt;h3 id=&quot;1-linear-svm&quot;&gt;1. linear SVM&lt;/h3&gt;

&lt;p&gt;在分类任务中，样本label为&lt;script type=&quot;math/tex&quot;&gt;{-1,1}&lt;/script&gt;，关于从sign distance转换到geometry distance的过程其实很容易理解，
sign distance可以衡量某个样本被分类的置信，如果sign distance越大，那么该样本被分为该类别的可信度就更大；
而geometry distance可以理解为样本距离超平面&lt;script type=&quot;math/tex&quot;&gt;Y = w^TX + b&lt;/script&gt;的距离，是sign distance归一化的结果，
求解目标为&lt;script type=&quot;math/tex&quot;&gt;argmax(\frac { \mid w^Tx + b \mid } { \mid  \mid w \mid  \mid })&lt;/script&gt;，并且需要满足约束：&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i + b) \geq 1&lt;/script&gt;，为了求解方便，可以不加证明的令&lt;script type=&quot;math/tex&quot;&gt;\mid w^Tx+b \mid =1&lt;/script&gt;，形式化如下：&lt;/p&gt;

&lt;p&gt;
$$
\begin{cases} 
argmax \frac{1}{ \mid  \mid w \mid  \mid } \\ 
y_i(w^Tx_i+b) \geq 1   ~~~ i=1,2,\cdot \cdot ,n 
\end{cases} 
$$
&lt;/p&gt;
&lt;p&gt;而此时最大化 &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{ \mid  \mid w \mid  \mid }&lt;/script&gt;等价于最小化 &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; ，并且最小化和最小化&lt;script type=&quot;math/tex&quot;&gt;\frac {1}{2} w^Tw&lt;/script&gt;等价，&lt;/p&gt;

&lt;p&gt;所以1.1可以变为：&lt;/p&gt;
&lt;p&gt;
$$
\begin{cases}
argmin \frac {1}{2} w^Tw \\
y_i(w^Tx_i+b) \geq 1 ~~~ i=1,2,\cdot \cdot ,n 
\end{cases}
$$
&lt;/p&gt;
&lt;p&gt;由此可以得到不等式约束问题，原始问题通过分析不难发现，求解十分困难，不过对于PSO(粒子群算法)而言，往往可以求得比较好的解。不过在碰到这种带不等式约束的问题的时候，我们可以通过拉格朗日对偶性质将原始问题转化为对偶问题，在最大熵模型中也有类似的处理过程。首先构造拉格朗日函数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(w,b,\alpha) = \frac {1} {2} w^Tw - \sum_{i=1}^N a_i[y_i(w^Tx_i + b) - 1]&lt;/script&gt;

&lt;p&gt;首先我们可以得到这个函数的等价形式，令&lt;script type=&quot;math/tex&quot;&gt;\theta = argmax_\alpha L(w,b,\alpha)&lt;/script&gt;，那么：&lt;/p&gt;

&lt;p&gt;
$$
\theta=
\begin{cases}\frac {1} {2} w^Twy_i(w^Tx_i+b) \geq 1 \\
\infty &amp;lt; y_i(w^Tx_i+b) &amp;lt; 1
\end{cases}
$$
&lt;/p&gt;

&lt;p&gt;可见，&lt;script type=&quot;math/tex&quot;&gt;min\theta​&lt;/script&gt;和原始优化目标&lt;script type=&quot;math/tex&quot;&gt;argmin \frac {1}{2}w^Tw​&lt;/script&gt;等价。令&lt;script type=&quot;math/tex&quot;&gt;p=min\theta​&lt;/script&gt;，其对偶形式&lt;script type=&quot;math/tex&quot;&gt;d=max_{\alpha}min_{w,b}L(w,\alpha)​&lt;/script&gt;，那么必然会有&lt;script type=&quot;math/tex&quot;&gt;p \geq d​&lt;/script&gt;。此时令&lt;script type=&quot;math/tex&quot;&gt;g=argmin_{w} L(w,b,\alpha)​&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;g \leq L(w^*,b,\alpha) \leq \frac {1}{2} {w^*}^Tw^* = p​&lt;/script&gt;，所以&lt;script type=&quot;math/tex&quot;&gt;p \geq d​&lt;/script&gt;。所以此时该算法满足弱对偶，我们可以通过求解该弱对偶问题去近似求解原始问题，在EM中就是不断优化极大似然下界~ 并且可以知道的是，不管原始问题是何种优化，对偶问题都会是凸优化，也即都会存在极值。不过在SVM中，我们是可以把弱对偶加强，变成strong duality，也即&lt;script type=&quot;math/tex&quot;&gt;p = d​&lt;/script&gt;，优化对偶问题等价于对原始问题的求解。那么怎么判断该对偶问题是强对偶问题呢？KKT条件。如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}  \bigtriangledown L(w,b,\alpha) = 0 \\ \alpha_i(y_i(w^Tx_i+b) - 1) = 0 \\ \alpha_i \geq 0 \end{cases}&lt;/script&gt;

&lt;p&gt;很明显，此时KKT条件成立，所以满足强对偶。其中&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i+b)-1) = 0&lt;/script&gt;但此时KKT只是必要条件，不过由于我们的原始问题是凸优化，所以KKT便是充要条件了。&lt;/p&gt;

&lt;p&gt;对对偶问题，首先是&lt;script type=&quot;math/tex&quot;&gt;min_{w,b}L(w,b,\alpha)&lt;/script&gt;，分别对&lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;求导，得：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac {\partial L(w,\alpha,b)} {\partial(w)} = w - \sum_{i=1}^N \alpha_iy_ix_i=0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac {\partial L(w,\alpha,b)} {\partial(b)} = \sum_{i=1}^N\alpha_iy_i=0&lt;/script&gt;

&lt;p&gt;随后，将&lt;script type=&quot;math/tex&quot;&gt;w=\sum_{i=1}^N\alpha_iy_ix_i&lt;/script&gt;代入&lt;script type=&quot;math/tex&quot;&gt;L(w,b,\alpha)&lt;/script&gt;中，则有：&lt;/p&gt;
&lt;p&gt;
$$
\begin{eqnarray*}
L(w,\alpha,b)&amp;amp;=&amp;amp;\frac {1}{2} w^Tw - \sum_{i=1}^N\alpha_i[y_i(w^Tx_i+b)-1]\\&amp;amp;=&amp;amp;\frac{1}{2}\sum_{i=1}^N\alpha_iy_ix_i^T\sum_{j=1}^N\alpha_jy_jx_j - \sum_{i=1}^N\alpha_i[y_i(w^Tx_i+b)-1]\\ &amp;amp;=&amp;amp;\frac {1}{2} \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j - \sum_{i=1}^N\alpha_i\sum_{j=1}^N\alpha_jy_jx_j^Tx_i + \sum_{i=1}^N\alpha_i\\&amp;amp;=&amp;amp;-\frac {1}{2} \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j + \sum_{i=1}^N\alpha_i\end{eqnarray*}
$$
&lt;/p&gt;

&lt;p&gt;所以：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}-\frac {1}{2} \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j + \sum_{i=1}^N\alpha_i\\s.t~~~\sum_{i=1}^N\alpha_iy_i=0 
\end{cases}&lt;/script&gt;

&lt;p&gt;到这，我们还没有考虑soft margin。实际情况中，总是会存在一定的噪声数据，使得我们的分类超平面被这些噪声数据所误导，从而使得模型的variance增大，所以一般来讲都会采用soft margin构建优化函数，我们以&lt;script type=&quot;math/tex&quot;&gt;\varepsilon&lt;/script&gt;的范围容许一定的误差，即原来的&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i+b) \geq 1&lt;/script&gt;此时为&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i + b) \geq 1 - \varepsilon_i&lt;/script&gt;，所以我们的优化目标变为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}argmin \frac {1}{2} w^Tw + C\sum_{i=1}^N\varepsilon_i \\s.t ~~ y_i(w^Tx_i+b) \geq 1 - \varepsilon_i ~~i=1,2,\cdot \cdot ,n \\ \sum_{i=1}^N\varepsilon_i \geq C \end{cases}&lt;/script&gt;

&lt;p&gt;关于上式，可以看成是利用hinge loss加l2范数正则项的结果，SVM此时的损失函数可以表示为 &lt;script type=&quot;math/tex&quot;&gt;min_{w,b} \sum_{i=1}^N [1-y_i(w^Tx_i+b)]_{+} + \lambda\ \mid w\ \mid ^2&lt;/script&gt;，其中如果&lt;script type=&quot;math/tex&quot;&gt;z&gt;0&lt;/script&gt;，那么&lt;script type=&quot;math/tex&quot;&gt;z_{+}&lt;/script&gt;=z，否则等于0。如果令&lt;script type=&quot;math/tex&quot;&gt;\varepsilon_i=1-y_i(w^Tx_i+b),\varepsilon_i \geq 0&lt;/script&gt;，那么此时最优化问题为 &lt;script type=&quot;math/tex&quot;&gt;min_{w,b} \sum_{i=1}^N\varepsilon_i + \lambda\ \mid w\ \mid ^2&lt;/script&gt;，如果取&lt;script type=&quot;math/tex&quot;&gt;\lambda=\frac {1}{2C}&lt;/script&gt;，那么就和上述优化目标等价，所以可以看出，软间隔实际上是在ERM的基础上加了SRM~&lt;/p&gt;

&lt;p&gt;之后的推导没有多大差别，只不过在求导的过程中出现了&lt;script type=&quot;math/tex&quot;&gt;\alpha_i = C - \varepsilon_i&lt;/script&gt;，又有前面在对偶转换使用的KKT条件之一&lt;script type=&quot;math/tex&quot;&gt;\alpha_i(y_i(w^Tx_i+b) - 1) =0&lt;/script&gt;可得，在分类超平面上的点，也即满足&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i+b)-1 + \varepsilon_i = 0&lt;/script&gt;，而那些不在超平面的点，必然有&lt;script type=&quot;math/tex&quot;&gt;\alpha_i=0&lt;/script&gt;，而在超平面上的，则有&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt; \alpha_i &lt; C %]]&gt;&lt;/script&gt;，在超平面之外的则是&lt;script type=&quot;math/tex&quot;&gt;\alpha_i=C&lt;/script&gt;。&lt;/p&gt;

&lt;h3 id=&quot;2-nonlinear-svm&quot;&gt;2. nonlinear SVM&lt;/h3&gt;

&lt;p&gt;非线性情况之下，利用线性曲线去拟合，明显会产生underfitting，但是我们可以通过函数映射的方式，将原来空间中的非线性特征，映射到高维空间中，使得样本可分或者近似可分，实际上是，机器学习中有一种叫做基展开的技术，就是处理这种线性到非线性的特征映射。不过对于SVM中使用这种非线性变化是因为它能够和核函数配合的天衣无缝。&lt;/p&gt;

&lt;p&gt;这里用一个简单的例子作简要说明。对于&lt;script type=&quot;math/tex&quot;&gt;x=(x_1,x_2)&lt;/script&gt;二维空间的某个点，我们将其映射到三维空间。所利用的映射函数可以为&lt;script type=&quot;math/tex&quot;&gt;\phi (x_1,x_2) = (x_1^2,x_2^2,2x_1x_2)&lt;/script&gt;，那么在三维空间中，样本线性可分的可能性更大，但是计算开销却上升了，因为在转化成对偶问题之后就产生了向量内积运算。对于原始二维空间中的两点&lt;script type=&quot;math/tex&quot;&gt;p=(\eta_1,\eta_2),q=(\gamma_1,\gamma_2)&lt;/script&gt;，在三维空间中的向量内积为&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;\phi(\eta_1,\eta_2),\phi(\gamma_1,\gamma_2)&gt; = \eta_1^2\gamma_1^2 + \eta_2^2\gamma_2^2 + 4\eta_1\eta_2\gamma_1\gamma_2 %]]&gt;&lt;/script&gt;，这和&lt;script type=&quot;math/tex&quot;&gt;\eta_1^2\gamma_1^2 + \eta_2^2\gamma_2^2 + 2\eta_1\eta_2\gamma_1\gamma_2&lt;/script&gt;十分相似，而后者却等于&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;(\eta_1,\eta_2), (\gamma_1, \gamma_2)&gt;^2 %]]&gt;&lt;/script&gt;，所以只需要令&lt;script type=&quot;math/tex&quot;&gt;\phi(x_1,x_2) = (x_1^2, x_2^2, \sqrt {2} x_1x_2)&lt;/script&gt;，就可以得到&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;\phi(\eta_1,\eta_2),\phi(\gamma_1,\gamma_2)&gt; %]]&gt;&lt;/script&gt;=&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;(\eta_1,\eta_2), (\gamma_1, \gamma_2)&gt;^2 %]]&gt;&lt;/script&gt;~ 由此可以推广到高维。不难看出在高维空间中的内积可以通过在原始空间内积的平方得到~此时&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
K(p,q) = &lt;\phi(\eta_1,\eta_2), \phi(\gamma_1, \gamma_2)&gt; %]]&gt;&lt;/script&gt;=&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;p,q&gt;^2 %]]&gt;&lt;/script&gt;。对偶转换之后只需要将&lt;script type=&quot;math/tex&quot;&gt;x_i,x_j&lt;/script&gt;的内积运算更换成&lt;script type=&quot;math/tex&quot;&gt;K(x_i,x_j)&lt;/script&gt;，即可处理非线性数据~&lt;/p&gt;

&lt;h3 id=&quot;3-smo&quot;&gt;3. SMO&lt;/h3&gt;

&lt;p&gt;SMO本质上上一种坐标上升优化算法，坐标上升可以理解为在&lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;维向量构成的空间中，每次选择一个维度进行优化，最终能够求得比较合适的解。SMO每次选择两个参数，因为此时待求变量有&lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^N\alpha_iy_i =0&lt;/script&gt;的约束。SMO的优化过程如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;选择&lt;script type=&quot;math/tex&quot;&gt;\alpha_i, \alpha_j&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;固定其他参数，然后对&lt;script type=&quot;math/tex&quot;&gt;\alpha_i, \alpha_j&lt;/script&gt;进行优化&lt;/li&gt;
  &lt;li&gt;利用&lt;script type=&quot;math/tex&quot;&gt;\alpha_i, \alpha_j&lt;/script&gt;，对截距进行优化&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在了解确切的&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;贪心选择策略之前，先假定我们已经将&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;选择妥当，然后直接对&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;进行优化。根据条件&lt;script type=&quot;math/tex&quot;&gt;\sum_{k=1}^N\alpha_ky_k=0&lt;/script&gt;，令&lt;script type=&quot;math/tex&quot;&gt;A = y_i\sum_{k!=i,j}^N\alpha_ky_k&lt;/script&gt;. 那么&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;的关系为&lt;script type=&quot;math/tex&quot;&gt;\alpha_i = A - y_iy_j\alpha_j&lt;/script&gt;。此时我们的优化目标为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;max_{\alpha} L(\alpha) = \sum_{k=1}^N\alpha_k - \frac{1}{2} \sum_{l=1}^N\sum_{k=1}^N\alpha_l\alpha_ky_ly_kK_{l,k}&lt;/script&gt;

&lt;p&gt;其中，&lt;script type=&quot;math/tex&quot;&gt;K_{l,k} = K(x_l, x_j)&lt;/script&gt;，令&lt;script type=&quot;math/tex&quot;&gt;B = \sum_{k!=i,j}\alpha_k, S = y_iy_j, V_i = \sum_{k!=i,j}\alpha_ky_kK_{i,k}, V_j =\sum_{k!=i,j}\alpha_ky_kK_{j,k}&lt;/script&gt;，那么有：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align} L(\alpha) &amp;amp; = \sum_{k!=i,j}^N\alpha_k + \alpha_i + \alpha_j \\
&amp;amp; - \frac {1}{2}[\alpha_i\alpha_i y_iy_iK_{i,i} + \alpha_j\alpha_jy_jy_jK_{j,j} + 2\alpha_i\alpha_jy_iy_jK_{i,j} + \\
&amp;amp; 2\sum_{k!=i,j}^N\alpha_i\alpha_ky_iy_kK_{i,k} + 2\sum_{k!=i,j}^N\alpha_j\alpha_ky_jy_kK_{j,k} + \sum_{l!=i,j}^N \sum_{k!=i,j}^N\alpha_l\alpha_ky_ly_kK_{l,k}]\end{align}
$$
&lt;/p&gt;

&lt;p&gt;
$$
\begin{align}
L(\alpha) &amp;amp; = B + A - S\alpha_j + \alpha_j - \frac{1}{2} [2\alpha_i\alpha_jSK_{i,j} + \alpha_i^2K_{i,i} + \alpha_j^2K_{j,j} \\
&amp;amp; + 2\alpha_iy_iV_i + 2\alpha_jy_jV_j +  \sum_{l!=i,j}^N \sum_{k!=i,j}^N\alpha_l\alpha_ky_ly_kK_{l,k}]    \\
&amp;amp; =-S\alpha_j + \alpha_j  - \frac{1}{2}K_{i,i}(A-S\alpha_j)^2 - \frac{1}{2} K_{j,j}\alpha_j^2 - K_{i,j}\alpha_j(A-S\alpha_j)S \\
&amp;amp; - \alpha_jy_jV_j - (A-S\alpha_j)y_iV_i + \varepsilon_{constant} \end{align}
$$
&lt;/p&gt;

&lt;p&gt;其中，&lt;script type=&quot;math/tex&quot;&gt;\varepsilon_{constant}&lt;/script&gt;为一些常量，在求极值点是可以忽略，上式对&lt;script type=&quot;math/tex&quot;&gt;\alpha_j&lt;/script&gt;求导，有：&lt;/p&gt;
&lt;p&gt;
$$
\begin{eqnarray*} \frac {\partial_{L(\alpha_j)}} {\partial_{\alpha_j}} &amp;amp;=&amp;amp; -S + 1 + ASK_{i,i} -K_{i,i}\alpha_j - K_{j,j}\alpha_j -ASK_{i,j} + 2K_{i,j}\alpha_j -y_jV_j + Sy_iV_i=0 \\ \alpha_j&amp;amp;=&amp;amp; \frac {-S + 1 + AS(K_{i,i}-K_{i,j}) + y_j(V_i-V_j)}{K_{i,i} + K_{j,j} - 2K_{i,j}} \end{eqnarray*}
$$
&lt;/p&gt;

&lt;p&gt;在优化&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;的时候，其他参数没有被改变。&lt;/p&gt;
&lt;p&gt;
$$
\begin{eqnarray*}
\alpha_iy_i+\alpha_jy_j&amp;amp;=&amp;amp; -\sum_{k!=i,j}\alpha_k^{old}y_k=\alpha_i^{old}y_i + \alpha_j^{old}y_j \\ 
V_i &amp;amp;=&amp;amp; \sum_{k!=i,j}\alpha_ky_kK_{i,k} \\
&amp;amp;=&amp;amp;\sum_{k!=i,j}\alpha_k^{old}y_kK_{i,k} \\
&amp;amp;=&amp;amp; \sum_{k=1}^N\alpha_k^{old}y_kK_{i,k} + b - b - \alpha_i^{old}y_iK_{i,i} - \alpha_j^{old}y_jK_{i,j} \\
V_j &amp;amp;=&amp;amp;\sum_{k!=i,j}\alpha_ky_kK_{j,k} \\
&amp;amp;=&amp;amp;\sum_{k!=i,j}\alpha_k^{old}y_kK_{j,k} \\
&amp;amp;=&amp;amp; \sum_{k=1}^N\alpha_k^{old}y_kK_{j,k} + b - b - \alpha_j^{old}y_jK_{j,j} - \alpha_i^{old}y_iK_{i,j}  
\end{eqnarray*}
$$
&lt;/p&gt;
&lt;p&gt;且&lt;script type=&quot;math/tex&quot;&gt;g(x_i) = \sum_{k=1}^N\alpha_k^{old}y_kK_{i,k} + b&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;g(x_j) = \sum_{k=1}^N\alpha_k^{old}y_kK_{j,k} + b&lt;/script&gt;，所以：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V_i - V_j = g(x_i) - g(x_j) - \alpha_i^{old}y_iK_{i,i} + \alpha_j^{old}y_jK_{j,j} - \alpha_j^{old}y_jK_{i,j} + \alpha_i^{old}y_iK_{i,j}&lt;/script&gt;

&lt;p&gt;然后，将A = &lt;script type=&quot;math/tex&quot;&gt;\alpha_i^{old}+S\alpha_j^{old}&lt;/script&gt;，S=&lt;script type=&quot;math/tex&quot;&gt;y_iy_j&lt;/script&gt;代入&lt;script type=&quot;math/tex&quot;&gt;\alpha_j&lt;/script&gt;表达式，可得：&lt;/p&gt;

&lt;p&gt;
$$
\begin{eqnarray*}\alpha_j &amp;amp;=&amp;amp; \frac {\{y_jy_j - y_iy_j +(\alpha_i^{old}+y_iy_j\alpha_j^{old})y_iy_j(K_{i,i}-K_{i,j}) \\ 
+ y_j(g(x_i) - g(x_j) - \alpha_i^{old}y_iK_{i,i} + \alpha_j^{old}y_jK_{j,j} - \alpha_j^{old}y_jK_{i,j} + \alpha_i^{old}y_iK_{i,j})\}} {K_{i,i} + K_{j,j} - 2K_{i,j}} \\ &amp;amp;=&amp;amp; \frac {y_j[y_j-y_i + y_j\alpha_j^{old}(K_{i,i} + K_{j,j} - 2K_{i,j} + g(x_i) - g(x_j) ]} {K_{i,i} + K_{j,j} - 2K_{i,j}} \\ &amp;amp;=&amp;amp; \alpha_j^{old} + \frac{y_j[y_j-y_i + g(x_i)-g(x_j)]} {K_{i,i} + K_{j,j} - 2K_{i,j}}\end{eqnarray*}
$$
&lt;/p&gt;

&lt;p&gt;然后令&lt;script type=&quot;math/tex&quot;&gt;E_i = g(x_i) - y_i&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;\eta = K_{i,i} + K_{j,j} - 2K{i,j}&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;\alpha_j^{new,unc} = \alpha_j^{old} + \frac {y_j(E_i - E_j)} {\eta}&lt;/script&gt;，此时求出的&lt;script type=&quot;math/tex&quot;&gt;\alpha_j&lt;/script&gt;还需要经过边界判定，对&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;有&lt;script type=&quot;math/tex&quot;&gt;\alpha_iy_i + \alpha_jy_j = \alpha_i^{old} + \alpha_j^{old}&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt;=\alpha_i&lt;=C %]]&gt;&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt;=\alpha_j&lt;=C %]]&gt;&lt;/script&gt;的条件限制，所以必须对&lt;script type=&quot;math/tex&quot;&gt;\alpha_i, \alpha_j&lt;/script&gt;的上下边界&lt;script type=&quot;math/tex&quot;&gt;L,H&lt;/script&gt;进行确认。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;如果&lt;script type=&quot;math/tex&quot;&gt;y_i = y_j&lt;/script&gt;，那么&lt;script type=&quot;math/tex&quot;&gt;L= max(0, \alpha_j^{old} - \alpha_i^{old})&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;H=min(C,C+\alpha_j^{old} - \alpha_i^{old})&lt;/script&gt; &lt;br /&gt;
如果&lt;script type=&quot;math/tex&quot;&gt;y_i!=y_j&lt;/script&gt;，那么&lt;script type=&quot;math/tex&quot;&gt;L=max(0, \alpha_j^{old} + \alpha_i^{old} - C)&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;H=min(C,\alpha_j^{old} + \alpha_i^{old})&lt;/script&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;根据上式可以得到&lt;script type=&quot;math/tex&quot;&gt;\alpha_j^{new}&lt;/script&gt;为：
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
alpha_j^{new} = \begin{cases} H,~~~\alpha_j^{new,unc} &gt; H \\ \alpha_j^{new,unc},~~~~~ L&lt;=\alpha_j^{new,unc}&lt;=H \\ L,~~~~~\alpha_j^{new,unc} &lt; L \end{cases} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;由此可以得到&lt;script type=&quot;math/tex&quot;&gt;\alpha_i^{new}= \alpha_i^{old} + y_iy_j(\alpha_j^{old} - \alpha_j^{new})&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;关于&lt;script type=&quot;math/tex&quot;&gt;alpha_i, \alpha_j&lt;/script&gt;的更新策略完成，但是对于上式中的bias也需要进行更新，以保证KKT条件&lt;script type=&quot;math/tex&quot;&gt;\alpha_j(y_j(\sum_{i=1}^N\alpha_iy_iK_{i,j} + b) - 1) = 0&lt;/script&gt;成立。&lt;/p&gt;

&lt;p&gt;当&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt; \alpha_i^{new} &lt; C %]]&gt;&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;y_i(\sum_{k=1}^N\alpha_ky_kK_{i,k}+b) = 1&lt;/script&gt; ，所以&lt;script type=&quot;math/tex&quot;&gt;b_1^{new} = y_i - \sum_{k!=i,j}^N\alpha_ky_kK_{i,k} - \alpha_i^{new}y_iK_{i,i} - \alpha_j^{new}y_jK_{i,j}&lt;/script&gt;，又知&lt;script type=&quot;math/tex&quot;&gt;E_i = g(x_i) - y_i = \sum_{k=1}^N\alpha_ky_kK_{i,k} + b^{old} - y_i&lt;/script&gt;，此时除&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;以外的都不会发生变化，所以&lt;script type=&quot;math/tex&quot;&gt;E_i = \sum_{k!=i,j}^N\alpha_ky_kK_{k,i} + \alpha_i^{old}y_iKii + \alpha_j^{old}y_jK_{i,j} + b^{old} - y_i&lt;/script&gt;，也即 &lt;script type=&quot;math/tex&quot;&gt;b_i^{new} = -E_i - y_iK_{i,i}(\alpha_i^{new} - \alpha_i^{old}) - y_jK_{i,j}(\alpha_j^{new} - \alpha_j^{old}) + b^{old}&lt;/script&gt;
同1，当&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt; \alpha_j^{new} &lt; C %]]&gt;&lt;/script&gt;，则&lt;script type=&quot;math/tex&quot;&gt;b_2^{new} = -E_j - y_iK_{i,j}(\alpha_i^{new} - \alpha_i^{old}) - y_jK_{j,j}(\alpha_j^{new} - \alpha_j^{old}) + b^{old}&lt;/script&gt;
如果&lt;script type=&quot;math/tex&quot;&gt;\alpha_i^{new}&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;\alpha_j^{new}&lt;/script&gt;都满足条件，则&lt;script type=&quot;math/tex&quot;&gt;b_1^{new}&lt;/script&gt;=&lt;script type=&quot;math/tex&quot;&gt;b_2^{new}&lt;/script&gt;
如果&lt;script type=&quot;math/tex&quot;&gt;\alpha_i^{new}&lt;/script&gt;、&lt;script type=&quot;math/tex&quot;&gt;\alpha_j^{new}&lt;/script&gt;为0或者C，那么取&lt;script type=&quot;math/tex&quot;&gt;b_1^{new}&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;b_2^{new}&lt;/script&gt;的中值即可
至此，SMO算法结束，不过实际中，需要实时更新&lt;script type=&quot;math/tex&quot;&gt;E_i&lt;/script&gt;，所以在更新完bias之后，再利用已有的信息重新更新&lt;script type=&quot;math/tex&quot;&gt;E_i&lt;/script&gt;即可。&lt;/p&gt;

&lt;p&gt;最终实现见：&lt;a href=&quot;https://github.com/kymo/SUML/tree/master/SVM&quot;&gt;svm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;参考资料&lt;/p&gt;

&lt;p&gt;李航 《统计学习方法》&lt;/p&gt;

&lt;p&gt;http://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html&lt;/p&gt;

</description>
        <pubDate>Sat, 02 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/02/SVM.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/02/SVM.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>SVD</title>
        <description>&lt;p&gt;奇异值分解作为一种矩阵分解的算法，广泛的应用于数据分析、信号处理、模式识别、图片压缩、天气预测以及潜在概率语义分析等应用。其实质是通过分解矩阵，找到矩阵行列之间的某种潜在关系。如在推荐系统中利用SVD分解用户-商品的评分矩阵，便可以得到用户和商品之间的关联。&lt;/p&gt;

&lt;h3 id=&quot;特征值分解&quot;&gt;特征值分解&lt;/h3&gt;
&lt;p&gt;一般矩阵乘以某个向量，代表着对该向量进行某种线性变换：拉伸、平移以及旋转，在计算机图形学中经常对三维场景的物体在各种不同的坐标系下进行变换也利用了该种性质，直接利用物体的三维坐标乘以变换矩阵即可实现。如果对向量的线性变换仅仅只是对向量进行了缩放，那么这种情况便是特征值分解，既有&lt;script type=&quot;math/tex&quot;&gt;Ax=\lambda x&lt;/script&gt;. 特征值分解的物理意义在不同的应用场景下有不同的解释。不过一般都是对对称矩阵&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;进行分解，得到一组正交的特征向量作为正交基，使得矩阵&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;能够被投射到以该正交基为基底的空间中，特征值则表示在不同的正交基下的投射距离，特征值越大说明矩阵在对应的特征向量上的方差越大，功率越大，信息熵也越大，在PCA(principal  component analysis)中，即可以利用特征值分解，获取最重要的K个成分对数据进行降维，保证在数据规模可控的情况下获得足够的信息量。&lt;/p&gt;

&lt;h3 id=&quot;奇异值分解&quot;&gt;奇异值分解&lt;/h3&gt;
&lt;p&gt;奇异值分解可以认为是特征值分解的扩充，即解决对于矩阵&lt;script type=&quot;math/tex&quot;&gt;A(m \times n)&lt;/script&gt;能否在原始空间&lt;script type=&quot;math/tex&quot;&gt;R^n&lt;/script&gt;中找到一组正交基&lt;script type=&quot;math/tex&quot;&gt;(v_1, v_2, v_3, \cdot \cdot \cdot , v_n)&lt;/script&gt;，使得经过该线性变换矩阵投射得新的空间&lt;script type=&quot;math/tex&quot;&gt;R^m&lt;/script&gt;中的向量仍然正交的问题，即当&lt;script type=&quot;math/tex&quot;&gt;i!=j ，v_i^T \times v_j = 0&lt;/script&gt;时，&lt;script type=&quot;math/tex&quot;&gt;(Av_i,Av_j) = 0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;对于矩阵&lt;script type=&quot;math/tex&quot;&gt;A^TA(n \times n)&lt;/script&gt;，该矩阵为对称矩阵，且有&lt;script type=&quot;math/tex&quot;&gt;r(A^TA)=r&lt;/script&gt;，则可以通过特征值分解得到对应的一组正交基&lt;script type=&quot;math/tex&quot;&gt;(v_1, v_2, v_3, \cdot \cdot \cdot , v_r)&lt;/script&gt;，使得&lt;script type=&quot;math/tex&quot;&gt;A^TAv_i=\lambda v_i&lt;/script&gt;.
对于经过&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;线性变化投射到新的空间的向量&lt;script type=&quot;math/tex&quot;&gt;Av_i&lt;/script&gt;，容易推知当&lt;script type=&quot;math/tex&quot;&gt;i!=j&lt;/script&gt;时，&lt;script type=&quot;math/tex&quot;&gt;(Av_i,Av_j)&lt;/script&gt;=0. 从而可以得到投射之后的新的一组正交基&lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;为&lt;script type=&quot;math/tex&quot;&gt;(Av_1,Av_2,  \cdot  \cdot  \cdot  Av_r)&lt;/script&gt;，对其进行标准化，有&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;u_i=\frac{Av_i}{\mid Av_i\mid} =\frac{1}{\sqrt {\lambda_i}} Av_i =&gt; Av_i = \sqrt {\lambda_i}u_i = \delta_i u_i&lt;/script&gt;

&lt;p&gt;则有&lt;/p&gt;

&lt;p&gt;$A(v_1, v_2, v_3, \cdot \cdot \cdot, v_r) = (\delta_1 u_1 , \delta_2 u_2 , \delta_3 u_3, \cdot \cdot \cdot , \delta_r u_r) = (u_1, u_2, \cdot \cdot \cdot, u_r) \times [
\begin {matrix}
\delta_1, \cdot \cdot \cdot, 0 \\
0, \cdot \cdot \cdot, 0 \\
0, \cdot \cdot \cdot, \delta_r
\end {matrix}
]
$&lt;/p&gt;

&lt;p&gt;即&lt;script type=&quot;math/tex&quot;&gt;AV = U\Sigma =&gt; AVV^T = U\Sigma V^T =&gt; A =  U\Sigma V^T (VV^T=E)&lt;/script&gt; ，其中，$\Sigma$的对角元素为特征值的开平方，表示&lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;向量经过&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;方阵变换之后与&lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;向量之间的缩放对应关系，此时&lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;矩阵大小为&lt;script type=&quot;math/tex&quot;&gt;m \times r&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;矩阵大小为&lt;script type=&quot;math/tex&quot;&gt;n \times r&lt;/script&gt;，也可以将&lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;扩充为&lt;script type=&quot;math/tex&quot;&gt;m \times m&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;扩充为 &lt;script type=&quot;math/tex&quot;&gt;n \times n&lt;/script&gt;，此时&lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt;大小则为&lt;script type=&quot;math/tex&quot;&gt;m \times n&lt;/script&gt;，即在原来的基础之上增加若干个0行和0列.&lt;/p&gt;

&lt;p&gt;从另外一个角度，对于任意矩阵&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;，可以分解为&lt;script type=&quot;math/tex&quot;&gt;A= U\Sigma V^T&lt;/script&gt;的形式，于是可以得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;AA^T=U\Sigma V^T V\Sigma^TU^T = U\Sigma \Sigma^TU^T&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A^TA= V\Sigma^TU^T  U\Sigma V^T = V\Sigma \Sigma^TV^T&lt;/script&gt;

&lt;p&gt;所以，&lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;可以看成是对矩阵&lt;script type=&quot;math/tex&quot;&gt;AA^T&lt;/script&gt;进行特征值分解得到的正交基，&lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;可以看成是对矩阵&lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt;进行分解得到的正交基。&lt;/p&gt;

&lt;h3 id=&quot;svd-在lsa中的应用&quot;&gt;SVD 在LSA中的应用&lt;/h3&gt;

&lt;p&gt;奇异值分解在诸多方面都有十分成功的应用，在数据压缩中，可以通过将原始的矩阵进行奇异值分解，去除奇异值较小的部分得到低阶近似，从而实现对数据的降维；在语义分析中，通过对构造的文档-词汇矩阵&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;进行奇异值分解&lt;script type=&quot;math/tex&quot;&gt;D=U \Sigma V^T&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D = [
\begin{array}{ccc}
x_{1,1}  , \cdot \cdot  \cdot  ,  x_{1,n} \\\
\cdot \cdot  \cdot   ,  \cdot \cdot  \cdot  ,  \cdot \cdot  \cdot \\\
x_{m,1}   ,  \cdot \cdot  \cdot  ,  x_{m,n} 
\end{array}
]
= [t_1 , t_2, \cdot \cdot \cdot , t_n]^T = [d_1 , d_2, \cdot \cdot \cdot , d_m]&lt;/script&gt;

&lt;p&gt;其中，&lt;script type=&quot;math/tex&quot;&gt;t_i^T = (x_{i,1}, x_{i,2}, \cdot \cdot \cdot , x_{i,n}) , d_j^T = (x_{1,j}, x_{2,j}, \cdot \cdot \cdot , x_{m,j})&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;进行SVD分解之后&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D=U\Sigma V^T = (u_1, u_2,  \cdot \cdot \cdot, u_r) \times [
\begin {matrix}
\delta_1, \cdot \cdot \cdot, 0 \\\
0, \cdot \cdot \cdot, 0 \\\
0, \cdot \cdot \cdot, \delta_r
\end {matrix}
]
\times 
(v_1, v_2, \cdot \cdot \cdot , v_r)^T&lt;/script&gt;

&lt;p&gt;随后进行k-阶近似，取最大的k个奇异值&lt;script type=&quot;math/tex&quot;&gt;\Sigma_k&lt;/script&gt;以及对应的奇异向量&lt;script type=&quot;math/tex&quot;&gt;U_k,V_k&lt;/script&gt;，便可以对&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;进行降维，将词汇-文档空间映射到语义空间中去，此时k值可以认为是语义空间中的主题数，&lt;script type=&quot;math/tex&quot;&gt;U_k&lt;/script&gt;可以看做在该语义空间中，文档上的主题分布，而&lt;script type=&quot;math/tex&quot;&gt;V_k&lt;/script&gt;则可以看成是主题上的词汇分布。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_k = U_k \Sigma_k V_k^T = (u_1, u_2,  \cdot \cdot \cdot, u_k) \times [
\begin {matrix}
\delta_1, \cdot \cdot \cdot, 0 \\\
0, \cdot \cdot \cdot, 0 \\\
0, \cdot \cdot \cdot, \delta_k
\end {matrix}
]
\times 
(v_1, v_2, \cdot \cdot \cdot , v_k)^T&lt;/script&gt;

&lt;p&gt;此时，&lt;script type=&quot;math/tex&quot;&gt;d_j^T = (v_{1,j}，v_{2,j}, \cdot \cdot \cdot ,v_{k,j})&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;t_i^T =  (u_{1,j}，u_{2,j}, \cdot \cdot \cdot ,u_{k,j})&lt;/script&gt;
此时D仍然是&lt;script type=&quot;math/tex&quot;&gt;m \times n&lt;/script&gt;，得到对词汇-文档原始矩阵的奇异值分解结果之后，变可以完成包括词汇相似度计算、文档相似度计算等任务。&lt;/p&gt;

&lt;h4 id=&quot;1-词汇相似度计算计算语料中的两个词汇pq的语义相似度&quot;&gt;1. 词汇相似度计算，计算语料中的两个词汇&lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt;的语义相似度&lt;/h4&gt;
&lt;p&gt;分别计算&lt;script type=&quot;math/tex&quot;&gt;\Sigma_k \times t_i&lt;/script&gt;以及&lt;script type=&quot;math/tex&quot;&gt;\Sigma_k \times t_j&lt;/script&gt;(i和j为词汇p,q对应的ID)，可以得到两个列向量，然后通过某种距离计算方式可以得到两者的语义距离.&lt;/p&gt;

&lt;h4 id=&quot;2-文档相似度计算计算语料中的两篇文档p-q的语义相似度&quot;&gt;2. 文档相似度计算，计算语料中的两篇文档&lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt;的语义相似度&lt;/h4&gt;
&lt;p&gt;分别计算&lt;script type=&quot;math/tex&quot;&gt;\Sigma_k \times d_i&lt;/script&gt;以及&lt;script type=&quot;math/tex&quot;&gt;\Sigma_k \times d_j&lt;/script&gt;(i和j为词汇p,q对应的ID)，可以得到两个列向量，然后通过某种距离计算方式可以得到两者的语义距离.&lt;/p&gt;

&lt;h4 id=&quot;3-分本聚类&quot;&gt;3. 分本聚类&lt;/h4&gt;
&lt;p&gt;低阶降维之后可以的到语义空间的文档特征，从而可以利用一些无监督的聚类算法进行文档聚类，当然也可以作为分档分类的特征。&lt;/p&gt;

&lt;h4 id=&quot;4-lsa-缺点&quot;&gt;4. LSA 缺点&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;新生成的矩阵的解释性比较差.造成这种难以解释的结果是因为SVD只是一种数学变换，并无法对应成现实中的概念。&lt;/li&gt;
  &lt;li&gt;LSA无法扑捉一词多以的现象。在原始词-向量矩阵中，每个文档的每个词只能有一个含义。比如同一篇文章中的“The Chair of Board”和”the chair maker”的chair会被认为一样。在语义空间中，含有一词多意现象的词其向量会呈现多个语义的平均。相应的，如果有其中一个含义出现的特别频繁，则语义向量会向其倾斜。&lt;/li&gt;
  &lt;li&gt;LSA具有词袋模型的缺点，即在一篇文章，或者一个句子中忽略词语的先后顺序。&lt;/li&gt;
  &lt;li&gt;LSA的概率模型假设文档和词的分布是服从联合正态分布的，但从观测数据来看是服从泊松分布的。因此LSA算法的一个改进PLSA使用了多项分布，其效果要好于LSA。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;参考资料&quot;&gt;参考资料&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.puffinwarellc.com/index.php/news-and-articles/articles/30-singular-value-decomposition-tutorial.html?showall=1&quot;&gt;Singular Value Decomposition (SVD) Tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.sciencenet.cn/blog-696950-699432.html&quot;&gt;奇异值分解(SVD) — 几何意义&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/21874816&quot;&gt;如何理解矩阵特征值？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://charlesx.top/2016/03/Singularly-Valuable-Decomposition/&quot;&gt;漫谈奇异值分解&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.math.washington.edu/~morrow/464_14/svd.pdf&quot;&gt;Dan Kalman. A singularly valuable decomposition: The SVD of a matrix[J]. College Mathematics Journal, 1996, 27(1):2–23.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/roger__wong/article/details/41175967&quot;&gt;LSA潜在语义分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sat, 02 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/02/SVD.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/02/SVD.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>Random Forest</title>
        <description>&lt;p&gt;一般我们提到序列标注问题，会有三种模型很引人瞩目，最大熵马尔科夫模型、隐马尔科夫模型和条件穗机场模型。三种的联系和区别详见：隐马尔可夫模型 最大熵马尔可夫模型 条件随机场 区别和联系。&lt;/p&gt;

&lt;p&gt;###HMM简单介绍&lt;/p&gt;

&lt;p&gt;在介绍CRF建模过程，我们先简单的了解下HMM，因为HMM可以认为是CRF的子集，任何可以由HMM解决的问题都可以由CRF解决~&lt;/p&gt;

&lt;p&gt;HMM有两种假设：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;某种状态只依赖于前一状态，也即隐含状态满足一阶马尔科夫性质；&lt;/li&gt;
  &lt;li&gt;观测状态之间彼此独立&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面用一个简单的中文分词来介绍HMM吧。
一般在中文分词中，我们会设定某个中文汉字所处的状态（B:词的开头；M：在词中间；E：在词结尾；S：单字成词），这四种状态（BMES）可以认为是我们的隐含状态，而该状态对应的中文字符，则是我们的观测状态。&lt;/p&gt;

&lt;p&gt;通过大量的语料的统计可以得到这四种状态之间的转移概率矩阵p(SJ|SI)p(SJ|SI)
即由状态SISI转移到SJSJ的概率。同时也可以统计出初始状态的概率π(SI)π(SI)，另外我们的发射概率（也即有隐含状态（BMES）生成汉字的概率）也可以通过统计得出，当然了，也可以利用EM算法去训练，但在此不再赘述。&lt;/p&gt;

&lt;p&gt;HMM一般会解决三类问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;模型学习，可以使用统计也可以使用EM；&lt;/li&gt;
  &lt;li&gt;给定观测状态，确定最可能的隐含状态序列，比如中文分词、词性标注等；&lt;/li&gt;
  &lt;li&gt;给定隐含状态，确定最可能的观测序列，比如天气预报。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;中文分词给定的是观测序列（即一个个的中文字符），任务即是找到某种最可能的隐含状态，也即判断中文字符所处的位置(BMES)，使得该观测序列产生的概率最大。在进行计算的时候，我们不可能对所有的序列进行判断， 这将是一个NP问题。下图是对“我来自武汉”的一个HMM模型图示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/image/t.png&quot; alt=&quot;HMM图片&quot; /&gt;&lt;/p&gt;

&lt;p&gt;途中蓝色部分为隐含状态之间的转移概率矩阵，蓝线表示状态的转移。红线表示由隐含状态生成观测状态。计算的时候，我们可以根据迭代公式计算出前向概率，然后反向递推，得到生成该序列最大概率的隐含状态序列。比如上图中最后最有可能得到的生成“我、来、自、武、汉”这一观测序列的隐含状态序列为“SBEBE”，也即分词结果为”我\来自\武汉”。&lt;/p&gt;

&lt;p&gt;###CRF&lt;/p&gt;

&lt;p&gt;HMM的假设简单粗暴，却行之有效，然而在分词或者词性标注这些应用上，却无法达到CRF所取得的效果，原因之一正是在于其假设。HMM过于local，导致其损失了非常多global的信息，而CRF却可以利用这些信息，从而可以得到更好的结果。&lt;/p&gt;

&lt;p&gt;CRF自从被Lafferty等人发明之后，由最初的应用于序列标注的自然语言处理领域广泛的扩散至生物信息学、计算机视觉等领域。CRF中十分重要的一 个概念为特征函数，其是状态转移特征函数和状态生成特征函数的集合。其实刚开始看CRF的时候这个特征函数很不容易理解，但是我们可以把它理解成一个对某种规则的一种特征化处理，比如在词性标注中，在训练的时候如果当前词为“形容词”，并且它以“的”结尾，那么该特征函数输出为1，否则为0。并且我们需要对每一个特征函数赋以某种权值，则如果最后的结果中该特征和权重的积较大，可以表征：以”的“结尾的词以某种概率是形容词“，而这个权重λλ是模型训练的目标。&lt;/p&gt;

&lt;p&gt;参考文献&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://wenku.baidu.com/link?url=7LBbXiKPWAPnqYexmBOhz4iCUSny6Ayg3M53Ls0IiVKdqLq-9YPNAiW3WKJ5UgihjWKmm4yTpahIIeu75BB_mM_Q1QicaLIGrOiwHUO8ktu###&quot;&gt;百度文库&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/felomeng/article/details/4367250&quot;&gt;http://blog.csdn.net/felomeng/article/details/4367250&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sat, 02 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/02/RandomForests.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/02/RandomForests.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
  </channel>
</rss>
