<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aron&#39;s blog</title>
    <description>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</description>
    <link>http://kymo.github.io/</link>
    <atom:link href="http://kymo.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 03 Apr 2016 09:31:57 +0800</pubDate>
    <lastBuildDate>Sun, 03 Apr 2016 09:31:57 +0800</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>Ensemble Learning Methods</title>
        <description>&lt;p&gt;集成学习的关键是弱学习器的组合，目标是为了提升模型的性能，一般用在Model Selection，此时降低了选择弱名的可能性。当然集成学习也广泛的应用于结果置信度检验、特征选择、数据融合、增量学习等方面，取得了良好的效果。&lt;/p&gt;

&lt;p&gt;一般而言，对于给定的任务，比如分类，首先在处理完特征之后，我们需要考虑的是选择何种模型来对这些样本进行训练。一般我们没有样本数据产生过程的先验知识，只能通过手动的选择调参，才有可能得到一个较好拟合已知样本的模型。集成学习给了一个非常完美的模型选择的解决方案，通过训练多个子模型，组成committee，最后综合决策，达到了自动选择模型的效果。&lt;/p&gt;

&lt;p&gt;一般模型的性能某种程度取决于数据集的大小，也即观测样本的覆盖度。数据集过大对于一般模型可能会导致训练过拟合或者训练性能瓶颈，此时则可以切分数据集，在子数据集上单独训练模型，然后按照某种组合策略构建committee。数据集过小对于一般模型则会欠拟合，此时可以使用bootstrap等抽样方法，在每一份抽样样本中单独训练模型，组成committee。&lt;/p&gt;

&lt;p&gt;一般而言，样本的分类边界十分复杂，如果仅仅使用一般的模型，则很难学习出能够较好拟合样本的分类边界。但是如果使用集成学习框架，通过学习多个子模型，则能够很好的学习出该分类边界。&lt;/p&gt;

&lt;p&gt;另外，如果我们收集到的数据来源很多，导致数据的类型、维度不同，如果将所有来源的数据都放在同一个向量中，可能会降低模型的学习能力。但是假如我们在不同来源的数据中根据该数据特性或者其他先验知识单独训练模型，则效果一般都会好很多。今天看了一篇关于新浪微博垃圾用户检测的论文，研究现状里就提到了采用这种思想的方法，通过观测，发现垃圾用户一般有三种行为：广告、重复转发和恶意关注，然后根据这三种类型去抽取特征，得到三种类型的特征向量之后，然后分别在三种特征上进行子模型训练，最后效果也还不错。&lt;/p&gt;

&lt;p&gt;此外，集成学习框架还可以用来对结果进行评估，可以根据committee中的子模型的vote去计算。&lt;/p&gt;

&lt;p&gt;有人总结了使用ensemble learning的三大原因：&lt;/p&gt;

&lt;p&gt;统计学，已知样本无法完成的表达原始数据的生成分布，通过bootstrap等方法，可以尽量的拟合原始分布训练模型。
可计算，体现于模型选择，通过模型融合的解决方案将缺乏模型先验知识的人从单模型调优中解放出来。
任意拟合，单模型无法拟合出复杂分类边界，则模型融合则很好的解决了这一问题。
参考文献 http://www.scholarpedia.org/article/Ensemble_learning&lt;/p&gt;

</description>
        <pubDate>Sun, 03 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/03/EnsembleLearningMethods.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/03/EnsembleLearningMethods.html</guid>
        
        
        <category>machinelearning</category>
        
      </item>
    
      <item>
        <title>Probability Latent Semantic Analysis</title>
        <description>&lt;p&gt;pLSA是一种Topic Model，全称概率潜在语义分析，将文本的高维稀疏向量表示成低位维稠密向量。它是一种无监督学习方法，假设整个文档集合是由若干个主题组成，某一篇文章都以一定的概率(p(z|d))属于某一主题，在给定主题的情况下，每个词汇都以一定的概率(p(w|z))产生。
pLSA是基于这样一个假设，假设文档和主题的分布以及主题和词汇的分布都是多项式分布，因此一个词汇的生成过程可以表示为：
	以一定的概率生成一篇文档
	在该文档中按照文档主题分布选择一个主题
	在该主题下面，按照主题-词汇分布生成一个词汇
可以得到如下模型：(p(w,d)=p(d)p(w|d);~~~~p(w|d)=\sum_{z=1}p(z|d)p(w|z))，其中z为主题，w为词，d为文档。其实这个过程也很容易理解，假设现在我们要写篇paper，在动笔之前，会列好提纲，大概选择几个点(也就是主题)去稀疏，在充实这些主题的过程中，我们会去选择和这个主题相关的词进行修饰，比如写到关于pLSA的文章的时候，我们会选择”建模、概率、主题模型、极大似然“等词去完善这个主题。在得到模型之后，我们就需要进行Model Inference，也就是参数估计了，pLSA作为概率学派的经典之一，一般是采用MLE作为inference的tool。
首先构造极大似然函数：(L(\theta) = \prod_{i=1}^N\prod_{j=1}^Np(d_i,w_j)^{n(d_i,w_j)})，其中(n(d_i,w_j))表示的是词(w_j)在和文档(d_i)的共现频率，由于我们(p(w|z))和(p(z|d))都是多项式分布，&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    接着，是参数估计，一般而言，此时可以选用极大似然估计。极大对数似然函数如下：
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果直接求导的话，很明显就得需要求解(n+m)个方程， 明显不可取。
可以采用EM算法，EM算法的基本思想，随机初始化相关参数。随后，在E步，计算潜在变量的后验概率。M步，利用潜在变量的后验概率去更新未知参数。
问题：为什么可以收敛？
在e步：&lt;/p&gt;

&lt;p&gt;M步，最大化极大似然期望：&lt;/p&gt;

&lt;p&gt;又由于此时：&lt;/p&gt;

&lt;p&gt;所以：&lt;/p&gt;

&lt;p&gt;利用Jensen不等式，对于凸函数有：&lt;/p&gt;

&lt;p&gt;可以得到：&lt;/p&gt;

&lt;p&gt;此时即转化为，对于约束条件： ，求E’(L)的最大值。
很显然可以构造拉格朗日乘法进行求解：&lt;/p&gt;

&lt;p&gt;分别对 求导可得：&lt;/p&gt;

&lt;p&gt;联立方程组(1),(2),(3),(4)求解可得：&lt;/p&gt;

&lt;p&gt;因此，在M步更求新参数，通过不断迭代，如果求得的极大似然趋近收敛，则训练结束。
总结一下：
E步：求当前估计的参数条件下的后验概率：p(z|w,d)
M步：最大化complete data(加上隐藏变量主题)极大似然估计的期望
E步和M步循环迭代，直至收敛。
实现的源码见：plsa.cpp plsa.h ，由于切词用到了公司的lib库，所以切词的初始化和切词的过程在源码中被删去，可以去中科院nlp主页获取相关模块。&lt;/p&gt;

</description>
        <pubDate>Sat, 02 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/naturallanguageprocessing/2016/04/02/pLSA.html</link>
        <guid isPermaLink="true">http://kymo.github.io/naturallanguageprocessing/2016/04/02/pLSA.html</guid>
        
        
        <category>naturallanguageprocessing</category>
        
      </item>
    
      <item>
        <title>Support Vector Machine</title>
        <description>&lt;p&gt;Support Vector Machine ，支持向量机，通常用来进行classification，但是也有做regression。SVM在面对非线性问题上具有独特的优势。本文从linear和nonlinear两种情况下对SVM的建模过程、优化目标的求解推导过程以及优化算法SMO进行阐述。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a^2 + b^2 = c^2&lt;/script&gt;

&lt;h3 id=&quot;linear-svm&quot;&gt;1. linear SVM&lt;/h3&gt;

&lt;p&gt;在分类任务中，样本label为&lt;script type=&quot;math/tex&quot;&gt;{-1,1}&lt;/script&gt;，关于从sign distance转换到geometry distance的过程其实很容易理解，
sign distance可以衡量某个样本被分类的置信，如果sign distance越大，那么该样本被分为该类别的可信度就更大；
而geometry distance可以理解为样本距离超平面&lt;script type=&quot;math/tex&quot;&gt;Y = w^TX + b&lt;/script&gt;的距离，是sign distance归一化的结果，
求解目标为&lt;script type=&quot;math/tex&quot;&gt;argmax(\frac {|w^Tx + b|} {||w||})&lt;/script&gt;，并且需要满足约束：&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i + b) \geq 1&lt;/script&gt;，为了求解方便，可以不加证明的令&lt;script type=&quot;math/tex&quot;&gt;|w^Tx+b|=1&lt;/script&gt;，形式化如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}argmax \frac{1}{||w||} \\ s.t ~~ y_i(w^Tx_i+b) \geq 1 ~~i=1,2,\cdot \cdot ,n \end{cases}&lt;/script&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;而此时最大化 $$\frac{1}{&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;w&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;}&lt;script type=&quot;math/tex&quot;&gt;等价于最小化&lt;/script&gt;w&lt;script type=&quot;math/tex&quot;&gt;，并且最小化和最小化&lt;/script&gt;\frac {1}{2} w^Tw$$等价，&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;所以1.1可以变为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}argmmin \frac {1}{2} w^Tw \\ s.t ~~ y_i(w^Tx_i+b) \geq 1 ~~i=1,2,\cdot \cdot ,n \end{cases}&lt;/script&gt;

&lt;p&gt;由此可以得到不等式约束问题，原始问题通过分析不难发现，求解十分困难，不过对于PSO(粒子群算法)而言，往往可以求得比较好的解。不过在碰到这种带不等式约束的问题的时候，我们可以通过拉格朗日对偶性质将原始问题转化为对偶问题，在最大熵模型中也有类似的处理过程。首先构造拉格朗日函数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(w,b,\alpha) = \frac {1} {2} w^Tw - \sum_{i=1}^N a_i[y_i(w^Tx_i + b) - 1]&lt;/script&gt;

&lt;p&gt;首先我们可以得到这个函数的等价形式，令&lt;script type=&quot;math/tex&quot;&gt;\theta = argmax_\alpha L(w,b,\alpha)&lt;/script&gt;，那么：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\theta=\begin{cases}\frac {1} {2} w^Tw~~~y_i(w^Tx_i+b) \geq 1\\ \infty~~~~ y_i(w^Tx_i+b) &lt; 1~~\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;可见，&lt;script type=&quot;math/tex&quot;&gt;min\theta&lt;/script&gt;和原始优化目标&lt;script type=&quot;math/tex&quot;&gt;argmin \frac {1}{2}w^Tw&lt;/script&gt;等价。令&lt;script type=&quot;math/tex&quot;&gt;p=min\theta&lt;/script&gt;，其对偶形式&lt;script type=&quot;math/tex&quot;&gt;d=max_{\alpha}min_{w,b}L(w,\alpha)&lt;/script&gt;，那么必然会有&lt;script type=&quot;math/tex&quot;&gt;p \geq d&lt;/script&gt;。此时令&lt;script type=&quot;math/tex&quot;&gt;g=argmin_{w} L(w,b,\alpha)&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;g \leq L(w^*,b,\alpha) \leq \frac {1}{2} {w^*}^Tw^* = p&lt;/script&gt;，所以&lt;script type=&quot;math/tex&quot;&gt;p \geq d&lt;/script&gt;。所以此时该算法满足弱对偶，我们可以通过求解该弱对偶问题去近似求解原始问题，在EM中就是不断优化极大似然下界~ 并且可以知道的是，不管原始问题是何种优化，对偶问题都会是凸优化，也即都会存在极值。不过在SVM中，我们是可以把弱对偶加强，变成strong duality，也即&lt;script type=&quot;math/tex&quot;&gt;p = d&lt;/script&gt;，优化对偶问题等价于对原始问题的求解。那么怎么判断该对偶问题是强对偶问题呢？KKT条件。如下：&lt;/p&gt;

&lt;p&gt;\begin{cases}  \bigtriangledown L(w,b,\alpha) = 0 \ \alpha_i(y_i(w^Tx_i+b) - 1) = 0\ \alpha_i \geq 0 \end{cases}&lt;/p&gt;

&lt;p&gt;很明显，此时KKT条件成立，所以满足强对偶。其中&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i+b)-1) = 0&lt;/script&gt;但此时KKT只是必要条件，不过由于我们的原始问题是凸优化，所以KKT便是充要条件了。&lt;/p&gt;

&lt;p&gt;对对偶问题，首先是&lt;script type=&quot;math/tex&quot;&gt;min_{w,b}L(w,b,\alpha)&lt;/script&gt;，分别对&lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;求导，得：&lt;/p&gt;

&lt;p&gt;[\frac {\partial L(w,\alpha,b)} {\partial(w)} = w - \sum_{i=1}^N \alpha_iy_ix_i=0]&lt;/p&gt;

&lt;p&gt;[\frac {\partial L(w,\alpha,b)} {\partial(b)} = \sum_{i=1}^N\alpha_iy_i=0]&lt;/p&gt;

&lt;p&gt;随后，将&lt;script type=&quot;math/tex&quot;&gt;w=\sum_{i=1}^N\alpha_iy_ix_i&lt;/script&gt;代入&lt;script type=&quot;math/tex&quot;&gt;L(w,b,\alpha)&lt;/script&gt;中，则有：&lt;/p&gt;

&lt;p&gt;\begin{eqnarray&lt;em&gt;}
L(w,\alpha,b)&amp;amp;=&amp;amp;\frac {1}{2} w^Tw - \sum_{i=1}^N\alpha_i[y_i(w^Tx_i+b)-1]\&amp;amp;=&amp;amp;\frac{1}{2}\sum_{i=1}^N\alpha_iy_ix_i^T\sum_{j=1}^N\alpha_jy_jx_j - \sum_{i=1}^N\alpha_i[y_i(w^Tx_i+b)-1]\ &amp;amp;=&amp;amp;\frac {1}{2} \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j - \sum_{i=1}^N\alpha_i\sum_{j=1}^N\alpha_jy_jx_j^Tx_i + \sum_{i=1}^N\alpha_i\&amp;amp;=&amp;amp;-\frac {1}{2} \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j + \sum_{i=1}^N\alpha_i\end{eqnarray&lt;/em&gt;}&lt;/p&gt;

&lt;p&gt;所以：&lt;/p&gt;

&lt;p&gt;[\begin{cases}-\frac {1}{2} \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j + \sum_{i=1}^N\alpha_i\s.t~~~\sum_{i=1}^N\alpha_iy_i=0 \end{cases}]&lt;/p&gt;

&lt;p&gt;到这，我们还没有考虑soft margin。实际情况中，总是会存在一定的噪声数据，使得我们的分类超平面被这些噪声数据所误导，从而使得模型的variance增大，所以一般来讲都会采用soft margin构建优化函数，我们以&lt;script type=&quot;math/tex&quot;&gt;\varepsilon&lt;/script&gt;的范围容许一定的误差，即原来的&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i+b) \geq 1&lt;/script&gt;此时为&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i + b) \geq 1 - \varepsilon_i&lt;/script&gt;，所以我们的优化目标变为：&lt;/p&gt;

&lt;p&gt;[\begin{cases}argmin \frac {1}{2} w^Tw + C\sum_{i=1}^N\varepsilon_i \s.t ~~ y_i(w^Tx_i+b) \geq 1 - \varepsilon_i ~~i=1,2,\cdot \cdot ,n \ \sum_{i=1}^N\varepsilon_i \geq C \end{cases}]&lt;/p&gt;

&lt;p&gt;关于上式，可以看成是利用hinge loss加l2范数正则项的结果，SVM此时的损失函数可以表示为 &lt;script type=&quot;math/tex&quot;&gt;min_{w,b} \sum_{i=1}^N [1-y_i(w^Tx_i+b)]_{+} + \lambda\|w\|^2&lt;/script&gt;，其中如果&lt;script type=&quot;math/tex&quot;&gt;z&gt;0&lt;/script&gt;，那么&lt;script type=&quot;math/tex&quot;&gt;z_{+}&lt;/script&gt;=z，否则等于0。如果令&lt;script type=&quot;math/tex&quot;&gt;\varepsilon_i=1-y_i(w^Tx_i+b),\varepsilon_i \geq 0&lt;/script&gt;，那么此时最优化问题为 &lt;script type=&quot;math/tex&quot;&gt;min_{w,b} \sum_{i=1}^N\varepsilon_i + \lambda\|w\|^2&lt;/script&gt;，如果取&lt;script type=&quot;math/tex&quot;&gt;\lambda=\frac {1}{2C}&lt;/script&gt;，那么就和上述优化目标等价，所以可以看出，软间隔实际上是在ERM的基础上加了SRM~&lt;/p&gt;

&lt;p&gt;之后的推导没有多大差别，只不过在求导的过程中出现了&lt;script type=&quot;math/tex&quot;&gt;\alpha_i = C - \varepsilon_i&lt;/script&gt;，又有前面在对偶转换使用的KKT条件之一&lt;script type=&quot;math/tex&quot;&gt;\alpha_i(y_i(w^Tx_i+b) - 1) =0&lt;/script&gt;可得，在分类超平面上的点，也即满足&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i+b)-1 + \varepsilon_i = 0&lt;/script&gt;，而那些不在超平面的点，必然有&lt;script type=&quot;math/tex&quot;&gt;\alpha_i=0&lt;/script&gt;，而在超平面上的，则有&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt; \alpha_i &lt; C %]]&gt;&lt;/script&gt;，在超平面之外的则是&lt;script type=&quot;math/tex&quot;&gt;\alpha_i=C&lt;/script&gt;。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;nonlinear SVM&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;非线性情况之下，利用线性曲线去拟合，明显会产生underfitting，但是我们可以通过函数映射的方式，将原来空间中的非线性特征，映射到高维空间中，使得样本可分或者近似可分，实际上是，机器学习中有一种叫做基展开的技术，就是处理这种线性到非线性的特征映射。不过对于SVM中使用这种非线性变化是因为它能够和核函数配合的天衣无缝。&lt;/p&gt;

&lt;p&gt;这里用一个简单的例子作简要说明。对于&lt;script type=&quot;math/tex&quot;&gt;x=(x_1,x_2)&lt;/script&gt;二维空间的某个点，我们将其映射到三维空间。所利用的映射函数可以为&lt;script type=&quot;math/tex&quot;&gt;\phi (x_1,x_2) = (x_1^2,x_2^2,2x_1x_2)&lt;/script&gt;，那么在三维空间中，样本线性可分的可能性更大，但是计算开销却上升了，因为在转化成对偶问题之后就产生了向量内积运算。对于原始二维空间中的两点&lt;script type=&quot;math/tex&quot;&gt;p=(\eta_1,\eta_2),q=(\gamma_1,\gamma_2)&lt;/script&gt;，在三维空间中的向量内积为&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;\phi(\eta_1,\eta_2),\phi(\gamma_1,\gamma_2)&gt; = \eta_1^2\gamma_1^2 + \eta_2^2\gamma_2^2 + 4\eta_1\eta_2\gamma_1\gamma_2 %]]&gt;&lt;/script&gt;，这和&lt;script type=&quot;math/tex&quot;&gt;\eta_1^2\gamma_1^2 + \eta_2^2\gamma_2^2 + 2\eta_1\eta_2\gamma_1\gamma_2&lt;/script&gt;十分相似，而后者却等于&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;(\eta_1,\eta_2), (\gamma_1, \gamma_2)&gt;^2 %]]&gt;&lt;/script&gt;，所以只需要令&lt;script type=&quot;math/tex&quot;&gt;\phi(x_1,x_2) = (x_1^2, x_2^2, \sqrt {2} x_1x_2)&lt;/script&gt;，就可以得到&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;\phi(\eta_1,\eta_2),\phi(\gamma_1,\gamma_2)&gt; %]]&gt;&lt;/script&gt;=&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;(\eta_1,\eta_2), (\gamma_1, \gamma_2)&gt;^2 %]]&gt;&lt;/script&gt;~ 由此可以推广到高维。不难看出在高维空间中的内积可以通过在原始空间内积的平方得到~此时&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
K(p,q) = &lt;\phi(\eta_1,\eta_2), \phi(\gamma_1, \gamma_2)&gt; %]]&gt;&lt;/script&gt;=&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;p,q&gt;^2 %]]&gt;&lt;/script&gt;。对偶转换之后只需要将&lt;script type=&quot;math/tex&quot;&gt;x_i,x_j&lt;/script&gt;的内积运算更换成&lt;script type=&quot;math/tex&quot;&gt;K(x_i,x_j)&lt;/script&gt;，即可处理非线性数据~&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;SMO&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;SMO本质上上一种坐标上升优化算法，坐标上升可以理解为在&lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;维向量构成的空间中，每次选择一个维度进行优化，最终能够求得比较合适的解。SMO每次选择两个参数，因为此时待求变量有&lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^N\alpha_iy_i =0&lt;/script&gt;的约束。SMO的优化过程如下：&lt;/p&gt;

&lt;p&gt;选择&lt;script type=&quot;math/tex&quot;&gt;\alpha_i, \alpha_j&lt;/script&gt;
固定其他参数，然后对&lt;script type=&quot;math/tex&quot;&gt;\alpha_i, \alpha_j&lt;/script&gt;进行优化
利用&lt;script type=&quot;math/tex&quot;&gt;\alpha_i, \alpha_j&lt;/script&gt;，对截距进行优化
在了解确切的&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;贪心选择策略之前，先假定我们已经将&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;选择妥当，然后直接对&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;进行优化。根据条件&lt;script type=&quot;math/tex&quot;&gt;\sum_{k=1}^N\alpha_ky_k=0&lt;/script&gt;，令&lt;script type=&quot;math/tex&quot;&gt;A = y_i\sum_{k!=i,j}^N\alpha_ky_k&lt;/script&gt;. 那么&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;的关系为&lt;script type=&quot;math/tex&quot;&gt;\alpha_i = A - y_iy_j\alpha_j&lt;/script&gt;。此时我们的优化目标[max_{\alpha} L(\alpha) = \sum_{k=1}^N\alpha_k - \frac{1}{2} \sum_{l=1}^N\sum_{k=1}^N\alpha_l\alpha_ky_ly_kK_{l,k}]&lt;/p&gt;

&lt;p&gt;其中，&lt;script type=&quot;math/tex&quot;&gt;K_{l,k} = K(x_l, x_j)&lt;/script&gt;，令&lt;script type=&quot;math/tex&quot;&gt;B = \sum_{k!=i,j}\alpha_k, S = y_iy_j, V_i = \sum_{k!=i,j}\alpha_ky_kK_{i,k}, V_j =\sum_{k!=i,j}\alpha_ky_kK_{j,k}&lt;/script&gt;，那么有：&lt;/p&gt;

&lt;p&gt;\begin{eqnarray&lt;em&gt;} L(\alpha) &amp;amp;=&amp;amp; \sum_{k!=i,j}^N\alpha_k + \alpha_i + \alpha_j - \frac {1}{2}[\alpha_i\alpha_i y_iy_iK_{i,i} + \alpha_j\alpha_jy_jy_jK_{j,j} + 2\alpha_i\alpha_jy_iy_jK_{i,j} + 2\sum_{k!=i,j}^N\alpha_i\alpha_ky_iy_kK_{i,k} + 2\sum_{k!=i,j}^N\alpha_j\alpha_ky_jy_kK_{j,k} + \sum_{l!=i,j}^N \sum_{k!=i,j}^N\alpha_l\alpha_ky_ly_kK_{l,k}] \                                                               &amp;amp;=&amp;amp; B + A - S\alpha_j + \alpha_j - \frac{1}{2} [2\alpha_i\alpha_jSK_{i,j} + \alpha_i^2K_{i,i} + \alpha_j^2K_{j,j} + 2\alpha_iy_iV_i + 2\alpha_jy_jV_j +  \sum_{l!=i,j}^N \sum_{k!=i,j}^N\alpha_l\alpha_ky_ly_kK_{l,k}]    \                                                          &amp;amp;=&amp;amp;-S\alpha_j + \alpha_j  - \frac{1}{2}K_{i,i}(A-S\alpha_j)^2 - \frac{1}{2} K_{j,j}\alpha_j^2 - K_{i,j}\alpha_j(A-S\alpha_j)S - \alpha_jy_jV_j - (A-S\alpha_j)y_iV_i + \varepsilon_{constant} \end{eqnarray&lt;/em&gt;}&lt;/p&gt;

&lt;p&gt;其中，&lt;script type=&quot;math/tex&quot;&gt;\varepsilon_{constant}&lt;/script&gt;为一些常量，在求极值点是可以忽略，上式对&lt;script type=&quot;math/tex&quot;&gt;\alpha_j&lt;/script&gt;求导，有：&lt;/p&gt;

&lt;p&gt;\begin{eqnarray&lt;em&gt;} \frac {\partial_{L(\alpha_j)}} {\partial_{\alpha_j}} &amp;amp;=&amp;amp; -S + 1 + ASK_{i,i} -K_{i,i}\alpha_j - K_{j,j}\alpha_j -ASK_{i,j} + 2K_{i,j}\alpha_j -y_jV_j + Sy_iV_i=0 \ \alpha_j&amp;amp;=&amp;amp; \frac {-S + 1 + AS(K_{i,i}-K_{i,j}) + y_j(V_i-V_j)}{K_{i,i} + K_{j,j} - 2K_{i,j}} \end{eqnarray&lt;/em&gt;}&lt;/p&gt;

&lt;p&gt;又有优化&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;的时候，其他参数没有被改变。&lt;/p&gt;

&lt;p&gt;\begin{eqnarray&lt;em&gt;}\alpha_iy_i+\alpha_jy_j&amp;amp;=&amp;amp; -\sum_{k!=i,j}\alpha_k^{old}y_k=\alpha_i^{old}y_i + \alpha_j^{old}y_j \ V_i &amp;amp;=&amp;amp; \sum_{k!=i,j}\alpha_ky_kK_{i,k}=\sum_{k!=i,j}\alpha_k^{old}y_kK_{i,k} = \sum_{k=1}^N\alpha_k^{old}y_kK_{i,k} + b - b - \alpha_i^{old}y_iK_{i,i} - \alpha_j^{old}y_jK_{i,j} \ V_j &amp;amp; = &amp;amp;\sum_{k!=i,j}\alpha_ky_kK_{j,k}=\sum_{k!=i,j}\alpha_k^{old}y_kK_{j,k} = \sum_{k=1}^N\alpha_k^{old}y_kK_{j,k} + b - b - \alpha_j^{old}y_jK_{j,j} - \alpha_i^{old}y_iK_{i,j}  \end{eqnarray&lt;/em&gt;}&lt;/p&gt;

&lt;p&gt;且&lt;script type=&quot;math/tex&quot;&gt;g(x_i) = \sum_{k=1}^N\alpha_k^{old}y_kK_{i,k} + b&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;g(x_j) = \sum_{k=1}^N\alpha_k^{old}y_kK_{j,k} + b&lt;/script&gt;，所以：&lt;/p&gt;

&lt;p&gt;[V_i - V_j = g(x_i) - g(x_j) - \alpha_i^{old}y_iK_{i,i} + \alpha_j^{old}y_jK_{j,j} - \alpha_j^{old}y_jK_{i,j} + \alpha_i^{old}y_iK_{i,j} ]&lt;/p&gt;

&lt;p&gt;然后，将A = &lt;script type=&quot;math/tex&quot;&gt;\alpha_i^{old}+S\alpha_j^{old}&lt;/script&gt;，S=&lt;script type=&quot;math/tex&quot;&gt;y_iy_j&lt;/script&gt;代入&lt;script type=&quot;math/tex&quot;&gt;\alpha_j&lt;/script&gt;表达式，可得：&lt;/p&gt;

&lt;p&gt;\begin{eqnarray&lt;em&gt;}\alpha_j &amp;amp;=&amp;amp; \frac {y_jy_j - y_iy_j +(\alpha_i^{old}+y_iy_j\alpha_j^{old})y_iy_j(K_{i,i}-K_{i,j}) + y_j(g(x_i) - g(x_j) - \alpha_i^{old}y_iK_{i,i} + \alpha_j^{old}y_jK_{j,j} - \alpha_j^{old}y_jK_{i,j} + \alpha_i^{old}y_iK_{i,j})} {K_{i,i} + K_{j,j} - 2K_{i,j}} \ &amp;amp;=&amp;amp; \frac {y_j[y_j-y_i + y_j\alpha_j^{old}(K_{i,i} + K_{j,j} - 2K_{i,j} + g(x_i) - g(x_j) ]} {K_{i,i} + K_{j,j} - 2K_{i,j}}\ &amp;amp;=&amp;amp; \alpha_j^{old} + \frac{y_j[y_j-y_i + g(x_i)-g(x_j)]} {K_{i,i} + K_{j,j} - 2K_{i,j}}\end{eqnarray&lt;/em&gt;}&lt;/p&gt;

&lt;p&gt;然后令&lt;script type=&quot;math/tex&quot;&gt;E_i = g(x_i) - y_i&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;\eta = K_{i,i} + K_{j,j} - 2K{i,j}&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;\alpha_j^{new,unc} = \alpha_j^{old} + \frac {y_j(E_i - E_j)} {\eta}&lt;/script&gt;，此时求出的&lt;script type=&quot;math/tex&quot;&gt;\alpha_j&lt;/script&gt;还需要经过边界判定，对&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;有&lt;script type=&quot;math/tex&quot;&gt;\alpha_iy_i + \alpha_jy_j = \alpha_i^{old} + \alpha_j^{old}&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt;=\alpha_i&lt;=C %]]&gt;&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt;=\alpha_j&lt;=C %]]&gt;&lt;/script&gt;的条件限制，所以必须对&lt;script type=&quot;math/tex&quot;&gt;\alpha_i, \alpha_j&lt;/script&gt;的上下边界&lt;script type=&quot;math/tex&quot;&gt;L,H&lt;/script&gt;进行确认。&lt;/p&gt;

&lt;p&gt;如果&lt;script type=&quot;math/tex&quot;&gt;y_i = y_j&lt;/script&gt;，那么&lt;script type=&quot;math/tex&quot;&gt;L= max(0, \alpha_j^{old} - \alpha_i^{old})&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;H=min(C,C+\alpha_j^{old} - \alpha_i^{old})&lt;/script&gt;
如果&lt;script type=&quot;math/tex&quot;&gt;y_i!=y_j&lt;/script&gt;，那么&lt;script type=&quot;math/tex&quot;&gt;L=max(0, \alpha_j^{old} + \alpha_i^{old} - C)&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;H=min(C,\alpha_j^{old} + \alpha_i^{old})&lt;/script&gt;
根据上式可以得到&lt;script type=&quot;math/tex&quot;&gt;\alpha_j^{new}&lt;/script&gt;为：&lt;/p&gt;

&lt;p&gt;[\alpha_j^{new} = \begin{cases} H,~~~\alpha_j^{new,unc} &amp;gt; H \ \alpha_j^{new,unc},~~~~~ L&amp;lt;=\alpha_j^{new,unc}&amp;lt;=H \ L,~~~~~\alpha_j^{new,unc} &amp;lt; L \end{cases}]&lt;/p&gt;

&lt;p&gt;由此可以得到&lt;script type=&quot;math/tex&quot;&gt;\alpha_i^{new}= \alpha_i^{old} + y_iy_j(\alpha_j^{old} - \alpha_j^{new})&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;关于&lt;script type=&quot;math/tex&quot;&gt;alpha_i, \alpha_j&lt;/script&gt;的更新策略完成，但是对于上式中的bias也需要进行更新，以保证KKT条件&lt;script type=&quot;math/tex&quot;&gt;\alpha_j(y_j(\sum_{i=1}^N\alpha_iy_iK_{i,j} + b) - 1) = 0&lt;/script&gt;成立。&lt;/p&gt;

&lt;p&gt;当&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt; \alpha_i^{new} &lt; C %]]&gt;&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;y_i(\sum_{k=1}^N\alpha_ky_kK_{i,k}+b) = 1&lt;/script&gt; ，所以&lt;script type=&quot;math/tex&quot;&gt;b_1^{new} = y_i - \sum_{k!=i,j}^N\alpha_ky_kK_{i,k} - \alpha_i^{new}y_iK_{i,i} - \alpha_j^{new}y_jK_{i,j}&lt;/script&gt;，又知&lt;script type=&quot;math/tex&quot;&gt;E_i = g(x_i) - y_i = \sum_{k=1}^N\alpha_ky_kK_{i,k} + b^{old} - y_i&lt;/script&gt;，此时除&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;以外的都不会发生变化，所以&lt;script type=&quot;math/tex&quot;&gt;E_i = \sum_{k!=i,j}^N\alpha_ky_kK_{k,i} + \alpha_i^{old}y_iKii + \alpha_j^{old}y_jK_{i,j} + b^{old} - y_i&lt;/script&gt;，也即 &lt;script type=&quot;math/tex&quot;&gt;b_i^{new} = -E_i - y_iK_{i,i}(\alpha_i^{new} - \alpha_i^{old}) - y_jK_{i,j}(\alpha_j^{new} - \alpha_j^{old}) + b^{old}&lt;/script&gt;
同1，当&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt; \alpha_j^{new} &lt; C %]]&gt;&lt;/script&gt;，则&lt;script type=&quot;math/tex&quot;&gt;b_2^{new} = -E_j - y_iK_{i,j}(\alpha_i^{new} - \alpha_i^{old}) - y_jK_{j,j}(\alpha_j^{new} - \alpha_j^{old}) + b^{old}&lt;/script&gt;
如果&lt;script type=&quot;math/tex&quot;&gt;\alpha_i^{new}&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;\alpha_j^{new}&lt;/script&gt;都满足条件，则&lt;script type=&quot;math/tex&quot;&gt;b_1^{new}&lt;/script&gt;=&lt;script type=&quot;math/tex&quot;&gt;b_2^{new}&lt;/script&gt;
如果&lt;script type=&quot;math/tex&quot;&gt;\alpha_i^{new}&lt;/script&gt;、&lt;script type=&quot;math/tex&quot;&gt;\alpha_j^{new}&lt;/script&gt;为0或者C，那么取&lt;script type=&quot;math/tex&quot;&gt;b_1^{new}&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;b_2^{new}&lt;/script&gt;的中值即可
至此，SMO算法结束，不过实际中，需要实时更新&lt;script type=&quot;math/tex&quot;&gt;E_i&lt;/script&gt;，所以在更新完bias之后，再利用已有的信息重新更新&lt;script type=&quot;math/tex&quot;&gt;E_i&lt;/script&gt;即可。&lt;/p&gt;

&lt;p&gt;最终实现见：SVM@GITHUB&lt;/p&gt;

&lt;p&gt;参考资料&lt;/p&gt;

&lt;p&gt;李航 《统计学习方法》&lt;/p&gt;

&lt;p&gt;http://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html&lt;/p&gt;

</description>
        <pubDate>Sat, 02 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/02/SVM.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/02/SVM.html</guid>
        
        
        <category>machinelearning</category>
        
      </item>
    
      <item>
        <title>Random Forest</title>
        <description>&lt;p&gt;一般我们提到序列标注问题，会有三种模型很引人瞩目，最大熵马尔科夫模型、隐马尔科夫模型和条件穗机场模型。三种的联系和区别详见：隐马尔可夫模型 最大熵马尔可夫模型 条件随机场 区别和联系。&lt;/p&gt;

&lt;h3 id=&quot;hmm&quot;&gt;HMM简单介绍&lt;/h3&gt;

&lt;p&gt;在介绍CRF建模过程，我们先简单的了解下HMM，因为HMM可以认为是CRF的子集，任何可以由HMM解决的问题都可以由CRF解决~&lt;/p&gt;

&lt;p&gt;HMM有两种假设：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;某种状态只依赖于前一状态，也即隐含状态满足一阶马尔科夫性质；&lt;/li&gt;
  &lt;li&gt;观测状态之间彼此独立&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面用一个简单的中文分词来介绍HMM吧。
一般在中文分词中，我们会设定某个中文汉字所处的状态（B:词的开头；M：在词中间；E：在词结尾；S：单字成词），这四种状态（BMES）可以认为是我们的隐含状态，而该状态对应的中文字符，则是我们的观测状态。&lt;/p&gt;

&lt;p&gt;通过大量的语料的统计可以得到这四种状态之间的转移概率矩阵p(SJ|SI)p(SJ|SI)
即由状态SISI转移到SJSJ的概率。同时也可以统计出初始状态的概率π(SI)π(SI)，另外我们的发射概率（也即有隐含状态（BMES）生成汉字的概率）也可以通过统计得出，当然了，也可以利用EM算法去训练，但在此不再赘述。&lt;/p&gt;

&lt;p&gt;HMM一般会解决三类问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;模型学习，可以使用统计也可以使用EM；&lt;/li&gt;
  &lt;li&gt;给定观测状态，确定最可能的隐含状态序列，比如中文分词、词性标注等；&lt;/li&gt;
  &lt;li&gt;给定隐含状态，确定最可能的观测序列，比如天气预报。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;中文分词给定的是观测序列（即一个个的中文字符），任务即是找到某种最可能的隐含状态，也即判断中文字符所处的位置(BMES)，使得该观测序列产生的概率最大。在进行计算的时候，我们不可能对所有的序列进行判断， 这将是一个NP问题。下图是对“我来自武汉”的一个HMM模型图示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/image/t.png&quot; alt=&quot;HMM图片&quot; /&gt;&lt;/p&gt;

&lt;p&gt;途中蓝色部分为隐含状态之间的转移概率矩阵，蓝线表示状态的转移。红线表示由隐含状态生成观测状态。计算的时候，我们可以根据迭代公式计算出前向概率，然后反向递推，得到生成该序列最大概率的隐含状态序列。比如上图中最后最有可能得到的生成“我、来、自、武、汉”这一观测序列的隐含状态序列为“SBEBE”，也即分词结果为”我\来自\武汉”。&lt;/p&gt;

&lt;h3 id=&quot;crf&quot;&gt;CRF&lt;/h3&gt;

&lt;p&gt;HMM的假设简单粗暴，却行之有效，然而在分词或者词性标注这些应用上，却无法达到CRF所取得的效果，原因之一正是在于其假设。HMM过于local，导致其损失了非常多global的信息，而CRF却可以利用这些信息，从而可以得到更好的结果。&lt;/p&gt;

&lt;p&gt;CRF自从被Lafferty等人发明之后，由最初的应用于序列标注的自然语言处理领域广泛的扩散至生物信息学、计算机视觉等领域。CRF中十分重要的一 个概念为特征函数，其是状态转移特征函数和状态生成特征函数的集合。其实刚开始看CRF的时候这个特征函数很不容易理解，但是我们可以把它理解成一个对某种规则的一种特征化处理，比如在词性标注中，在训练的时候如果当前词为“形容词”，并且它以“的”结尾，那么该特征函数输出为1，否则为0。并且我们需要对每一个特征函数赋以某种权值，则如果最后的结果中该特征和权重的积较大，可以表征：以”的“结尾的词以某种概率是形容词“，而这个权重λλ是模型训练的目标。&lt;/p&gt;

&lt;p&gt;参考文献&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://wenku.baidu.com/link?url=7LBbXiKPWAPnqYexmBOhz4iCUSny6Ayg3M53Ls0IiVKdqLq-9YPNAiW3WKJ5UgihjWKmm4yTpahIIeu75BB_mM_Q1QicaLIGrOiwHUO8ktu###&quot;&gt;百度文库&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/felomeng/article/details/4367250&quot;&gt;http://blog.csdn.net/felomeng/article/details/4367250&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sat, 02 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/02/RandomForests.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/02/RandomForests.html</guid>
        
        
        <category>machinelearning</category>
        
      </item>
    
      <item>
        <title>HMM</title>
        <description>&lt;h1 id=&quot;hmm&quot;&gt;HMM&lt;/h1&gt;

&lt;p&gt;在了解隐马尔科夫模型之前，先得了解马尔科夫过程，一般而言，在理解一个模型的时候，最好是能够知道该模型提出的初衷，站在设计者的角度来思考模型总能达到事半功倍的效果。
n阶马尔科夫过过程的当前状态只依赖于前n个时刻的状态，通常而言，n一般为1，也即常见的马尔科夫过程。##&lt;/p&gt;

&lt;p&gt;隐马尔科夫模型在基础的马尔科夫过程中做了简单的变换，将状态作为一种隐含状态，观测序列作为观测状态，并且状态之间有概率转移关系，状态到观测序列之间也有概率转移关系。
由此可以得到隐马尔科夫模型的形式化表示：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;HMM=(pi,A,B)HMM=(pi,A,B)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其中，pi为系统的初始状态，A为状态转移矩阵，其中AijAij表示从隐含状态i变成隐含状态j的概率，B为混淆矩阵或者观测状态生成矩阵，B)ijB)ij表示隐含状态i生成观测序列元素j的概率。
在实例讲解隐马尔科夫模型的例子中，有一个较为经典，也即海藻和天气。海藻的状态我们随时可以观测得到，而天气的状态是一种未知的变量。假设我们知道天气之间变化的概率A，也知道当海藻处于某种观测状态的前提下各种天气状态的出现概率B，同时也有初始状态，也即给出了一个HMM模型，那么我们可以解决以下两种问题：&lt;/p&gt;

&lt;p&gt;1.给定海藻的观测状态序列，求出该观测序列出现的概率，对应于HMM三大问题之概率计算问题&lt;br /&gt;
2.给定某种观测状态序列，求出该观测序列出现的最大的状态概率，对应于HMM三大问题之预测问题&lt;/p&gt;

&lt;p&gt;对于第一种问题，可以使用最为原始的暴力出奇迹，枚举所有的状态，求出状态和观测序列的联合概率分布，然后进行求和，即可得到最后的结果，也可以使用更为优雅的算法，比如即将介绍的基于动态规划的forward-backward算法，这两种算法也是HMM这类概率图模型所特有的一种求解方法，就像是BP之于NN一般。
1&amp;gt; forword-algorithm
前向算法，是一种动态规划算法，之前看NLP的另外一个算法中也使用了动态规划的思想，它具有许多独特的性质，比如子问题最优，也即全局最优可以通过枚举子问题最优得到，联想到HMM的图模型结构，我们不难得知，在时刻t处于状态i的情况(此时观测序列为O1,O2,O3,…,Ot)下，它可以由多种子状态按照HMM的性质转化而来，即可以由时刻t-1处于状态j的情况(此时观测序列为O1,O2,O3,…,Ot-1)首先从状态i转换到j，然后生成观测序列Ot得到，所以可以得到如下的DP状态转移方程：
Alpha[t][i] = sigma(j=1,N){Alpha[t-1][j] * A[j][i]} * B[i][t]
上述方程也可以通过贝叶斯公式推出来。由此，问题1可以通过累加Alpha[T][i]得到。&lt;/p&gt;

&lt;p&gt;2&amp;gt; Backward-algorithm
有了前向算法，后向算法就很容易理解了，在时刻t处于状态i的前提下，观测序列为O(t+1)…O(T)的情况可以由在时刻t+1处于状态j的前提下的情况，首先从状态j转移到状态i，然后生成观测序列O(t+1)的过程得到，也即：
Beta[t][i] = sigma(j=1,N){Beta[t+1][j] * A[j][i] * B[j][t+1]}&lt;/p&gt;

&lt;p&gt;由此，问题1也可以通过Beta计算得到.&lt;/p&gt;

&lt;p&gt;HMM的基于统计的train的过程以及decode的过程见 HMM ，其中还给出了HMM在分词中的应用～&lt;/p&gt;

</description>
        <pubDate>Sat, 02 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/02/HMM.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/02/HMM.html</guid>
        
        
        <category>machinelearning</category>
        
      </item>
    
      <item>
        <title>ESL Notes</title>
        <description>&lt;p&gt;对ml的大部分模型有了了解之后，混沌中遇到师兄赠书，便要了ESL，度厂实习期间，看的晕晕乎乎，英文略显晦涩，但是坚持至今，现在最大的障碍是太多细节和精髓无法掌握，只好做些笔记，帮助理解。借助豆瓣笔记强大的latex支持，以后的笔记都会发在豆瓣，这里只是提供外链~&lt;/p&gt;

&lt;p&gt;第1章 略&lt;/p&gt;

&lt;p&gt;第2章 Overview of Supervised Learning&lt;/p&gt;

&lt;p&gt;第3章 Linear Methods for Regression&lt;/p&gt;

&lt;p&gt;第4章 Linear Methods for Classification&lt;/p&gt;

&lt;p&gt;第5章 Basis Expansions And Regularization&lt;/p&gt;

&lt;p&gt;第6章 Kernel Methods&lt;/p&gt;

&lt;p&gt;第7章 Model Assessment And Selection&lt;/p&gt;

&lt;p&gt;第8章 Model Inference And Averaging&lt;/p&gt;

&lt;p&gt;第9章 Additive Models And Trees&lt;/p&gt;

&lt;p&gt;第10章 Boosting And Additive Trees&lt;/p&gt;

&lt;p&gt;第11章 Neural Networks&lt;/p&gt;

&lt;p&gt;第12-14章节 略&lt;/p&gt;

</description>
        <pubDate>Sat, 02 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/02/ESL_Notes.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/02/ESL_Notes.html</guid>
        
        
        <category>machinelearning</category>
        
      </item>
    
      <item>
        <title>Dynamic Programming</title>
        <description>&lt;h3 id=&quot;dp&quot;&gt;状态压缩DP&lt;/h3&gt;
&lt;p&gt;状态压缩一般适用于状态非常多的动态规划问题，之所以叫状态压缩DP，是因为在动态规划的过程中，通过位压缩技术，可以压缩状态空间，而状态压缩问题一般都伴随着问题的条件限制，比如说相邻不能相同，或者相隔n个单元不能相同等。&lt;/p&gt;

&lt;p&gt;来个例子说明这个问题&lt;/p&gt;

&lt;p&gt;对于poj 3254，问题抽离出来的大意是，有个一部分空格无法填入值的矩阵，然后让你在剩余空格中填入0或者1，并且相邻空格不能填1，问最终由多少种填法。&lt;/p&gt;

&lt;p&gt;我们将一行填入0,1之后看成一个二进制数state，但是此时如果列数很多的情况的话，时间复杂度还是不可小觑的，所以当加入一些条件，比如上述问题中的相邻空格不能为1，就可以将状态进一步压缩，对于某个状态i，如果i &amp;amp; (i « 1) 不为0 的话，说明此时i的二进制格式中，必然有相邻的1。所以进一步压缩，新的状态总数必然要减少很多。而新的state可以作为这一行的此时的状态，当然是否合法还需要按照当前行的可填入空格的情况而论的。&lt;/p&gt;

&lt;p&gt;稍微分析可知，当第i行状态为state时，此时总的方案数必然是第i-1行合法状态的累加。&lt;/p&gt;

&lt;p&gt;状态表示：dp[i][state] 表示第i行的状态为state，此时获得的最大方案数；&lt;/p&gt;

&lt;p&gt;状态方程转移：dp[i][state] = Add(dp[i - 1][_state])，此时state为第i行状态，_state为第i-1行状态；&lt;/p&gt;

&lt;p&gt;代码见：https://github.com/kymo/acm/blob/master/POJ/ztysdp_3254.cpp&lt;/p&gt;

&lt;h3 id=&quot;dp-1&quot;&gt;树形DP&lt;/h3&gt;

&lt;p&gt;由于树本身的递归特性，所以在树上进行DP则是再好不过的了，父节点的状态在递归的时候，通过栈返回的子节点的状态得到更新。这里举出两个例子。&lt;/p&gt;

&lt;p&gt;例(poj2432):
有n个人，每个人有个给定的价值，并给出两两之间的上下级关系，求一个集合，使得集合元素不存在上下级关系，且他们的价值和最大？&lt;/p&gt;

&lt;p&gt;稍微分析一下，得知某个节点的状态有两种，选入集合或者不选入集合。由此我们可以令：
D[i][0]表示不选节点i，以i节点为根节点的子树得到的最大值
D[i][1]表示选了节点i，以i节点为根节点的子树得到的最大值
由此可以得到我们的DP方程如下：&lt;/p&gt;

&lt;p&gt;代码:&lt;a href=&quot;https://github.com/kymo/acm/blob/master/POJ/2342.cpp&quot;&gt;https://github.com/kymo/acm/blob/master/POJ/2342.cpp&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;POJ 1463题目大意如此，只不过在本题是安排士兵的位置，状态方程和&lt;/p&gt;

&lt;p&gt;例(HDU 2196)
有n(n 最大为10^4)个节点的树，每条边有一定的距离，求距离每一个节点最远的距离值。
按照常规思路的话，可能是用最短路算法，但是n的数据范围不允许我们如此暴力。仔细分析下，在树的父子结构一定的情况下，每个节点最长的距离的值必然和子节点和父节点相关，&lt;/p&gt;

</description>
        <pubDate>Sat, 02 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/algorithm/2016/04/02/DP.html</link>
        <guid isPermaLink="true">http://kymo.github.io/algorithm/2016/04/02/DP.html</guid>
        
        
        <category>algorithm</category>
        
      </item>
    
      <item>
        <title>Bit Manupulation</title>
        <description>&lt;p&gt;Using bit manupulation to solve the Eight-Queen &amp;amp; Sudoku Problems. Sometime it seems that using bit manupulation can make the problem easy to be interpretated.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sodoku Problem&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;01 class Solution {
02     public: bool dfs(vector &amp;lt; int &amp;gt; &amp;amp;empty_sells, vector &amp;lt; vector &amp;lt; char &amp;gt; &amp;gt;&amp;amp;board, int start, int * col, int * row, int* grid) {
03         if (start == empty_sells.size()) return true;
04         int r = empty_sells[start] / 9;
05         int c = empty_sells[start] % 9;
06         int cur_val = col
[c][/c]
 | row[r] | grid[(r / 3) * 3 + c / 3];
07
08         for (int j = 1; j &amp;lt;= 9; j++) {
09             if ((cur_val | (1 « (j - 1))) ^ cur_val) {
10                 col
[c][/c]
 |= (1 « (j - 1));
11                 row[r] |= (1 « (j - 1));
12                 grid[(r / 3) * 3 + c / 3] |= (1 « (j - 1));
13                 board[r]
[c][/c]
 = ‘0’ + j;
14                 if (dfs(empty_sells, board, start + 1, col, row, grid)) {
15                     return true;
16                 }
17                 board[r]
[c][/c]
 = ‘.’;
18                 col
[c][/c]
 ^= (1 « (j - 1));
19                 row[r] ^= (1 « (j - 1));
20                 grid[(r / 3) * 3 + c / 3] ^= (1 « (j - 1));
21             }
22         }
23         return false;
24
25     }
26     void solveSudoku(vector &amp;lt; vector &amp;lt; char &amp;gt; &amp;gt;&amp;amp;board) {
27         int col[9],
28         row[9],
29         grid[9];
30         memset(col, 0, sizeof(col));
31         memset(row, 0, sizeof(row));
32         memset(grid, 0, sizeof(grid));
33         vector &amp;lt; int &amp;gt; empty_sells;
34         for (int j = 0; j &amp;lt; board.size(); j++) {
35             for (int k = 0; k &amp;lt; board[0].size(); k++) {
36                 if (board[j][k] != ‘.’) {
37                     row[j] |= (1 « (board[j][k] - ‘0’ - 1));
38                     col[k] |= (1 « (board[j][k] - ‘0’ - 1));
39                     grid[(j / 3) * 3 + k / 3] |= (1 « (board[j][k] - ‘0’ - 1));
40                 } else {
41                     empty_sells.push_back(j * 9 + k);
42                 }
43             }
44         }
45         dfs(empty_sells, board, 0, col, row, grid);
46     }
47 };&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Eight Queens&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;01 void search(int n, vector&lt;string&gt; &amp;amp;matrix, int cur, int col, int left, int right) {
02         if (cur == n) {
03             ans.push_back(matrix);
04             return;
05         }
06         for (int j = 0; j &amp;lt; n; j ++) {
07             if (((col &amp;gt;&amp;gt; j) % 2) || ((left &amp;gt;&amp;gt; (cur + j)) % 2) || ((right &amp;gt;&amp;gt; (cur + n - 1 - j)) % 2)) continue;
08             matrix[cur][j] = &#39;Q&#39;;
09             search(n, matrix, cur + 1, col | (1 &amp;lt;&amp;lt; j), left | (1 &amp;lt;&amp;lt; (cur + j)), right | (1 &amp;lt;&amp;lt; (cur + n - 1 - j)));
10             matrix[cur][j] = &#39;.&#39;;
11         }
12     }
13     vector&amp;lt;vector&lt;string&gt; &amp;gt; solveNQueens(int n) {
14         col = left = right = 0;
15         vector&lt;string&gt; matrix;
16         string str = &quot;&quot;;
17         for (int j = 0; j &amp;lt; n; j ++) str += &quot;.&quot;;
18         for (int i = 0; i &amp;lt; n; i ++) {
19             matrix.push_back(str);
20         }
21         search(n, matrix, 0, 0, 0, 0);
22         return ans;
23     }&lt;/string&gt;&lt;/string&gt;&lt;/string&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 02 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/algorithm/2016/04/02/Bit.html</link>
        <guid isPermaLink="true">http://kymo.github.io/algorithm/2016/04/02/Bit.html</guid>
        
        
        <category>algorithm</category>
        
      </item>
    
  </channel>
</rss>
