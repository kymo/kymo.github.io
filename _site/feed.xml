<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aron blog</title>
    <description>nihao
</description>
    <link>http://kymo.github.io/</link>
    <atom:link href="http://kymo.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 02 Mar 2017 23:57:15 +0800</pubDate>
    <lastBuildDate>Thu, 02 Mar 2017 23:57:15 +0800</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>RNN</title>
        <description>&lt;h2 id=&quot;递归神经网络的一些小事儿&quot;&gt;递归神经网络的一些小事儿&lt;/h2&gt;

&lt;h3 id=&quot;1-从神经网络谈起&quot;&gt;1. 从神经网络谈起&lt;/h3&gt;

&lt;p&gt;了解神经网络的都知道，神经网络作为一种非线性模型，在监督学习领域取得了state-of-art的效果，其中反向传播算法的提出居功至伟，到如今仍然是主流的优化神经网络参数的算法. 递归神经网络、卷积神经网络以及深度神经网络作为人工神经网络的”变种”，仍然延续了ANN的诸多特质，如权值连接，激励函数，以神经元为计算单元等，只不过因为应用场景的不同衍生了不同的特性，如：处理变长数据、权值共享等。&lt;/p&gt;

&lt;p&gt;为了介绍RNN，先简单的介绍ANN. ANN的结构很容易理解，一般是三层结构（输入层-隐含层-输出层）. 隐含层输出&lt;script type=&quot;math/tex&quot;&gt;o_j&lt;/script&gt; 和输出层输出&lt;script type=&quot;math/tex&quot;&gt;o_k&lt;/script&gt;如下。其中&lt;script type=&quot;math/tex&quot;&gt;net_j&lt;/script&gt;为隐含层第&lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;个神经元的输入,&lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt;为输入层和隐含层的连接权值矩阵，&lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;为隐含层和输出层之间的连接权值矩阵.&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
o_j &amp;amp; =f(net_j) \\
o_k &amp;amp; =f(net_k) \\ 
net_j &amp;amp; =\sum_i(x_{i}u_{i,j})+b_j \\
net_k &amp;amp; =\sum_j(o_{j}v_{j,k})+b_k
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;定义损失函数为&lt;script type=&quot;math/tex&quot;&gt;E_p=\frac{1}{2}\sum_k (o_k - d_k)^2&lt;/script&gt; ,其中&lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;为样本下标，&lt;script type=&quot;math/tex&quot;&gt;o^k&lt;/script&gt;为第&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;个输出层神经元的输出,&lt;script type=&quot;math/tex&quot;&gt;d^k&lt;/script&gt;为样本在第$k$个编码值。然后分别对参数&lt;script type=&quot;math/tex&quot;&gt;v_{j,k}&lt;/script&gt;、&lt;script type=&quot;math/tex&quot;&gt;u_{i,j}&lt;/script&gt; 进行求导，可得：&lt;/p&gt;
&lt;p&gt;
$$
\begin{align}
\frac{\partial E_p}{\partial v_{j,k}} &amp;amp; = \frac{\partial E_p}{\partial net_k} \frac{\partial net_k}{\partial v_{j,k}} \\
&amp;amp; = \frac{\partial E_p}{\partial net_k}o_j \\
&amp;amp; = \frac{\partial E_p}{\partial o_k}\frac{\partial o_k}{\partial net_k}o_j \\
&amp;amp; = (o_k-d_k)o_k(1-o_k)o_j
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;
$$
\begin{align}
\frac{\partial E_p} {\partial u_{i,j}} &amp;amp; = \frac{\partial E_p} {\partial net_j} \frac{\partial net_j} {\partial u_{i,j}} \\
&amp;amp; =x_i \sum_k \frac{\partial E_p} {\partial net_k} \frac{\partial net_k}{\partial o_j} \frac{\partial o_j}{\partial net_j}  \\
&amp;amp; =x_i \sum_k \frac{\partial E_p}{\partial net_k} v_{j,k} o_j(1-o_j) \\
&amp;amp; = x_i o_j(1-o_j) \sum_k \frac{\partial E_p}{\partial net_k} v_{j,k} 
\end{align}
$$
&lt;/p&gt;

&lt;p&gt;从对$\frac{\partial E_p} {\partial u_{i,j}}$的推导可以得到反向传播的核心思路，令误差项$\beta_k = \frac{\partial E_p} {\partial net_k}$, 则有：&lt;/p&gt;

&lt;p&gt;
$$
\beta_k=o_l(1-o_l)\sum_l\beta_lw_{lk}
$$
&lt;/p&gt;

&lt;p&gt;反向传播的实质是基于梯度下降的优化方法，只不过在优化的过程使用了一种更为优雅的权值更新方式。&lt;/p&gt;

&lt;h3 id=&quot;2-循环神经网络&quot;&gt;2. 循环神经网络&lt;/h3&gt;

&lt;p&gt;传统的神经网络一般都是全连接结构，且非相邻两层之间是没有连接的。一般而言，定长输入的样本很容易通过神经网络来解决，但是类似于NLP中的序列标注这样的非定长输入，前向神经网络却无能为力。&lt;/p&gt;

&lt;p&gt;于是有人提出了循环神经网络(Recurrent Neural Network)，这是一种无法像前向神经网络一样有可以具象的网络结构的模型，一般认为是网络隐层节点之间有相互作用的连接，其实质可以认为是多个具有相同结构和参数的前向神经网络的stacking, 前向神经网络的数目和输入序列的长度一致，且序列中毗邻的元素对应的前向神经网络的隐层之间有互联结构，其图示( &lt;a href=&quot;http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/&quot;&gt;来源&lt;/a&gt; )如下.
:&lt;img src=&quot;http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图只是一个比较抽象的结构，下面是一个以时间展开的更为具体的结构(&lt;a href=&quot;http://www.cnblogs.com/YiXiaoZhou/p/6058890.html&quot;&gt;来源&lt;/a&gt;)~
:&lt;img src=&quot;http://images2015.cnblogs.com/blog/1027162/201611/1027162-20161113162111280-1753976877.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从图中可以看出，每个输出层神经元的输出和前向神经网络&lt;/p&gt;

</description>
        <pubDate>Fri, 02 Dec 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/12/02/RNN.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/12/02/RNN.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>轻量应用服务器架构</title>
        <description>&lt;h1 id=&quot;sub&quot;&gt;SUB&lt;/h1&gt;

&lt;h2 id=&quot;socket&quot;&gt;SOCKET&lt;/h2&gt;

&lt;h3 id=&quot;设置为非阻塞模式可以设置selecteoll-io模型将&quot;&gt;设置为非阻塞模式，可以设置SELECT/EOLL IO模型.将&lt;/h3&gt;
&lt;p&gt;Read, Write, Accept作为Reactor函数传递给IO模型&lt;/p&gt;

&lt;h3 id=&quot;有新的连接发过来数据利用解析任务解析该任务然&quot;&gt;有新的连接发过来数据，利用解析任务解析该任务，然&lt;/h3&gt;
&lt;p&gt;后将解析出来的数据按照类型生成新的对应任务丢到队
列中. 任务处理完成之后，设置该FD为写，并将写的容
一并返回.&lt;/p&gt;

&lt;h2 id=&quot;io&quot;&gt;IO&lt;/h2&gt;

&lt;h3 id=&quot;select&quot;&gt;Select&lt;/h3&gt;

&lt;h4 id=&quot;使用read_set和write_set处理连接和读写并接受&quot;&gt;使用read_set和write_set处理连接和读写，并接受&lt;/h4&gt;
&lt;p&gt;server 传递过来的回调函数指针作为事件处理方法&lt;/p&gt;

&lt;h3 id=&quot;epoll&quot;&gt;Epoll&lt;/h3&gt;

&lt;h4 id=&quot;todo&quot;&gt;TODO&lt;/h4&gt;

&lt;h2 id=&quot;taskmgr&quot;&gt;TaskMgr&lt;/h2&gt;

&lt;h3 id=&quot;task&quot;&gt;task&lt;/h3&gt;

&lt;h4 id=&quot;对任务的封装单个任务会有自己运行的回掉函数以&quot;&gt;对任务的封装，单个任务会有自己运行的回掉函数，以&lt;/h4&gt;
&lt;p&gt;及运行时的参数空间,运行run完成之后，这个run会将
数据写回缓冲区. 然后调用call_back函数，将结果写
会fd,不同的任务会继承该task, 计算任务可以由用户指
定运行的回调函数.&lt;/p&gt;

&lt;h3 id=&quot;task_handler&quot;&gt;task_handler&lt;/h3&gt;

&lt;h4 id=&quot;作为线程池的thead_handler每次都会从任务队列&quot;&gt;作为线程池的thead_handler，每次都会从任务队列&lt;/h4&gt;
&lt;p&gt;中取一个任务进行运行&lt;/p&gt;

&lt;h2 id=&quot;threadmgr&quot;&gt;ThreadMgr&lt;/h2&gt;

&lt;h3 id=&quot;thread&quot;&gt;thread&lt;/h3&gt;

&lt;h4 id=&quot;对线程的封装线程运行函数由thread_handler指定&quot;&gt;对线程的封装，线程运行函数由thread_handler指定&lt;/h4&gt;

&lt;h3 id=&quot;thread_handler&quot;&gt;thread_handler&lt;/h3&gt;

&lt;h4 id=&quot;线程真正的实现逻辑不同类型的线程有不同的handl&quot;&gt;线程真正的实现逻辑，不同类型的线程有不同的handl&lt;/h4&gt;
&lt;p&gt;er去处理，对于线程池中的handler会不断的从taskM
gr中取task进行处理&lt;/p&gt;

&lt;h3 id=&quot;thread_pool&quot;&gt;thread_pool&lt;/h3&gt;

&lt;h4 id=&quot;线程池模型根据配置中的线程数启动每个线程分配一&quot;&gt;线程池模型,根据配置中的线程数启动每个线程分配一&lt;/h4&gt;
&lt;p&gt;个handler&lt;/p&gt;

&lt;h2 id=&quot;pluginmgr&quot;&gt;PluginMgr&lt;/h2&gt;

&lt;h2 id=&quot;dictmgr&quot;&gt;DictMgr&lt;/h2&gt;

&lt;h2 id=&quot;configmgr&quot;&gt;ConfigMgr&lt;/h2&gt;

&lt;h2 id=&quot;memorymgr&quot;&gt;MemoryMgr&lt;/h2&gt;
</description>
        <pubDate>Tue, 20 Sep 2016 00:00:00 +0800</pubDate>
        <link>http://kymo.github.io/2016/09/20/%E8%BD%BB%E9%87%8F%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%9E%B6%E6%9E%84.html</link>
        <guid isPermaLink="true">http://kymo.github.io/2016/09/20/%E8%BD%BB%E9%87%8F%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%9E%B6%E6%9E%84.html</guid>
        
        
      </item>
    
      <item>
        <title>CRF 笔记</title>
        <description>&lt;h1 id=&quot;crf-笔记&quot;&gt;CRF 笔记&lt;/h1&gt;

&lt;h5 id=&quot;dgm转换成ugm&quot;&gt;DGM转换成UGM&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;节点之间连接&lt;/li&gt;
  &lt;li&gt;节点的双亲连接，有节点都指向同一个节点，说明两者之间有关联&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;两者之间还是有关联的&lt;/p&gt;

</description>
        <pubDate>Tue, 16 Aug 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/08/16/CRF.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/08/16/CRF.html</guid>
        
        <category>CRF</category>
        
        <category>NLP</category>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>论文阅读笔记</title>
        <description>&lt;h3 id=&quot;learning-n-best-correction-models-from-implicit-user-feedback-in-a-muti-modal-local-search-application&quot;&gt;Learning N-best Correction Models from Implicit User Feedback in a Muti-Modal Local Search Application&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;通过用户的反馈对语音识别的结果进行纠错，通过点击数据构造result confusion matrix,然后利用该矩阵对结果进行重排&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通常而言，用户的点击数据可以用来对已有的声学模型或者语言模型进行概率，当然也可以利用这些数据对语言识别的结果进行纠正，本文作者想要构建的就是基于用户数据的纠错模型。&lt;/p&gt;

&lt;p&gt;本文的工作和主流的改写流程差不多，主要包含了两个方面：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;语音识别系统首先会给出n-best候选，不同于一般的候选，这里会增加一部分通过点击数据统计出来的额外的候选。&lt;/li&gt;
  &lt;li&gt;接着进行ReScore，构建更加精确的n-best list&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于第一步，需要从user click data中统计出result confusion matrix,该矩阵揭示了当展现某一个query的时候，用户点击其他query的频次，比如对于识别系统识别出来的n-best list:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Sterling  Stirling  Burlington  Cooling&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过点击数据可以构造如下的result confusion matrix:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Bar&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Bowling&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Burgerking&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;…&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Burlington&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Sterling&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Burlington&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;…&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Cooling&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;…&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Sterling&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;…&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Stirling&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;…&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;得到这个result confusion matrix之后，就可以将原有的n-best list进行扩充(在矩阵中共现过便可加入），得到如下结果：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Sterling  Striling  Burlington  Cooling  Bar  Bowling   Burgetking&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对于第二步，需要利用矩阵的统计频次.$Score(word_i)=\sum_{i=0}C(word_i, otherword)$，其中$word_i$表示第$i$个n-best list的元素，$otherword$表示用户在展现$word_i$之后点击的$word$.&lt;/p&gt;
</description>
        <pubDate>Tue, 16 Aug 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/08/16/PaperReadingNote.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/08/16/PaperReadingNote.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>Ensemble Learning Methods</title>
        <description>&lt;p&gt;集成学习的关键是弱学习器的组合，目标是为了提升模型的性能，一般用在Model Selection，此时降低了选择弱名的可能性。当然集成学习也广泛的应用于结果置信度检验、特征选择、数据融合、增量学习等方面，取得了良好的效果。&lt;/p&gt;

&lt;p&gt;一般而言，对于给定的任务，比如分类，首先在处理完特征之后，我们需要考虑的是选择何种模型来对这些样本进行训练。一般我们没有样本数据产生过程的先验知识，只能通过手动的选择调参，才有可能得到一个较好拟合已知样本的模型。集成学习给了一个非常完美的模型选择的解决方案，通过训练多个子模型，组成committee，最后综合决策，达到了自动选择模型的效果。&lt;/p&gt;

&lt;p&gt;一般模型的性能某种程度取决于数据集的大小，也即观测样本的覆盖度。数据集过大对于一般模型可能会导致训练过拟合或者训练性能瓶颈，此时则可以切分数据集，在子数据集上单独训练模型，然后按照某种组合策略构建committee。数据集过小对于一般模型则会欠拟合，此时可以使用bootstrap等抽样方法，在每一份抽样样本中单独训练模型，组成committee。&lt;/p&gt;

&lt;p&gt;一般而言，样本的分类边界十分复杂，如果仅仅使用一般的模型，则很难学习出能够较好拟合样本的分类边界。
但是如果使用集成学习框架，通过学习多个子模型，则能够很好的学习出该分类边界。&lt;/p&gt;

&lt;p&gt;另外，如果我们收集到的数据来源很多，导致数据的类型、维度不同，如果将所有来源的数据都放在同一个向量中，可能会降低模型的学习能力。
但是假如我们在不同来源的数据中根据该数据特性或者其他先验知识单独训练模型，则效果一般都会好很多。
今天看了一篇关于新浪微博垃圾用户检测的论文，研究现状里就提到了采用这种思想的方法，通过观测，发现垃圾用户一般有三种行为：广告、重复转发和恶意关注，然后根据这三种类型去抽取特征，得到三种类型的特征向量之后，然后分别在三种特征上进行子模型训练，最后效果也还不错。&lt;/p&gt;

&lt;p&gt;此外，集成学习框架还可以用来对结果进行评估，可以根据committee中的子模型的vote去计算。&lt;/p&gt;

&lt;p&gt;有人总结了使用ensemble learning的三大原因：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;统计学，已知样本无法完成的表达原始数据的生成分布，通过bootstrap等方法，可以尽量的拟合原始分布训练模型。&lt;/li&gt;
  &lt;li&gt;可计算，体现于模型选择，通过模型融合的解决方案将缺乏模型先验知识的人从单模型调优中解放出来。&lt;/li&gt;
  &lt;li&gt;任意拟合，单模型无法拟合出复杂分类边界，则模型融合则很好的解决了这一问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;参考文献 &lt;a href=&quot;http://www.scholarpedia.org/article/Ensemble_learning&quot;&gt;http://www.scholarpedia.org/article/Ensemble_learning&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 03 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/03/EnsembleLearningMethods.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/03/EnsembleLearningMethods.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>Probability Latent Semantic Analysis</title>
        <description>&lt;p&gt;pLSA是一种主题模型(Topic Model)，全称概率潜在语义分析，是一种将文本的高维稀疏向量表示成低位维稠密向量的映射方法。
它是一种无监督学习方法，假设整个文档集合是由若干个主题组成，某一篇文章都以一定的概率&lt;script type=&quot;math/tex&quot;&gt;p(z \mid d)&lt;/script&gt;属于某一主题;在给定主题的情况下，以&lt;script type=&quot;math/tex&quot;&gt;p(w \mid z)&lt;/script&gt;的概率产生文档的词。
pLSA是基于这样一个假设，假设文档和主题的分布以及主题和词汇的分布都是多项式分布，因此一个词汇的生成过程可以表示为：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;以一定的概率生成一篇文档&lt;/li&gt;
  &lt;li&gt;在该文档中按照文档主题分布选择一个主题&lt;/li&gt;
  &lt;li&gt;在该主题下面，按照主题-词汇分布生成一个词汇&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可以得到如下模型&lt;script type=&quot;math/tex&quot;&gt;p(w,d)=p(d)p(w \mid d); p(w \mid d)=\sum_{z=1}p(z \mid d)p(w \mid z)&lt;/script&gt;，其中z为主题，w为词，d为文档。
其实这个过程也很容易理解，假设现在我们要写篇paper，在动笔之前，会列好提纲，大概选择几个点(也就是主题)。
在完善这些主题的过程中，我们会在积累的词典中去选择和这个主题相适应的词进行修饰。
比如写到关于pLSA的文章的时候，我们会选择”建模、概率、主题模型、极大似然“等词去完善这个主题。
在得到模型之后，我们就需要进行Model Inference，也就是参数估计了，pLSA作为概率图模型，一般是采用MLE作为inference的tool。
首先构造极大似然函数：&lt;script type=&quot;math/tex&quot;&gt;L(\theta) = \prod_{i=1}^N\prod_{j=1}^Np(d_i,w_j)^{n(d_i,w_j)}&lt;/script&gt;，其中&lt;script type=&quot;math/tex&quot;&gt;n(d_i,w_j)&lt;/script&gt;表示的是词&lt;script type=&quot;math/tex&quot;&gt;w_j&lt;/script&gt;在和文档&lt;script type=&quot;math/tex&quot;&gt;d_i&lt;/script&gt;的共现频率，由于我们&lt;script type=&quot;math/tex&quot;&gt;p(w \mid z)&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;p(z \mid d)&lt;/script&gt;都是多项式分布，&lt;br /&gt;
接着，是参数估计，一般而言，此时可以选用极大似然估计。极大对数似然函数如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\theta) = \prod_{i=1}^N\prod_{j=1}^Np(d_i,w_j)^{n(d_i,w_j)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;ln(L(\theta)) = \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)logp(d_i,w_j) \\
=  \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)log(p(d_i)\sum_{k=1}^Kp(z_k \mid d_i)p(w_j \mid z_k)) \\
= \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)log(p(d_i))+\sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)log(\sum_{k=1}^Kp(z_k\mid d_i)p(w_j \mid z_k))&lt;/script&gt;

&lt;p&gt;如果直接求导的话，很明显就得需要求解(n+m)个方程，几乎是不可能完成的任务。
一般采用最大期望算法(Maximum Expection， ME)，EM算法的基本思想很直观，即随机初始化相关参数；随后，在E步，计算潜在变量的后验概率；M步，
利用潜在变量的后验概率去更新未知参数。循环迭代直至达到最大迭代次数算法退出。&lt;/p&gt;

&lt;h3 id=&quot;问题为什么可以收敛&quot;&gt;问题：为什么可以收敛？&lt;/h3&gt;

&lt;p&gt;在e步&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(z_k \mid w_j, d_i) = \frac{p(z_k \mid w_j)p(w_j \mid z_k)} {\sum_{k=1}^Kp(z_k \mid w_j)p(w_j\mid z_k)}&lt;/script&gt;

&lt;p&gt;M步，最大化极大似然&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\theta) = \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)log(\sum_{k=1}^Kp(z_k\mid d_i)p(w_j \mid z_k))&lt;/script&gt;

&lt;p&gt;又由于此时：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{k=1}^K p(z_k|d_i)p(w_j|z_k) &gt; \sum_{k=1}^K p(z_k \mid d_i) p(z_k \mid d_i )p(w_j\mid z_k); \sum_{k=1}^Kp(z_k \mid d_i,w_j) = 1&lt;/script&gt;

&lt;p&gt;所以：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\theta) &gt; \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)log( \sum_{k=1}^K p(z_k \mid d_i) p(z_k \mid d_i )p(w_j\mid z_k) )&lt;/script&gt;

&lt;p&gt;利用Jensen不等式，对于凸函数&lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;有：&lt;script type=&quot;math/tex&quot;&gt;f(\sum_{i=1}^Nw_ix_i)&gt;=\sum_{i=1}^Nw_if(x_i)&lt;/script&gt;可以得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\theta) &gt; \sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)\sum_{k=1}^K p(z_k \mid d_i) log(p(z_k \mid d_i )p(w_j\mid z_k) ) = L'(\theta)&lt;/script&gt;

&lt;p&gt;此时即转化为，对于约束条件：&lt;script type=&quot;math/tex&quot;&gt;\sum_{k=1}^Kp(z_k|d_i)=1[1],\sum_{j=1}^Mp(w_j|z_k)=1[2]&lt;/script&gt; ，求&lt;script type=&quot;math/tex&quot;&gt;L'(\theta)&lt;/script&gt;的最大值。
对于这种带等式约束的优化问题，很显然可以构造拉格朗日乘法进行求解：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H = L'(\theta) + \sum_{i=1}^N\alpha_i(\sum_{k=1}^Kp(z_k|d_i)-1)+\sum_{k=1}^K\beta_k(\sum_{j=1}^Mp(w_j|z_k)-1)&lt;/script&gt;

&lt;p&gt;分别对 &lt;script type=&quot;math/tex&quot;&gt;p(z_k\mid d_i),p(w_j\mid z_k)&lt;/script&gt;求导可得：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=\sum_{j=1}^Mn(d_i,w_j)p(z_k|d_i,w_j)-\alpha_ip(z_k|d_i) = 0      [3] \\
	= \sum_{i=1}^Nn(d_i,w_j)p(z_k|d_i,w_j)-\beta_kp(w_j|z_k) = 0     [4]&lt;/script&gt;

&lt;p&gt;联立方程组[1],[2],[3],[4]求解可得：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(z_k|d_i) = \frac {\sum_{i=1}^N n(d_i,w_j)p(z_k|d_i,w_j)}{\sum_{i=1}^N\sum_{j=1}^Mn(d_i,w_j)p(z_k|d_i,w_j)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(w_j|z_k) = \frac {\sum_{j=1}^Mn(d_i,w_j)p(z_k|d_i,w_j)}{\sum_{j=1}^M\sum_{k=1}^Kn(d_i,w_j)p(z_k|d_i,w_j)} \\
 = \frac{\sum_{j=1}^Mn(d_i,w_j)p(z_k|d_i,w_j)}{n(d_i)}&lt;/script&gt;

&lt;p&gt;因此，在M步更求新参数，通过不断迭代，如果求得的极大似然趋近收敛，则训练结束。&lt;/p&gt;

&lt;p&gt;总结一下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;E步：求当前估计的参数条件下的后验概率：p(z \mid w,d)&lt;/li&gt;
  &lt;li&gt;M步：最大化complete data(加上隐藏变量主题)极大似然估计的期望&lt;/li&gt;
  &lt;li&gt;E步和M步循环迭代，直至收敛。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;实现的源码见：plsa.cpp plsa.h ，由于切词用到了公司的lib库，所以切词的初始化和切词的过程在源码中被删去，可以去中科院nlp主页获取相关模块。&lt;/p&gt;

</description>
        <pubDate>Sat, 02 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/naturallanguageprocessing/2016/04/02/pLSA.html</link>
        <guid isPermaLink="true">http://kymo.github.io/naturallanguageprocessing/2016/04/02/pLSA.html</guid>
        
        
        <category>NaturalLanguageProcessing</category>
        
      </item>
    
      <item>
        <title>Support Vector Machine</title>
        <description>&lt;p&gt;Support Vector Machine ，支持向量机，通常用来进行classification，但是也有做regression。SVM在面对非线性问题上具有独特的优势。本文从linear和nonlinear两种情况下对SVM的建模过程、优化目标的求解推导过程以及优化算法SMO进行阐述。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a^2 + b^2 = c^2&lt;/script&gt;

&lt;h3 id=&quot;1-linear-svm&quot;&gt;1. linear SVM&lt;/h3&gt;

&lt;p&gt;在分类任务中，样本label为&lt;script type=&quot;math/tex&quot;&gt;{-1,1}&lt;/script&gt;，关于从sign distance转换到geometry distance的过程其实很容易理解，
sign distance可以衡量某个样本被分类的置信，如果sign distance越大，那么该样本被分为该类别的可信度就更大；
而geometry distance可以理解为样本距离超平面&lt;script type=&quot;math/tex&quot;&gt;Y = w^TX + b&lt;/script&gt;的距离，是sign distance归一化的结果，
求解目标为&lt;script type=&quot;math/tex&quot;&gt;argmax(\frac { \mid w^Tx + b \mid } { \mid  \mid w \mid  \mid })&lt;/script&gt;，并且需要满足约束：&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i + b) \geq 1&lt;/script&gt;，为了求解方便，可以不加证明的令&lt;script type=&quot;math/tex&quot;&gt;\mid w^Tx+b \mid =1&lt;/script&gt;，形式化如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}argmax \frac{1}{ \mid  \mid w \mid  \mid } \\ s.t ~~ y_i(w^Tx_i+b) \geq 1 ~~i=1,2,\cdot \cdot ,n \end{cases}&lt;/script&gt;

&lt;p&gt;而此时最大化 &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{ \mid  \mid w \mid  \mid }&lt;/script&gt;等价于最小化 &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; ，并且最小化和最小化&lt;script type=&quot;math/tex&quot;&gt;\frac {1}{2} w^Tw&lt;/script&gt;等价，&lt;/p&gt;

&lt;p&gt;所以1.1可以变为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}argmmin \frac {1}{2} w^Tw \\ s.t ~~ y_i(w^Tx_i+b) \geq 1 ~~i=1,2,\cdot \cdot ,n \end{cases}&lt;/script&gt;

&lt;p&gt;由此可以得到不等式约束问题，原始问题通过分析不难发现，求解十分困难，不过对于PSO(粒子群算法)而言，往往可以求得比较好的解。不过在碰到这种带不等式约束的问题的时候，我们可以通过拉格朗日对偶性质将原始问题转化为对偶问题，在最大熵模型中也有类似的处理过程。首先构造拉格朗日函数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(w,b,\alpha) = \frac {1} {2} w^Tw - \sum_{i=1}^N a_i[y_i(w^Tx_i + b) - 1]&lt;/script&gt;

&lt;p&gt;首先我们可以得到这个函数的等价形式，令&lt;script type=&quot;math/tex&quot;&gt;\theta = argmax_\alpha L(w,b,\alpha)&lt;/script&gt;，那么：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\theta=\begin{cases}\frac {1} {2} w^Tw~~~y_i(w^Tx_i+b) \geq 1\\ \infty~~~~ y_i(w^Tx_i+b) &lt; 1~~\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;可见，&lt;script type=&quot;math/tex&quot;&gt;min\theta&lt;/script&gt;和原始优化目标&lt;script type=&quot;math/tex&quot;&gt;argmin \frac {1}{2}w^Tw&lt;/script&gt;等价。令&lt;script type=&quot;math/tex&quot;&gt;p=min\theta&lt;/script&gt;，其对偶形式&lt;script type=&quot;math/tex&quot;&gt;d=max_{\alpha}min_{w,b}L(w,\alpha)&lt;/script&gt;，那么必然会有&lt;script type=&quot;math/tex&quot;&gt;p \geq d&lt;/script&gt;。此时令&lt;script type=&quot;math/tex&quot;&gt;g=argmin_{w} L(w,b,\alpha)&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;g \leq L(w^*,b,\alpha) \leq \frac {1}{2} {w^*}^Tw^* = p&lt;/script&gt;，所以&lt;script type=&quot;math/tex&quot;&gt;p \geq d&lt;/script&gt;。所以此时该算法满足弱对偶，我们可以通过求解该弱对偶问题去近似求解原始问题，在EM中就是不断优化极大似然下界~ 并且可以知道的是，不管原始问题是何种优化，对偶问题都会是凸优化，也即都会存在极值。不过在SVM中，我们是可以把弱对偶加强，变成strong duality，也即&lt;script type=&quot;math/tex&quot;&gt;p = d&lt;/script&gt;，优化对偶问题等价于对原始问题的求解。那么怎么判断该对偶问题是强对偶问题呢？KKT条件。如下：&lt;/p&gt;

&lt;p&gt;\begin{cases}  \bigtriangledown L(w,b,\alpha) = 0 \ \alpha_i(y_i(w^Tx_i+b) - 1) = 0\ \alpha_i \geq 0 \end{cases}&lt;/p&gt;

&lt;p&gt;很明显，此时KKT条件成立，所以满足强对偶。其中&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i+b)-1) = 0&lt;/script&gt;但此时KKT只是必要条件，不过由于我们的原始问题是凸优化，所以KKT便是充要条件了。&lt;/p&gt;

&lt;p&gt;对对偶问题，首先是&lt;script type=&quot;math/tex&quot;&gt;min_{w,b}L(w,b,\alpha)&lt;/script&gt;，分别对&lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;求导，得：&lt;/p&gt;

&lt;p&gt;[\frac {\partial L(w,\alpha,b)} {\partial(w)} = w - \sum_{i=1}^N \alpha_iy_ix_i=0]&lt;/p&gt;

&lt;p&gt;[\frac {\partial L(w,\alpha,b)} {\partial(b)} = \sum_{i=1}^N\alpha_iy_i=0]&lt;/p&gt;

&lt;p&gt;随后，将&lt;script type=&quot;math/tex&quot;&gt;w=\sum_{i=1}^N\alpha_iy_ix_i&lt;/script&gt;代入&lt;script type=&quot;math/tex&quot;&gt;L(w,b,\alpha)&lt;/script&gt;中，则有：&lt;/p&gt;

&lt;p&gt;\begin{eqnarray&lt;em&gt;}
L(w,\alpha,b)&amp;amp;=&amp;amp;\frac {1}{2} w^Tw - \sum_{i=1}^N\alpha_i[y_i(w^Tx_i+b)-1]\&amp;amp;=&amp;amp;\frac{1}{2}\sum_{i=1}^N\alpha_iy_ix_i^T\sum_{j=1}^N\alpha_jy_jx_j - \sum_{i=1}^N\alpha_i[y_i(w^Tx_i+b)-1]\ &amp;amp;=&amp;amp;\frac {1}{2} \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j - \sum_{i=1}^N\alpha_i\sum_{j=1}^N\alpha_jy_jx_j^Tx_i + \sum_{i=1}^N\alpha_i\&amp;amp;=&amp;amp;-\frac {1}{2} \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j + \sum_{i=1}^N\alpha_i\end{eqnarray&lt;/em&gt;}&lt;/p&gt;

&lt;p&gt;所以：&lt;/p&gt;

&lt;p&gt;[\begin{cases}-\frac {1}{2} \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j + \sum_{i=1}^N\alpha_i\s.t~~~\sum_{i=1}^N\alpha_iy_i=0 \end{cases}]&lt;/p&gt;

&lt;p&gt;到这，我们还没有考虑soft margin。实际情况中，总是会存在一定的噪声数据，使得我们的分类超平面被这些噪声数据所误导，从而使得模型的variance增大，所以一般来讲都会采用soft margin构建优化函数，我们以&lt;script type=&quot;math/tex&quot;&gt;\varepsilon&lt;/script&gt;的范围容许一定的误差，即原来的&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i+b) \geq 1&lt;/script&gt;此时为&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i + b) \geq 1 - \varepsilon_i&lt;/script&gt;，所以我们的优化目标变为：&lt;/p&gt;

&lt;p&gt;[\begin{cases}argmin \frac {1}{2} w^Tw + C\sum_{i=1}^N\varepsilon_i \s.t ~~ y_i(w^Tx_i+b) \geq 1 - \varepsilon_i ~~i=1,2,\cdot \cdot ,n \ \sum_{i=1}^N\varepsilon_i \geq C \end{cases}]&lt;/p&gt;

&lt;p&gt;关于上式，可以看成是利用hinge loss加l2范数正则项的结果，SVM此时的损失函数可以表示为 &lt;script type=&quot;math/tex&quot;&gt;min_{w,b} \sum_{i=1}^N [1-y_i(w^Tx_i+b)]_{+} + \lambda\ \mid w\ \mid ^2&lt;/script&gt;，其中如果&lt;script type=&quot;math/tex&quot;&gt;z&gt;0&lt;/script&gt;，那么&lt;script type=&quot;math/tex&quot;&gt;z_{+}&lt;/script&gt;=z，否则等于0。如果令&lt;script type=&quot;math/tex&quot;&gt;\varepsilon_i=1-y_i(w^Tx_i+b),\varepsilon_i \geq 0&lt;/script&gt;，那么此时最优化问题为 &lt;script type=&quot;math/tex&quot;&gt;min_{w,b} \sum_{i=1}^N\varepsilon_i + \lambda\ \mid w\ \mid ^2&lt;/script&gt;，如果取&lt;script type=&quot;math/tex&quot;&gt;\lambda=\frac {1}{2C}&lt;/script&gt;，那么就和上述优化目标等价，所以可以看出，软间隔实际上是在ERM的基础上加了SRM~&lt;/p&gt;

&lt;p&gt;之后的推导没有多大差别，只不过在求导的过程中出现了&lt;script type=&quot;math/tex&quot;&gt;\alpha_i = C - \varepsilon_i&lt;/script&gt;，又有前面在对偶转换使用的KKT条件之一&lt;script type=&quot;math/tex&quot;&gt;\alpha_i(y_i(w^Tx_i+b) - 1) =0&lt;/script&gt;可得，在分类超平面上的点，也即满足&lt;script type=&quot;math/tex&quot;&gt;y_i(w^Tx_i+b)-1 + \varepsilon_i = 0&lt;/script&gt;，而那些不在超平面的点，必然有&lt;script type=&quot;math/tex&quot;&gt;\alpha_i=0&lt;/script&gt;，而在超平面上的，则有&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt; \alpha_i &lt; C %]]&gt;&lt;/script&gt;，在超平面之外的则是&lt;script type=&quot;math/tex&quot;&gt;\alpha_i=C&lt;/script&gt;。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;nonlinear SVM&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;非线性情况之下，利用线性曲线去拟合，明显会产生underfitting，但是我们可以通过函数映射的方式，将原来空间中的非线性特征，映射到高维空间中，使得样本可分或者近似可分，实际上是，机器学习中有一种叫做基展开的技术，就是处理这种线性到非线性的特征映射。不过对于SVM中使用这种非线性变化是因为它能够和核函数配合的天衣无缝。&lt;/p&gt;

&lt;p&gt;这里用一个简单的例子作简要说明。对于&lt;script type=&quot;math/tex&quot;&gt;x=(x_1,x_2)&lt;/script&gt;二维空间的某个点，我们将其映射到三维空间。所利用的映射函数可以为&lt;script type=&quot;math/tex&quot;&gt;\phi (x_1,x_2) = (x_1^2,x_2^2,2x_1x_2)&lt;/script&gt;，那么在三维空间中，样本线性可分的可能性更大，但是计算开销却上升了，因为在转化成对偶问题之后就产生了向量内积运算。对于原始二维空间中的两点&lt;script type=&quot;math/tex&quot;&gt;p=(\eta_1,\eta_2),q=(\gamma_1,\gamma_2)&lt;/script&gt;，在三维空间中的向量内积为&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;\phi(\eta_1,\eta_2),\phi(\gamma_1,\gamma_2)&gt; = \eta_1^2\gamma_1^2 + \eta_2^2\gamma_2^2 + 4\eta_1\eta_2\gamma_1\gamma_2 %]]&gt;&lt;/script&gt;，这和&lt;script type=&quot;math/tex&quot;&gt;\eta_1^2\gamma_1^2 + \eta_2^2\gamma_2^2 + 2\eta_1\eta_2\gamma_1\gamma_2&lt;/script&gt;十分相似，而后者却等于&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;(\eta_1,\eta_2), (\gamma_1, \gamma_2)&gt;^2 %]]&gt;&lt;/script&gt;，所以只需要令&lt;script type=&quot;math/tex&quot;&gt;\phi(x_1,x_2) = (x_1^2, x_2^2, \sqrt {2} x_1x_2)&lt;/script&gt;，就可以得到&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;\phi(\eta_1,\eta_2),\phi(\gamma_1,\gamma_2)&gt; %]]&gt;&lt;/script&gt;=&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;(\eta_1,\eta_2), (\gamma_1, \gamma_2)&gt;^2 %]]&gt;&lt;/script&gt;~ 由此可以推广到高维。不难看出在高维空间中的内积可以通过在原始空间内积的平方得到~此时&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
K(p,q) = &lt;\phi(\eta_1,\eta_2), \phi(\gamma_1, \gamma_2)&gt; %]]&gt;&lt;/script&gt;=&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;p,q&gt;^2 %]]&gt;&lt;/script&gt;。对偶转换之后只需要将&lt;script type=&quot;math/tex&quot;&gt;x_i,x_j&lt;/script&gt;的内积运算更换成&lt;script type=&quot;math/tex&quot;&gt;K(x_i,x_j)&lt;/script&gt;，即可处理非线性数据~&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;SMO&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;SMO本质上上一种坐标上升优化算法，坐标上升可以理解为在&lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;维向量构成的空间中，每次选择一个维度进行优化，最终能够求得比较合适的解。SMO每次选择两个参数，因为此时待求变量有&lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^N\alpha_iy_i =0&lt;/script&gt;的约束。SMO的优化过程如下：&lt;/p&gt;

&lt;p&gt;选择&lt;script type=&quot;math/tex&quot;&gt;\alpha_i, \alpha_j&lt;/script&gt;
固定其他参数，然后对&lt;script type=&quot;math/tex&quot;&gt;\alpha_i, \alpha_j&lt;/script&gt;进行优化
利用&lt;script type=&quot;math/tex&quot;&gt;\alpha_i, \alpha_j&lt;/script&gt;，对截距进行优化
在了解确切的&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;贪心选择策略之前，先假定我们已经将&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;选择妥当，然后直接对&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;进行优化。根据条件&lt;script type=&quot;math/tex&quot;&gt;\sum_{k=1}^N\alpha_ky_k=0&lt;/script&gt;，令&lt;script type=&quot;math/tex&quot;&gt;A = y_i\sum_{k!=i,j}^N\alpha_ky_k&lt;/script&gt;. 那么&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;的关系为&lt;script type=&quot;math/tex&quot;&gt;\alpha_i = A - y_iy_j\alpha_j&lt;/script&gt;。此时我们的优化目标[max_{\alpha} L(\alpha) = \sum_{k=1}^N\alpha_k - \frac{1}{2} \sum_{l=1}^N\sum_{k=1}^N\alpha_l\alpha_ky_ly_kK_{l,k}]&lt;/p&gt;

&lt;p&gt;其中，&lt;script type=&quot;math/tex&quot;&gt;K_{l,k} = K(x_l, x_j)&lt;/script&gt;，令&lt;script type=&quot;math/tex&quot;&gt;B = \sum_{k!=i,j}\alpha_k, S = y_iy_j, V_i = \sum_{k!=i,j}\alpha_ky_kK_{i,k}, V_j =\sum_{k!=i,j}\alpha_ky_kK_{j,k}&lt;/script&gt;，那么有：&lt;/p&gt;

&lt;p&gt;\begin{eqnarray&lt;em&gt;} L(\alpha) &amp;amp;=&amp;amp; \sum_{k!=i,j}^N\alpha_k + \alpha_i + \alpha_j - \frac {1}{2}[\alpha_i\alpha_i y_iy_iK_{i,i} + \alpha_j\alpha_jy_jy_jK_{j,j} + 2\alpha_i\alpha_jy_iy_jK_{i,j} + 2\sum_{k!=i,j}^N\alpha_i\alpha_ky_iy_kK_{i,k} + 2\sum_{k!=i,j}^N\alpha_j\alpha_ky_jy_kK_{j,k} + \sum_{l!=i,j}^N \sum_{k!=i,j}^N\alpha_l\alpha_ky_ly_kK_{l,k}] \                                                               &amp;amp;=&amp;amp; B + A - S\alpha_j + \alpha_j - \frac{1}{2} [2\alpha_i\alpha_jSK_{i,j} + \alpha_i^2K_{i,i} + \alpha_j^2K_{j,j} + 2\alpha_iy_iV_i + 2\alpha_jy_jV_j +  \sum_{l!=i,j}^N \sum_{k!=i,j}^N\alpha_l\alpha_ky_ly_kK_{l,k}]    \                                                          &amp;amp;=&amp;amp;-S\alpha_j + \alpha_j  - \frac{1}{2}K_{i,i}(A-S\alpha_j)^2 - \frac{1}{2} K_{j,j}\alpha_j^2 - K_{i,j}\alpha_j(A-S\alpha_j)S - \alpha_jy_jV_j - (A-S\alpha_j)y_iV_i + \varepsilon_{constant} \end{eqnarray&lt;/em&gt;}&lt;/p&gt;

&lt;p&gt;其中，&lt;script type=&quot;math/tex&quot;&gt;\varepsilon_{constant}&lt;/script&gt;为一些常量，在求极值点是可以忽略，上式对&lt;script type=&quot;math/tex&quot;&gt;\alpha_j&lt;/script&gt;求导，有：&lt;/p&gt;

&lt;p&gt;\begin{eqnarray&lt;em&gt;} \frac {\partial_{L(\alpha_j)}} {\partial_{\alpha_j}} &amp;amp;=&amp;amp; -S + 1 + ASK_{i,i} -K_{i,i}\alpha_j - K_{j,j}\alpha_j -ASK_{i,j} + 2K_{i,j}\alpha_j -y_jV_j + Sy_iV_i=0 \ \alpha_j&amp;amp;=&amp;amp; \frac {-S + 1 + AS(K_{i,i}-K_{i,j}) + y_j(V_i-V_j)}{K_{i,i} + K_{j,j} - 2K_{i,j}} \end{eqnarray&lt;/em&gt;}&lt;/p&gt;

&lt;p&gt;又有优化&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;的时候，其他参数没有被改变。&lt;/p&gt;

&lt;p&gt;\begin{eqnarray&lt;em&gt;}\alpha_iy_i+\alpha_jy_j&amp;amp;=&amp;amp; -\sum_{k!=i,j}\alpha_k^{old}y_k=\alpha_i^{old}y_i + \alpha_j^{old}y_j \ V_i &amp;amp;=&amp;amp; \sum_{k!=i,j}\alpha_ky_kK_{i,k}=\sum_{k!=i,j}\alpha_k^{old}y_kK_{i,k} = \sum_{k=1}^N\alpha_k^{old}y_kK_{i,k} + b - b - \alpha_i^{old}y_iK_{i,i} - \alpha_j^{old}y_jK_{i,j} \ V_j &amp;amp; = &amp;amp;\sum_{k!=i,j}\alpha_ky_kK_{j,k}=\sum_{k!=i,j}\alpha_k^{old}y_kK_{j,k} = \sum_{k=1}^N\alpha_k^{old}y_kK_{j,k} + b - b - \alpha_j^{old}y_jK_{j,j} - \alpha_i^{old}y_iK_{i,j}  \end{eqnarray&lt;/em&gt;}&lt;/p&gt;

&lt;p&gt;且&lt;script type=&quot;math/tex&quot;&gt;g(x_i) = \sum_{k=1}^N\alpha_k^{old}y_kK_{i,k} + b&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;g(x_j) = \sum_{k=1}^N\alpha_k^{old}y_kK_{j,k} + b&lt;/script&gt;，所以：&lt;/p&gt;

&lt;p&gt;[V_i - V_j = g(x_i) - g(x_j) - \alpha_i^{old}y_iK_{i,i} + \alpha_j^{old}y_jK_{j,j} - \alpha_j^{old}y_jK_{i,j} + \alpha_i^{old}y_iK_{i,j} ]&lt;/p&gt;

&lt;p&gt;然后，将A = &lt;script type=&quot;math/tex&quot;&gt;\alpha_i^{old}+S\alpha_j^{old}&lt;/script&gt;，S=&lt;script type=&quot;math/tex&quot;&gt;y_iy_j&lt;/script&gt;代入&lt;script type=&quot;math/tex&quot;&gt;\alpha_j&lt;/script&gt;表达式，可得：&lt;/p&gt;

&lt;p&gt;\begin{eqnarray&lt;em&gt;}\alpha_j &amp;amp;=&amp;amp; \frac {y_jy_j - y_iy_j +(\alpha_i^{old}+y_iy_j\alpha_j^{old})y_iy_j(K_{i,i}-K_{i,j}) + y_j(g(x_i) - g(x_j) - \alpha_i^{old}y_iK_{i,i} + \alpha_j^{old}y_jK_{j,j} - \alpha_j^{old}y_jK_{i,j} + \alpha_i^{old}y_iK_{i,j})} {K_{i,i} + K_{j,j} - 2K_{i,j}} \ &amp;amp;=&amp;amp; \frac {y_j[y_j-y_i + y_j\alpha_j^{old}(K_{i,i} + K_{j,j} - 2K_{i,j} + g(x_i) - g(x_j) ]} {K_{i,i} + K_{j,j} - 2K_{i,j}}\ &amp;amp;=&amp;amp; \alpha_j^{old} + \frac{y_j[y_j-y_i + g(x_i)-g(x_j)]} {K_{i,i} + K_{j,j} - 2K_{i,j}}\end{eqnarray&lt;/em&gt;}&lt;/p&gt;

&lt;p&gt;然后令&lt;script type=&quot;math/tex&quot;&gt;E_i = g(x_i) - y_i&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;\eta = K_{i,i} + K_{j,j} - 2K{i,j}&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;\alpha_j^{new,unc} = \alpha_j^{old} + \frac {y_j(E_i - E_j)} {\eta}&lt;/script&gt;，此时求出的&lt;script type=&quot;math/tex&quot;&gt;\alpha_j&lt;/script&gt;还需要经过边界判定，对&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;有&lt;script type=&quot;math/tex&quot;&gt;\alpha_iy_i + \alpha_jy_j = \alpha_i^{old} + \alpha_j^{old}&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt;=\alpha_i&lt;=C %]]&gt;&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt;=\alpha_j&lt;=C %]]&gt;&lt;/script&gt;的条件限制，所以必须对&lt;script type=&quot;math/tex&quot;&gt;\alpha_i, \alpha_j&lt;/script&gt;的上下边界&lt;script type=&quot;math/tex&quot;&gt;L,H&lt;/script&gt;进行确认。&lt;/p&gt;

&lt;p&gt;如果&lt;script type=&quot;math/tex&quot;&gt;y_i = y_j&lt;/script&gt;，那么&lt;script type=&quot;math/tex&quot;&gt;L= max(0, \alpha_j^{old} - \alpha_i^{old})&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;H=min(C,C+\alpha_j^{old} - \alpha_i^{old})&lt;/script&gt;
如果&lt;script type=&quot;math/tex&quot;&gt;y_i!=y_j&lt;/script&gt;，那么&lt;script type=&quot;math/tex&quot;&gt;L=max(0, \alpha_j^{old} + \alpha_i^{old} - C)&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;H=min(C,\alpha_j^{old} + \alpha_i^{old})&lt;/script&gt;
根据上式可以得到&lt;script type=&quot;math/tex&quot;&gt;\alpha_j^{new}&lt;/script&gt;为：&lt;/p&gt;

&lt;p&gt;[\alpha_j^{new} = \begin{cases} H,~&lt;del&gt;\alpha_j^{new,unc} &amp;gt; H \ \alpha_j^{new,unc},&lt;/del&gt;~~~ L&amp;lt;=\alpha_j^{new,unc}&amp;lt;=H \ L,~~~~~\alpha_j^{new,unc} &amp;lt; L \end{cases}]&lt;/p&gt;

&lt;p&gt;由此可以得到&lt;script type=&quot;math/tex&quot;&gt;\alpha_i^{new}= \alpha_i^{old} + y_iy_j(\alpha_j^{old} - \alpha_j^{new})&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;关于&lt;script type=&quot;math/tex&quot;&gt;alpha_i, \alpha_j&lt;/script&gt;的更新策略完成，但是对于上式中的bias也需要进行更新，以保证KKT条件&lt;script type=&quot;math/tex&quot;&gt;\alpha_j(y_j(\sum_{i=1}^N\alpha_iy_iK_{i,j} + b) - 1) = 0&lt;/script&gt;成立。&lt;/p&gt;

&lt;p&gt;当&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt; \alpha_i^{new} &lt; C %]]&gt;&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;y_i(\sum_{k=1}^N\alpha_ky_kK_{i,k}+b) = 1&lt;/script&gt; ，所以&lt;script type=&quot;math/tex&quot;&gt;b_1^{new} = y_i - \sum_{k!=i,j}^N\alpha_ky_kK_{i,k} - \alpha_i^{new}y_iK_{i,i} - \alpha_j^{new}y_jK_{i,j}&lt;/script&gt;，又知&lt;script type=&quot;math/tex&quot;&gt;E_i = g(x_i) - y_i = \sum_{k=1}^N\alpha_ky_kK_{i,k} + b^{old} - y_i&lt;/script&gt;，此时除&lt;script type=&quot;math/tex&quot;&gt;\alpha_i,\alpha_j&lt;/script&gt;以外的都不会发生变化，所以&lt;script type=&quot;math/tex&quot;&gt;E_i = \sum_{k!=i,j}^N\alpha_ky_kK_{k,i} + \alpha_i^{old}y_iKii + \alpha_j^{old}y_jK_{i,j} + b^{old} - y_i&lt;/script&gt;，也即 &lt;script type=&quot;math/tex&quot;&gt;b_i^{new} = -E_i - y_iK_{i,i}(\alpha_i^{new} - \alpha_i^{old}) - y_jK_{i,j}(\alpha_j^{new} - \alpha_j^{old}) + b^{old}&lt;/script&gt;
同1，当&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0&lt; \alpha_j^{new} &lt; C %]]&gt;&lt;/script&gt;，则&lt;script type=&quot;math/tex&quot;&gt;b_2^{new} = -E_j - y_iK_{i,j}(\alpha_i^{new} - \alpha_i^{old}) - y_jK_{j,j}(\alpha_j^{new} - \alpha_j^{old}) + b^{old}&lt;/script&gt;
如果&lt;script type=&quot;math/tex&quot;&gt;\alpha_i^{new}&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;\alpha_j^{new}&lt;/script&gt;都满足条件，则&lt;script type=&quot;math/tex&quot;&gt;b_1^{new}&lt;/script&gt;=&lt;script type=&quot;math/tex&quot;&gt;b_2^{new}&lt;/script&gt;
如果&lt;script type=&quot;math/tex&quot;&gt;\alpha_i^{new}&lt;/script&gt;、&lt;script type=&quot;math/tex&quot;&gt;\alpha_j^{new}&lt;/script&gt;为0或者C，那么取&lt;script type=&quot;math/tex&quot;&gt;b_1^{new}&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;b_2^{new}&lt;/script&gt;的中值即可
至此，SMO算法结束，不过实际中，需要实时更新&lt;script type=&quot;math/tex&quot;&gt;E_i&lt;/script&gt;，所以在更新完bias之后，再利用已有的信息重新更新&lt;script type=&quot;math/tex&quot;&gt;E_i&lt;/script&gt;即可。&lt;/p&gt;

&lt;p&gt;最终实现见：SVM@GITHUB&lt;/p&gt;

&lt;p&gt;参考资料&lt;/p&gt;

&lt;p&gt;李航 《统计学习方法》&lt;/p&gt;

&lt;p&gt;http://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html&lt;/p&gt;

</description>
        <pubDate>Sat, 02 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/02/SVM.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/02/SVM.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>SVD</title>
        <description>&lt;p&gt;奇异值分解作为一种矩阵分解的算法，广泛的应用于数据分析、信号处理、模式识别、图片压缩、天气预测以及潜在概率语义分析等应用。其实质是通过分解矩阵，找到矩阵行列之间的某种潜在关系。如在推荐系统中利用SVD分解用户-商品的评分矩阵，便可以得到用户和商品之间的关联。&lt;/p&gt;

&lt;h3 id=&quot;特征值分解&quot;&gt;特征值分解&lt;/h3&gt;
&lt;p&gt;一般矩阵乘以某个向量，代表着对该向量进行某种线性变换：拉伸、平移以及旋转，在计算机图形学中经常对三维场景的物体在各种不同的坐标系下进行变换也利用了该种性质，直接利用物体的三维坐标乘以变换矩阵即可实现。如果对向量的线性变换仅仅只是对向量进行了缩放，那么这种情况便是特征值分解，既有&lt;script type=&quot;math/tex&quot;&gt;Ax=\lambda x&lt;/script&gt;. 特征值分解的物理意义在不同的应用场景下有不同的解释。不过一般都是对对称矩阵&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;进行分解，得到一组正交的特征向量作为正交基，使得矩阵&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;能够被投射到以该正交基为基底的空间中，特征值则表示在不同的正交基下的投射距离，特征值越大说明矩阵在对应的特征向量上的方差越大，功率越大，信息熵也越大，在PCA(principal  component analysis)中，即可以利用特征值分解，获取最重要的K个成分对数据进行降维，保证在数据规模可控的情况下获得足够的信息量。&lt;/p&gt;

&lt;h3 id=&quot;奇异值分解&quot;&gt;奇异值分解&lt;/h3&gt;
&lt;p&gt;奇异值分解可以认为是特征值分解的扩充，即解决对于矩阵&lt;script type=&quot;math/tex&quot;&gt;A(m \times n)&lt;/script&gt;能否在原始空间&lt;script type=&quot;math/tex&quot;&gt;R^n&lt;/script&gt;中找到一组正交基&lt;script type=&quot;math/tex&quot;&gt;(v_1, v_2, v_3, \cdot \cdot \cdot , v_n)&lt;/script&gt;，使得经过该线性变换矩阵投射得新的空间&lt;script type=&quot;math/tex&quot;&gt;R^m&lt;/script&gt;中的向量仍然正交的问题，即当&lt;script type=&quot;math/tex&quot;&gt;i!=j ，v_i^T \times v_j = 0&lt;/script&gt;时，&lt;script type=&quot;math/tex&quot;&gt;(Av_i,Av_j) = 0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;对于矩阵&lt;script type=&quot;math/tex&quot;&gt;A^TA(n \times n)&lt;/script&gt;，该矩阵为对称矩阵，且有&lt;script type=&quot;math/tex&quot;&gt;r(A^TA)=r&lt;/script&gt;，则可以通过特征值分解得到对应的一组正交基&lt;script type=&quot;math/tex&quot;&gt;(v_1, v_2, v_3, \cdot \cdot \cdot , v_r)&lt;/script&gt;，使得&lt;script type=&quot;math/tex&quot;&gt;A^TAv_i=\lambda v_i&lt;/script&gt;.
对于经过&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;线性变化投射到新的空间的向量&lt;script type=&quot;math/tex&quot;&gt;Av_i&lt;/script&gt;，容易推知当&lt;script type=&quot;math/tex&quot;&gt;i!=j&lt;/script&gt;时，&lt;script type=&quot;math/tex&quot;&gt;(Av_i,Av_j)&lt;/script&gt;=0. 从而可以得到投射之后的新的一组正交基&lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;为&lt;script type=&quot;math/tex&quot;&gt;(Av_1,Av_2,  \cdot  \cdot  \cdot  Av_r)&lt;/script&gt;，对其进行标准化，有&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;u_i=\frac{Av_i}{\mid Av_i\mid} =\frac{1}{\sqrt {\lambda_i}} Av_i =&gt; Av_i = \sqrt {\lambda_i}u_i = \delta_i u_i&lt;/script&gt;

&lt;p&gt;则有&lt;/p&gt;

&lt;p&gt;$A(v_1, v_2, v_3, \cdot \cdot \cdot, v_r) = (\delta_1 u_1 , \delta_2 u_2 , \delta_3 u_3, \cdot \cdot \cdot , \delta_r u_r) = (u_1, u_2, \cdot \cdot \cdot, u_r) \times [
\begin {matrix}
\delta_1, \cdot \cdot \cdot, 0 \\
0, \cdot \cdot \cdot, 0 \\
0, \cdot \cdot \cdot, \delta_r
\end {matrix}
]
$&lt;/p&gt;

&lt;p&gt;即&lt;script type=&quot;math/tex&quot;&gt;AV = U\Sigma =&gt; AVV^T = U\Sigma V^T =&gt; A =  U\Sigma V^T (VV^T=E)&lt;/script&gt; ，其中，$\Sigma$的对角元素为特征值的开平方，表示&lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;向量经过&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;方阵变换之后与&lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;向量之间的缩放对应关系，此时&lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;矩阵大小为&lt;script type=&quot;math/tex&quot;&gt;m \times r&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;矩阵大小为&lt;script type=&quot;math/tex&quot;&gt;n \times r&lt;/script&gt;，也可以将&lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;扩充为&lt;script type=&quot;math/tex&quot;&gt;m \times m&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;扩充为 &lt;script type=&quot;math/tex&quot;&gt;n \times n&lt;/script&gt;，此时&lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt;大小则为&lt;script type=&quot;math/tex&quot;&gt;m \times n&lt;/script&gt;，即在原来的基础之上增加若干个0行和0列.&lt;/p&gt;

&lt;p&gt;从另外一个角度，对于任意矩阵&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;，可以分解为&lt;script type=&quot;math/tex&quot;&gt;A= U\Sigma V^T&lt;/script&gt;的形式，于是可以得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;AA^T=U\Sigma V^T V\Sigma^TU^T = U\Sigma \Sigma^TU^T&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A^TA= V\Sigma^TU^T  U\Sigma V^T = V\Sigma \Sigma^TV^T&lt;/script&gt;

&lt;p&gt;所以，&lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;可以看成是对矩阵&lt;script type=&quot;math/tex&quot;&gt;AA^T&lt;/script&gt;进行特征值分解得到的正交基，&lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;可以看成是对矩阵&lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt;进行分解得到的正交基。&lt;/p&gt;

&lt;h3 id=&quot;svd-在lsa中的应用&quot;&gt;SVD 在LSA中的应用&lt;/h3&gt;

&lt;p&gt;奇异值分解在诸多方面都有十分成功的应用，在数据压缩中，可以通过将原始的矩阵进行奇异值分解，去除奇异值较小的部分得到低阶近似，从而实现对数据的降维；在语义分析中，通过对构造的文档-词汇矩阵&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;进行奇异值分解&lt;script type=&quot;math/tex&quot;&gt;D=U \Sigma V^T&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D = [
\begin{array}{ccc}
x_{1,1}  , \cdot \cdot  \cdot  ,  x_{1,n} \\\
\cdot \cdot  \cdot   ,  \cdot \cdot  \cdot  ,  \cdot \cdot  \cdot \\\
x_{m,1}   ,  \cdot \cdot  \cdot  ,  x_{m,n} 
\end{array}
]
= [t_1 , t_2, \cdot \cdot \cdot , t_n]^T = [d_1 , d_2, \cdot \cdot \cdot , d_m]&lt;/script&gt;

&lt;p&gt;其中，&lt;script type=&quot;math/tex&quot;&gt;t_i^T = (x_{i,1}, x_{i,2}, \cdot \cdot \cdot , x_{i,n}) , d_j^T = (x_{1,j}, x_{2,j}, \cdot \cdot \cdot , x_{m,j})&lt;/script&gt;。&lt;/p&gt;

&lt;p&gt;进行SVD分解之后&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D=U\Sigma V^T = (u_1, u_2,  \cdot \cdot \cdot, u_r) \times [
\begin {matrix}
\delta_1, \cdot \cdot \cdot, 0 \\\
0, \cdot \cdot \cdot, 0 \\\
0, \cdot \cdot \cdot, \delta_r
\end {matrix}
]
\times 
(v_1, v_2, \cdot \cdot \cdot , v_r)^T&lt;/script&gt;

&lt;p&gt;随后进行k-阶近似，取最大的k个奇异值&lt;script type=&quot;math/tex&quot;&gt;\Sigma_k&lt;/script&gt;以及对应的奇异向量&lt;script type=&quot;math/tex&quot;&gt;U_k,V_k&lt;/script&gt;，便可以对&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;进行降维，将词汇-文档空间映射到语义空间中去，此时k值可以认为是语义空间中的主题数，&lt;script type=&quot;math/tex&quot;&gt;U_k&lt;/script&gt;可以看做在该语义空间中，文档上的主题分布，而&lt;script type=&quot;math/tex&quot;&gt;V_k&lt;/script&gt;则可以看成是主题上的词汇分布。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_k = U_k \Sigma_k V_k^T = (u_1, u_2,  \cdot \cdot \cdot, u_k) \times [
\begin {matrix}
\delta_1, \cdot \cdot \cdot, 0 \\\
0, \cdot \cdot \cdot, 0 \\\
0, \cdot \cdot \cdot, \delta_k
\end {matrix}
]
\times 
(v_1, v_2, \cdot \cdot \cdot , v_k)^T&lt;/script&gt;

&lt;p&gt;此时，&lt;script type=&quot;math/tex&quot;&gt;d_j^T = (v_{1,j}，v_{2,j}, \cdot \cdot \cdot ,v_{k,j})&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;t_i^T =  (u_{1,j}，u_{2,j}, \cdot \cdot \cdot ,u_{k,j})&lt;/script&gt;
此时D仍然是&lt;script type=&quot;math/tex&quot;&gt;m \times n&lt;/script&gt;，得到对词汇-文档原始矩阵的奇异值分解结果之后，变可以完成包括词汇相似度计算、文档相似度计算等任务。&lt;/p&gt;

&lt;h4 id=&quot;1-词汇相似度计算计算语料中的两个词汇pq的语义相似度&quot;&gt;1. 词汇相似度计算，计算语料中的两个词汇&lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt;的语义相似度&lt;/h4&gt;
&lt;p&gt;分别计算&lt;script type=&quot;math/tex&quot;&gt;\Sigma_k \times t_i&lt;/script&gt;以及&lt;script type=&quot;math/tex&quot;&gt;\Sigma_k \times t_j&lt;/script&gt;(i和j为词汇p,q对应的ID)，可以得到两个列向量，然后通过某种距离计算方式可以得到两者的语义距离.&lt;/p&gt;

&lt;h4 id=&quot;2-文档相似度计算计算语料中的两篇文档p-q的语义相似度&quot;&gt;2. 文档相似度计算，计算语料中的两篇文档&lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt;的语义相似度&lt;/h4&gt;
&lt;p&gt;分别计算&lt;script type=&quot;math/tex&quot;&gt;\Sigma_k \times d_i&lt;/script&gt;以及&lt;script type=&quot;math/tex&quot;&gt;\Sigma_k \times d_j&lt;/script&gt;(i和j为词汇p,q对应的ID)，可以得到两个列向量，然后通过某种距离计算方式可以得到两者的语义距离.&lt;/p&gt;

&lt;h4 id=&quot;3-分本聚类&quot;&gt;3. 分本聚类&lt;/h4&gt;
&lt;p&gt;低阶降维之后可以的到语义空间的文档特征，从而可以利用一些无监督的聚类算法进行文档聚类，当然也可以作为分档分类的特征。&lt;/p&gt;

&lt;h4 id=&quot;4-lsa-缺点&quot;&gt;4. LSA 缺点&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;新生成的矩阵的解释性比较差.造成这种难以解释的结果是因为SVD只是一种数学变换，并无法对应成现实中的概念。&lt;/li&gt;
  &lt;li&gt;LSA无法扑捉一词多以的现象。在原始词-向量矩阵中，每个文档的每个词只能有一个含义。比如同一篇文章中的“The Chair of Board”和”the chair maker”的chair会被认为一样。在语义空间中，含有一词多意现象的词其向量会呈现多个语义的平均。相应的，如果有其中一个含义出现的特别频繁，则语义向量会向其倾斜。&lt;/li&gt;
  &lt;li&gt;LSA具有词袋模型的缺点，即在一篇文章，或者一个句子中忽略词语的先后顺序。&lt;/li&gt;
  &lt;li&gt;LSA的概率模型假设文档和词的分布是服从联合正态分布的，但从观测数据来看是服从泊松分布的。因此LSA算法的一个改进PLSA使用了多项分布，其效果要好于LSA。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;参考资料&quot;&gt;参考资料&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.puffinwarellc.com/index.php/news-and-articles/articles/30-singular-value-decomposition-tutorial.html?showall=1&quot;&gt;Singular Value Decomposition (SVD) Tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.sciencenet.cn/blog-696950-699432.html&quot;&gt;奇异值分解(SVD) — 几何意义&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/21874816&quot;&gt;如何理解矩阵特征值？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://charlesx.top/2016/03/Singularly-Valuable-Decomposition/&quot;&gt;漫谈奇异值分解&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.math.washington.edu/~morrow/464_14/svd.pdf&quot;&gt;Dan Kalman. A singularly valuable decomposition: The SVD of a matrix[J]. College Mathematics Journal, 1996, 27(1):2–23.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/roger__wong/article/details/41175967&quot;&gt;LSA潜在语义分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sat, 02 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/02/SVD.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/02/SVD.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>Random Forest</title>
        <description>&lt;p&gt;一般我们提到序列标注问题，会有三种模型很引人瞩目，最大熵马尔科夫模型、隐马尔科夫模型和条件穗机场模型。三种的联系和区别详见：隐马尔可夫模型 最大熵马尔可夫模型 条件随机场 区别和联系。&lt;/p&gt;

&lt;p&gt;###HMM简单介绍&lt;/p&gt;

&lt;p&gt;在介绍CRF建模过程，我们先简单的了解下HMM，因为HMM可以认为是CRF的子集，任何可以由HMM解决的问题都可以由CRF解决~&lt;/p&gt;

&lt;p&gt;HMM有两种假设：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;某种状态只依赖于前一状态，也即隐含状态满足一阶马尔科夫性质；&lt;/li&gt;
  &lt;li&gt;观测状态之间彼此独立&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面用一个简单的中文分词来介绍HMM吧。
一般在中文分词中，我们会设定某个中文汉字所处的状态（B:词的开头；M：在词中间；E：在词结尾；S：单字成词），这四种状态（BMES）可以认为是我们的隐含状态，而该状态对应的中文字符，则是我们的观测状态。&lt;/p&gt;

&lt;p&gt;通过大量的语料的统计可以得到这四种状态之间的转移概率矩阵p(SJ|SI)p(SJ|SI)
即由状态SISI转移到SJSJ的概率。同时也可以统计出初始状态的概率π(SI)π(SI)，另外我们的发射概率（也即有隐含状态（BMES）生成汉字的概率）也可以通过统计得出，当然了，也可以利用EM算法去训练，但在此不再赘述。&lt;/p&gt;

&lt;p&gt;HMM一般会解决三类问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;模型学习，可以使用统计也可以使用EM；&lt;/li&gt;
  &lt;li&gt;给定观测状态，确定最可能的隐含状态序列，比如中文分词、词性标注等；&lt;/li&gt;
  &lt;li&gt;给定隐含状态，确定最可能的观测序列，比如天气预报。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;中文分词给定的是观测序列（即一个个的中文字符），任务即是找到某种最可能的隐含状态，也即判断中文字符所处的位置(BMES)，使得该观测序列产生的概率最大。在进行计算的时候，我们不可能对所有的序列进行判断， 这将是一个NP问题。下图是对“我来自武汉”的一个HMM模型图示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/image/t.png&quot; alt=&quot;HMM图片&quot; /&gt;&lt;/p&gt;

&lt;p&gt;途中蓝色部分为隐含状态之间的转移概率矩阵，蓝线表示状态的转移。红线表示由隐含状态生成观测状态。计算的时候，我们可以根据迭代公式计算出前向概率，然后反向递推，得到生成该序列最大概率的隐含状态序列。比如上图中最后最有可能得到的生成“我、来、自、武、汉”这一观测序列的隐含状态序列为“SBEBE”，也即分词结果为”我\来自\武汉”。&lt;/p&gt;

&lt;p&gt;###CRF&lt;/p&gt;

&lt;p&gt;HMM的假设简单粗暴，却行之有效，然而在分词或者词性标注这些应用上，却无法达到CRF所取得的效果，原因之一正是在于其假设。HMM过于local，导致其损失了非常多global的信息，而CRF却可以利用这些信息，从而可以得到更好的结果。&lt;/p&gt;

&lt;p&gt;CRF自从被Lafferty等人发明之后，由最初的应用于序列标注的自然语言处理领域广泛的扩散至生物信息学、计算机视觉等领域。CRF中十分重要的一 个概念为特征函数，其是状态转移特征函数和状态生成特征函数的集合。其实刚开始看CRF的时候这个特征函数很不容易理解，但是我们可以把它理解成一个对某种规则的一种特征化处理，比如在词性标注中，在训练的时候如果当前词为“形容词”，并且它以“的”结尾，那么该特征函数输出为1，否则为0。并且我们需要对每一个特征函数赋以某种权值，则如果最后的结果中该特征和权重的积较大，可以表征：以”的“结尾的词以某种概率是形容词“，而这个权重λλ是模型训练的目标。&lt;/p&gt;

&lt;p&gt;参考文献&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://wenku.baidu.com/link?url=7LBbXiKPWAPnqYexmBOhz4iCUSny6Ayg3M53Ls0IiVKdqLq-9YPNAiW3WKJ5UgihjWKmm4yTpahIIeu75BB_mM_Q1QicaLIGrOiwHUO8ktu###&quot;&gt;百度文库&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/felomeng/article/details/4367250&quot;&gt;http://blog.csdn.net/felomeng/article/details/4367250&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sat, 02 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/02/RandomForests.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/02/RandomForests.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
      <item>
        <title>HMM</title>
        <description>&lt;p&gt;在了解隐马尔科夫模型之前，先得了解马尔科夫过程，一般而言，在理解一个模型的时候，最好是能够知道该模型提出的初衷，站在设计者的角度来思考模型总能达到事半功倍的效果。
n阶马尔科夫过过程的当前状态只依赖于前n个时刻的状态，通常而言，n一般为1，也即常见的马尔科夫过程。##&lt;/p&gt;

&lt;p&gt;隐马尔科夫模型在基础的马尔科夫过程中做了简单的变换，将状态作为一种隐含状态，观测序列作为观测状态，并且状态之间有概率转移关系，状态到观测序列之间也有概率转移关系。
由此可以得到隐马尔科夫模型的形式化表示：HMM=(pi,A,B)&lt;/p&gt;

&lt;p&gt;其中，pi为系统的初始状态，A为状态转移矩阵，其中&lt;script type=&quot;math/tex&quot;&gt;A_{i,j}&lt;/script&gt;表示从隐含状态i变成隐含状态j的概率，B为混淆矩阵或者观测状态生成矩阵，
&lt;script type=&quot;math/tex&quot;&gt;B_{i,j}&lt;/script&gt;表示隐含状态i生成观测序列元素j的概率。
在实例讲解隐马尔科夫模型的例子中，有一个较为经典，也即海藻和天气。海藻的状态我们随时可以观测得到，而天气的状态是一种未知的变量。假设我们知道天气之间变化的概率A，也知道当海藻处于某种观测状态的前提下各种天气状态的出现概率B，同时也有初始状态，也即给出了一个HMM模型，那么我们可以解决以下两种问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;给定海藻的观测状态序列，求出该观测序列出现的概率，对应于HMM三大问题之概率计算问题&lt;/li&gt;
  &lt;li&gt;给定某种观测状态序列，求出该观测序列出现的最大的状态概率，对应于HMM三大问题之预测问题&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于第一种问题，可以使用最为原始的暴力出奇迹，枚举所有的状态，求出状态和观测序列的联合概率分布，然后进行求和，即可得到最后的结果，也可以使用更为优雅的算法，比如即将介绍的基于动态规划的forward-backward算法，这两种算法也是HMM这类概率图模型所特有的一种求解方法，就像是BP之于NN一般。&lt;/p&gt;

&lt;h4 id=&quot;1-forword-algorithm&quot;&gt;1&amp;gt; forword-algorithm&lt;/h4&gt;

&lt;p&gt;前向算法，是一种动态规划算法，之前看NLP的另外一个算法中也使用了动态规划的思想，它具有许多独特的性质，比如子问题最优，也即全局最优可以通过枚举子问题最优得到，联想到HMM的图模型结构，我们不难得知，在时刻t处于状态i的情况(此时观测序列为O1,O2,O3,…,Ot)下，它可以由多种子状态按照HMM的性质转化而来，即可以由时刻t-1处于状态j的情况(此时观测序列为O1,O2,O3,…,Ot-1)首先从状态i转换到j，然后生成观测序列Ot得到，所以可以得到如下的DP状态转移方程：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha[t][i] = \sum_{j=1}^N{\alpha[t-1][j] * A[j][i]} * B[i][t]&lt;/script&gt;

&lt;p&gt;上述方程也可以通过贝叶斯公式推出来。由此，问题1可以通过累加Alpha[T][i]得到。&lt;/p&gt;

&lt;h3 id=&quot;2-backward-algorithm&quot;&gt;2&amp;gt; Backward-algorithm&lt;/h3&gt;

&lt;p&gt;有了前向算法，后向算法就很容易理解了，在时刻t处于状态i的前提下，观测序列为O(t+1)…O(T)的情况可以由在时刻t+1处于状态j的前提下的情况，首先从状态j转移到状态i，然后生成观测序列O(t+1)的过程得到，也即：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\beta[t][i] = \sum_{j=1}^N{\beta[t+1][j] * A[j][i] * B[j][t+1]}&lt;/script&gt;

&lt;p&gt;由此，问题1也可以通过Beta计算得到.&lt;/p&gt;

&lt;p&gt;HMM的基于统计的train的过程以及decode的过程见 HMM ，其中还给出了HMM在分词中的应用～&lt;/p&gt;

</description>
        <pubDate>Sat, 02 Apr 2016 03:24:56 +0800</pubDate>
        <link>http://kymo.github.io/machinelearning/2016/04/02/HMM.html</link>
        <guid isPermaLink="true">http://kymo.github.io/machinelearning/2016/04/02/HMM.html</guid>
        
        
        <category>MachineLearning</category>
        
      </item>
    
  </channel>
</rss>
