<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Support Vector Machine</title>
  <meta name="description" content="Support Vector Machine ，支持向量机，通常用来进行classification，但是也有做regression。SVM在面对非线性问题上具有独特的优势。本文从linear和nonlinear两种情况下对SVM的建模过程、优化目标的求解推导过程以及优化算法SMO进行阐述。">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://kymo.github.io/tt/2016/04/02/SVM.html">
  <link rel="alternate" type="application/rss+xml" title="Aron's blog" href="http://kymo.github.io/feed.xml" />
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Aron's blog</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Support Vector Machine</h1>
    <p class="post-meta">Apr 2, 2016</p>
  </header>

  <article class="post-content">
    <p>Support Vector Machine ，支持向量机，通常用来进行classification，但是也有做regression。SVM在面对非线性问题上具有独特的优势。本文从linear和nonlinear两种情况下对SVM的建模过程、优化目标的求解推导过程以及优化算法SMO进行阐述。</p>

<p>1、  linear SVM</p>

<table>
  <tbody>
    <tr>
      <td>在分类任务中，样本label为({-1,1})，关于从sign distance转换到geometry distance的过程其实很容易理解，sign distance可以衡量某个样本被分类的置信，如果sign distance越大，那么该样本被分为该类别的可信度就更大；而geometry distance可以理解为样本距离超平面(Y = w^TX + b)的距离，是sign distance归一化的结果，求解目标为(argmax(\frac {</td>
      <td>w^Tx + b</td>
      <td>)} {</td>
      <td> </td>
      <td>w</td>
      <td> </td>
      <td>}))，并且需要满足约束：(y_i(w^Tx_i + b) \geq 1)，为了求解方便，可以不加证明的令(</td>
      <td>w^Tx+b</td>
      <td>=1)，形式化如下：</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>\begin{cases}argmax \frac {1} {</td>
      <td> </td>
      <td>w</td>
      <td> </td>
      <td>} \ s.t ~~ y_i(w^Tx_i+b) \geq 1 ~~i=1,2,\cdot \cdot ,n \end{cases}</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>而此时最大化</td>
      <td>w</td>
      <td>可以转换为最小化</td>
      <td>w</td>
      <td>，并且最小化</td>
      <td>w</td>
      <td>和最小化(\frac {1}{2} w^Tw)等价，所以1.1可以变为：</td>
    </tr>
  </tbody>
</table>

<p>\begin{cases}argmmin \frac {1}{2} w^Tw \ s.t ~~ y_i(w^Tx_i+b) \geq 1 ~~i=1,2,\cdot \cdot ,n \end{cases}</p>

<p>由此可以得到不等式约束问题，原始问题通过分析不难发现，求解十分困难，不过对于PSO(粒子群算法)而言，往往可以求得比较好的解。不过在碰到这种带不等式约束的问题的时候，我们可以通过拉格朗日对偶性质将原始问题转化为对偶问题，在最大熵模型中也有类似的处理过程。首先构造拉格朗日函数：</p>

<p>[ L(w,b,\alpha) = \frac {1} {2} w^Tw - \sum_{i=1}^N a_i[y_i(w^Tx_i + b) - 1] ]</p>

<p>首先我们可以得到这个函数的等价形式，令(\theta = argmax_\alpha L(w,b,\alpha))，那么：</p>

<p>[\theta=\begin{cases}\frac {1} {2} w^Tw~~~y_i(w^Tx_i+b) \geq 1\ \infty~~~~ y_i(w^Tx_i+b) &lt; 1~~\end{cases}]</p>

<p>可见，(min\theta)和原始优化目标(argmin \frac {1}{2}w^Tw)等价。令(p=min\theta)，其对偶形式(d=max_{\alpha}min_{w,b}L(w,\alpha))，那么必然会有(p \geq d)。此时令(g=argmin_{w} L(w,b,\alpha))，(g \leq L(w^<em>,b,\alpha) \leq \frac {1}{2} {w^</em>}^Tw^* = p)，所以(p \geq d)。所以此时该算法满足弱对偶，我们可以通过求解该弱对偶问题去近似求解原始问题，在EM中就是不断优化极大似然下界~ 并且可以知道的是，不管原始问题是何种优化，对偶问题都会是凸优化，也即都会存在极值。不过在SVM中，我们是可以把弱对偶加强，变成strong duality，也即(p = d)，优化对偶问题等价于对原始问题的求解。那么怎么判断该对偶问题是强对偶问题呢？KKT条件。如下：</p>

<p>\begin{cases}  \bigtriangledown L(w,b,\alpha) = 0 \ \alpha_i(y_i(w^Tx_i+b) - 1) = 0\ \alpha_i \geq 0 \end{cases}</p>

<p>很明显，此时KKT条件成立，所以满足强对偶。其中(y_i(w^Tx_i+b)-1) = 0)但此时KKT只是必要条件，不过由于我们的原始问题是凸优化，所以KKT便是充要条件了。</p>

<p>对对偶问题，首先是(min_{w,b}L(w,b,\alpha))，分别对(w)和(b)求导，得：</p>

<p>[\frac {\partial L(w,\alpha,b)} {\partial(w)} = w - \sum_{i=1}^N \alpha_iy_ix_i=0]</p>

<p>[\frac {\partial L(w,\alpha,b)} {\partial(b)} = \sum_{i=1}^N\alpha_iy_i=0]</p>

<p>随后，将(w=\sum_{i=1}^N\alpha_iy_ix_i)代入(L(w,b,\alpha))中，则有：</p>

<p>\begin{eqnarray<em>}
L(w,\alpha,b)&amp;=&amp;\frac {1}{2} w^Tw - \sum_{i=1}^N\alpha_i[y_i(w^Tx_i+b)-1]\&amp;=&amp;\frac{1}{2}\sum_{i=1}^N\alpha_iy_ix_i^T\sum_{j=1}^N\alpha_jy_jx_j - \sum_{i=1}^N\alpha_i[y_i(w^Tx_i+b)-1]\ &amp;=&amp;\frac {1}{2} \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j - \sum_{i=1}^N\alpha_i\sum_{j=1}^N\alpha_jy_jx_j^Tx_i + \sum_{i=1}^N\alpha_i\&amp;=&amp;-\frac {1}{2} \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j + \sum_{i=1}^N\alpha_i\end{eqnarray</em>}</p>

<p>所以：</p>

<p>[\begin{cases}-\frac {1}{2} \sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jx_i^Tx_j + \sum_{i=1}^N\alpha_i\s.t~~~\sum_{i=1}^N\alpha_iy_i=0 \end{cases}]</p>

<p>到这，我们还没有考虑soft margin。实际情况中，总是会存在一定的噪声数据，使得我们的分类超平面被这些噪声数据所误导，从而使得模型的variance增大，所以一般来讲都会采用soft margin构建优化函数，我们以(\varepsilon)的范围容许一定的误差，即原来的(y_i(w^Tx_i+b) \geq 1)此时为(y_i(w^Tx_i + b) \geq 1 - \varepsilon_i)，所以我们的优化目标变为：</p>

<p>[\begin{cases}argmin \frac {1}{2} w^Tw + C\sum_{i=1}^N\varepsilon_i \s.t ~~ y_i(w^Tx_i+b) \geq 1 - \varepsilon_i ~~i=1,2,\cdot \cdot ,n \ \sum_{i=1}^N\varepsilon_i \geq C \end{cases}]</p>

<p>关于上式，可以看成是利用hinge loss加l2范数正则项的结果，SVM此时的损失函数可以表示为 (min_{w,b} \sum_{i=1}^N [1-y_i(w^Tx_i+b)]<em>{+} + \lambda|w|^2 )，其中如果(z&gt;0)，那么(z</em>{+})=z，否则等于0。如果令(\varepsilon_i=1-y_i(w^Tx_i+b),\varepsilon_i \geq 0)，那么此时最优化问题为 (min_{w,b} \sum_{i=1}^N\varepsilon_i + \lambda|w|^2)，如果取(\lambda=\frac {1}{2C})，那么就和上述优化目标等价，所以可以看出，软间隔实际上是在ERM的基础上加了SRM~</p>

<p>之后的推导没有多大差别，只不过在求导的过程中出现了(\alpha_i = C - \varepsilon_i)，又有前面在对偶转换使用的KKT条件之一(\alpha_i(y_i(w^Tx_i+b) - 1) =0 )可得，在分类超平面上的点，也即满足(y_i(w^Tx_i+b)-1 + \varepsilon_i = 0)，而那些不在超平面的点，必然有(\alpha_i=0)，而在超平面上的，则有(0&lt; \alpha_i &lt; C)，在超平面之外的则是(\alpha_i=C)。</p>

<ol>
  <li>nonlinear SVM</li>
</ol>

<p>非线性情况之下，利用线性曲线去拟合，明显会产生underfitting，但是我们可以通过函数映射的方式，将原来空间中的非线性特征，映射到高维空间中，使得样本可分或者近似可分，实际上是，机器学习中有一种叫做基展开的技术，就是处理这种线性到非线性的特征映射。不过对于SVM中使用这种非线性变化是因为它能够和核函数配合的天衣无缝。</p>

<p>这里用一个简单的例子作简要说明。对于(x=(x_1,x_2))二维空间的某个点，我们将其映射到三维空间。所利用的映射函数可以为(\phi (x_1,x_2) = (x_1^2,x_2^2,2x_1x_2))，那么在三维空间中，样本线性可分的可能性更大，但是计算开销却上升了，因为在转化成对偶问题之后就产生了向量内积运算。对于原始二维空间中的两点(p=(\eta_1,\eta_2),q=(\gamma_1,\gamma_2))，在三维空间中的向量内积为(&lt;\phi(\eta_1,\eta_2),\phi(\gamma_1,\gamma_2)&gt; = \eta_1^2\gamma_1^2 + \eta_2^2\gamma_2^2 + 4\eta_1\eta_2\gamma_1\gamma_2)，这和(\eta_1^2\gamma_1^2 + \eta_2^2\gamma_2^2 + 2\eta_1\eta_2\gamma_1\gamma_2)十分相似，而后者却等于(&lt;(\eta_1,\eta_2), (\gamma_1, \gamma_2)&gt;^2)，所以只需要令(\phi(x_1,x_2) = (x_1^2, x_2^2, \sqrt {2} x_1x_2))，就可以得到(&lt;\phi(\eta_1,\eta_2),\phi(\gamma_1,\gamma_2)&gt;)=(&lt;(\eta_1,\eta_2), (\gamma_1, \gamma_2)&gt;^2)~ 由此可以推广到高维。不难看出在高维空间中的内积可以通过在原始空间内积的平方得到~此时(K(p,q) = &lt;\phi(\eta_1,\eta_2), \phi(\gamma_1, \gamma_2)&gt;)=(&lt;p,q&gt;^2)。对偶转换之后只需要将(x_i,x_j)的内积运算更换成(K(x_i,x_j))，即可处理非线性数据~</p>

<ol>
  <li>SMO</li>
</ol>

<p>SMO本质上上一种坐标上升优化算法，坐标上升可以理解为在(p)维向量构成的空间中，每次选择一个维度进行优化，最终能够求得比较合适的解。SMO每次选择两个参数，因为此时待求变量有(\sum_{i=1}^N\alpha_iy_i =0)的约束。SMO的优化过程如下：</p>

<p>选择(\alpha_i, \alpha_j)
固定其他参数，然后对(\alpha_i, \alpha_j)进行优化
利用(\alpha_i, \alpha_j)，对截距进行优化
在了解确切的(\alpha_i,\alpha_j)贪心选择策略之前，先假定我们已经将(\alpha_i,\alpha_j)选择妥当，然后直接对(\alpha_i,\alpha_j)进行优化。根据条件(\sum_{k=1}^N\alpha_ky_k=0)，令(A = y_i\sum_{k!=i,j}^N\alpha_ky_k). 那么(\alpha_i,\alpha_j)的关系为(\alpha_i = A - y_iy_j\alpha_j)。此时我们的优化目标[max_{\alpha} L(\alpha) = \sum_{k=1}^N\alpha_k - \frac{1}{2} \sum_{l=1}^N\sum_{k=1}^N\alpha_l\alpha_ky_ly_kK_{l,k}]</p>

<p>其中，(K_{l,k} = K(x_l, x_j))，令(B = \sum_{k!=i,j}\alpha_k, S = y_iy_j, V_i = \sum_{k!=i,j}\alpha_ky_kK_{i,k}, V_j =\sum_{k!=i,j}\alpha_ky_kK_{j,k})，那么有：</p>

<p>\begin{eqnarray<em>} L(\alpha) &amp;=&amp; \sum_{k!=i,j}^N\alpha_k + \alpha_i + \alpha_j - \frac {1}{2}[\alpha_i\alpha_i y_iy_iK_{i,i} + \alpha_j\alpha_jy_jy_jK_{j,j} + 2\alpha_i\alpha_jy_iy_jK_{i,j} + 2\sum_{k!=i,j}^N\alpha_i\alpha_ky_iy_kK_{i,k} + 2\sum_{k!=i,j}^N\alpha_j\alpha_ky_jy_kK_{j,k} + \sum_{l!=i,j}^N \sum_{k!=i,j}^N\alpha_l\alpha_ky_ly_kK_{l,k}] \                                                               &amp;=&amp; B + A - S\alpha_j + \alpha_j - \frac{1}{2} [2\alpha_i\alpha_jSK_{i,j} + \alpha_i^2K_{i,i} + \alpha_j^2K_{j,j} + 2\alpha_iy_iV_i + 2\alpha_jy_jV_j +  \sum_{l!=i,j}^N \sum_{k!=i,j}^N\alpha_l\alpha_ky_ly_kK_{l,k}]    \                                                          &amp;=&amp;-S\alpha_j + \alpha_j  - \frac{1}{2}K_{i,i}(A-S\alpha_j)^2 - \frac{1}{2} K_{j,j}\alpha_j^2 - K_{i,j}\alpha_j(A-S\alpha_j)S - \alpha_jy_jV_j - (A-S\alpha_j)y_iV_i + \varepsilon_{constant} \end{eqnarray</em>}</p>

<p>其中，(\varepsilon_{constant})为一些常量，在求极值点是可以忽略，上式对(\alpha_j)求导，有：</p>

<p>\begin{eqnarray<em>} \frac {\partial_{L(\alpha_j)}} {\partial_{\alpha_j}} &amp;=&amp; -S + 1 + ASK_{i,i} -K_{i,i}\alpha_j - K_{j,j}\alpha_j -ASK_{i,j} + 2K_{i,j}\alpha_j -y_jV_j + Sy_iV_i=0 \ \alpha_j&amp;=&amp; \frac {-S + 1 + AS(K_{i,i}-K_{i,j}) + y_j(V_i-V_j)}{K_{i,i} + K_{j,j} - 2K_{i,j}} \end{eqnarray</em>}</p>

<p>又有优化(\alpha_i,\alpha_j)的时候，其他参数没有被改变。</p>

<p>\begin{eqnarray<em>}\alpha_iy_i+\alpha_jy_j&amp;=&amp; -\sum_{k!=i,j}\alpha_k^{old}y_k=\alpha_i^{old}y_i + \alpha_j^{old}y_j \ V_i &amp;=&amp; \sum_{k!=i,j}\alpha_ky_kK_{i,k}=\sum_{k!=i,j}\alpha_k^{old}y_kK_{i,k} = \sum_{k=1}^N\alpha_k^{old}y_kK_{i,k} + b - b - \alpha_i^{old}y_iK_{i,i} - \alpha_j^{old}y_jK_{i,j} \ V_j &amp; = &amp;\sum_{k!=i,j}\alpha_ky_kK_{j,k}=\sum_{k!=i,j}\alpha_k^{old}y_kK_{j,k} = \sum_{k=1}^N\alpha_k^{old}y_kK_{j,k} + b - b - \alpha_j^{old}y_jK_{j,j} - \alpha_i^{old}y_iK_{i,j}  \end{eqnarray</em>}</p>

<p>且(g(x_i) = \sum_{k=1}^N\alpha_k^{old}y_kK_{i,k} + b)，(g(x_j) = \sum_{k=1}^N\alpha_k^{old}y_kK_{j,k} + b)，所以：</p>

<p>[V_i - V_j = g(x_i) - g(x_j) - \alpha_i^{old}y_iK_{i,i} + \alpha_j^{old}y_jK_{j,j} - \alpha_j^{old}y_jK_{i,j} + \alpha_i^{old}y_iK_{i,j} ]</p>

<p>然后，将A = (\alpha_i^{old}+S\alpha_j^{old})，S=(y_iy_j)代入(\alpha_j)表达式，可得：</p>

<p>\begin{eqnarray<em>}\alpha_j &amp;=&amp; \frac {y_jy_j - y_iy_j +(\alpha_i^{old}+y_iy_j\alpha_j^{old})y_iy_j(K_{i,i}-K_{i,j}) + y_j(g(x_i) - g(x_j) - \alpha_i^{old}y_iK_{i,i} + \alpha_j^{old}y_jK_{j,j} - \alpha_j^{old}y_jK_{i,j} + \alpha_i^{old}y_iK_{i,j})} {K_{i,i} + K_{j,j} - 2K_{i,j}} \ &amp;=&amp; \frac {y_j[y_j-y_i + y_j\alpha_j^{old}(K_{i,i} + K_{j,j} - 2K_{i,j} + g(x_i) - g(x_j) ]} {K_{i,i} + K_{j,j} - 2K_{i,j}}\ &amp;=&amp; \alpha_j^{old} + \frac{y_j[y_j-y_i + g(x_i)-g(x_j)]} {K_{i,i} + K_{j,j} - 2K_{i,j}}\end{eqnarray</em>}</p>

<p>然后令(E_i = g(x_i) - y_i)，(\eta = K_{i,i} + K_{j,j} - 2K{i,j})，(\alpha_j^{new,unc} = \alpha_j^{old} + \frac {y_j(E_i - E_j)} {\eta})，此时求出的(\alpha_j)还需要经过边界判定，对(\alpha_i,\alpha_j)有(\alpha_iy_i + \alpha_jy_j = \alpha_i^{old} + \alpha_j^{old})和(0&lt;=\alpha_i&lt;=C)，(0&lt;=\alpha_j&lt;=C)的条件限制，所以必须对(\alpha_i, \alpha_j)的上下边界(L,H)进行确认。</p>

<p>如果(y_i = y_j)，那么(L= max(0, \alpha_j^{old} - \alpha_i^{old}))，(H=min(C,C+\alpha_j^{old} - \alpha_i^{old}))
如果(y_i!=y_j)，那么(L=max(0, \alpha_j^{old} + \alpha_i^{old} - C))，(H=min(C,\alpha_j^{old} + \alpha_i^{old}))
根据上式可以得到(\alpha_j^{new})为：</p>

<p>[\alpha_j^{new} = \begin{cases} H,~~~\alpha_j^{new,unc} &gt; H \ \alpha_j^{new,unc},~~~~~ L&lt;=\alpha_j^{new,unc}&lt;=H \ L,~~~~~\alpha_j^{new,unc} &lt; L \end{cases}]</p>

<p>由此可以得到(\alpha_i^{new}= \alpha_i^{old} + y_iy_j(\alpha_j^{old} - \alpha_j^{new}))。</p>

<p>关于(alpha_i, \alpha_j)的更新策略完成，但是对于上式中的bias也需要进行更新，以保证KKT条件(\alpha_j(y_j(\sum_{i=1}^N\alpha_iy_iK_{i,j} + b) - 1) = 0)成立。</p>

<p>当(0&lt; \alpha_i^{new} &lt; C)，(y_i(\sum_{k=1}^N\alpha_ky_kK_{i,k}+b) = 1) ，所以(b_1^{new} = y_i - \sum_{k!=i,j}^N\alpha_ky_kK_{i,k} - \alpha_i^{new}y_iK_{i,i} - \alpha_j^{new}y_jK_{i,j})，又知(E_i = g(x_i) - y_i = \sum_{k=1}^N\alpha_ky_kK_{i,k} + b^{old} - y_i)，此时除(\alpha_i,\alpha_j)以外的都不会发生变化，所以(E_i = \sum_{k!=i,j}^N\alpha_ky_kK_{k,i} + \alpha_i^{old}y_iKii + \alpha_j^{old}y_jK_{i,j} + b^{old} - y_i)，也即 (b_i^{new} = -E_i - y_iK_{i,i}(\alpha_i^{new} - \alpha_i^{old}) - y_jK_{i,j}(\alpha_j^{new} - \alpha_j^{old}) + b^{old})
同1，当(0&lt; \alpha_j^{new} &lt; C)，则(b_2^{new} = -E_j - y_iK_{i,j}(\alpha_i^{new} - \alpha_i^{old}) - y_jK_{j,j}(\alpha_j^{new} - \alpha_j^{old}) + b^{old})
如果(\alpha_i^{new})和(\alpha_j^{new})都满足条件，则(b_1^{new})=(b_2^{new})
如果(\alpha_i^{new})、(\alpha_j^{new})为0或者C，那么取(b_1^{new})和(b_2^{new})的中值即可
至此，SMO算法结束，不过实际中，需要实时更新(E_i)，所以在更新完bias之后，再利用已有的信息重新更新(E_i)即可。</p>

<p>最终实现见：SVM@GITHUB</p>

<p>参考资料</p>

<p>李航 《统计学习方法》</p>

<p>http://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html</p>


  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Aron's blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>Aron's blog</li>
          <li><a href="mailto:kymowind@gmail.com">kymowind@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/kymo">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">kymo</span>
            </a>
          </li>
          

          
          <li>
            <a href="https://twitter.com/jekyllrb">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                </svg>
              </span>

              <span class="username">jekyllrb</span>
            </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
