---
layout: post
title:  "HMM"
date:  2016-04-01 19:24:56
categories: tt
---

#HMM

在了解隐马尔科夫模型之前，先得了解马尔科夫过程，一般而言，在理解一个模型的时候，最好是能够知道该模型提出的初衷，站在设计者的角度来思考模型总能达到事半功倍的效果。
n阶马尔科夫过过程的当前状态只依赖于前n个时刻的状态，通常而言，n一般为1，也即常见的马尔科夫过程。##

隐马尔科夫模型在基础的马尔科夫过程中做了简单的变换，将状态作为一种隐含状态，观测序列作为观测状态，并且状态之间有概率转移关系，状态到观测序列之间也有概率转移关系。
由此可以得到隐马尔科夫模型的形式化表示：

> HMM=(pi,A,B)HMM=(pi,A,B)

其中，pi为系统的初始状态，A为状态转移矩阵，其中AijAij表示从隐含状态i变成隐含状态j的概率，B为混淆矩阵或者观测状态生成矩阵，B)ijB)ij表示隐含状态i生成观测序列元素j的概率。
在实例讲解隐马尔科夫模型的例子中，有一个较为经典，也即海藻和天气。海藻的状态我们随时可以观测得到，而天气的状态是一种未知的变量。假设我们知道天气之间变化的概率A，也知道当海藻处于某种观测状态的前提下各种天气状态的出现概率B，同时也有初始状态，也即给出了一个HMM模型，那么我们可以解决以下两种问题：

1.给定海藻的观测状态序列，求出该观测序列出现的概率，对应于HMM三大问题之概率计算问题<br/>
2.给定某种观测状态序列，求出该观测序列出现的最大的状态概率，对应于HMM三大问题之预测问题

对于第一种问题，可以使用最为原始的暴力出奇迹，枚举所有的状态，求出状态和观测序列的联合概率分布，然后进行求和，即可得到最后的结果，也可以使用更为优雅的算法，比如即将介绍的基于动态规划的forward-backward算法，这两种算法也是HMM这类概率图模型所特有的一种求解方法，就像是BP之于NN一般。
1> forword-algorithm
前向算法，是一种动态规划算法，之前看NLP的另外一个算法中也使用了动态规划的思想，它具有许多独特的性质，比如子问题最优，也即全局最优可以通过枚举子问题最优得到，联想到HMM的图模型结构，我们不难得知，在时刻t处于状态i的情况(此时观测序列为O1,O2,O3,…,Ot)下，它可以由多种子状态按照HMM的性质转化而来，即可以由时刻t-1处于状态j的情况(此时观测序列为O1,O2,O3,…,Ot-1)首先从状态i转换到j，然后生成观测序列Ot得到，所以可以得到如下的DP状态转移方程：
Alpha[t][i] = sigma(j=1,N){Alpha[t-1][j] * A[j][i]} * B[i][t]
上述方程也可以通过贝叶斯公式推出来。由此，问题1可以通过累加Alpha[T][i]得到。

2> Backward-algorithm
有了前向算法，后向算法就很容易理解了，在时刻t处于状态i的前提下，观测序列为O(t+1)…O(T)的情况可以由在时刻t+1处于状态j的前提下的情况，首先从状态j转移到状态i，然后生成观测序列O(t+1)的过程得到，也即：
Beta[t][i] = sigma(j=1,N){Beta[t+1][j] * A[j][i] * B[j][t+1]}

由此，问题1也可以通过Beta计算得到.

HMM的基于统计的train的过程以及decode的过程见 HMM ，其中还给出了HMM在分词中的应用～




[jekyll]:      http://jekyllrb.com
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-help]: https://github.com/jekyll/jekyll-help

