---
layout: post
title:  "RNN"
date:  2017-12-01 19:24:56
categories: MachineLearning
---

#### 从神经网络谈起

了解神经网络的都知道，神经网络作为一种非线性模型，在监督学习领域取得了state-of-art的效果，其中反向传播算法的提出居功至伟，到如今仍然是主流的优化神经网络参数的算法. 递归神经网络、卷积神经网络以及深度神经网络作为人工神经网络的"变种"，仍然延续了ANN的诸多特质，如权值连接，激励函数，以神经元为计算单元等，只不过因为应用场景的不同衍生了不同的特性，如：处理变长数据、权值共享等。

为了介绍RNN，先简单的介绍ANN. ANN的结构很容易理解，一般是三层结构（输入层-隐含层-输出层）. 隐含层输出$$O_j$$ 和输出层输出$$O_k$$如下。其中$$net_j$$为隐含层第$$j$$个神经元的输入,$$u$$为输入层和隐含层的连接权值矩阵，$$v$$为隐含层和输出层之间的连接权值矩阵.

$$O_j=f(net_j)$$
$$O_k=f(net_k)$$
$$net_j=\sum_i(x_{i}*u_{i,j})+b_j$$
$$net_k=\sum_j(O_{j}*v_{j,k})+b_k$$

定义损失函数为 $$E=\sum_p\sum_k||y_k^p-d_k^p||^2$$ ,其中$$p$$为样本下标，$$y^p$$为第$$p$$个样本的预测值,$$d^p$$为样本$$p$$的的真实值。然后分别对参数$$v_{j,k}$$、$$u_{i,j}$$ 进行求导，可得：



$$\frac{\partial E}{\partial v_{j,k}} = \ 
 = df$$
















