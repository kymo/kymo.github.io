---
layout: post
title:  "RNN"
date:  2016-12-01 19:24:56
categories: MachineLearning
---

#### 从神经网络谈起

了解神经网络的都知道，神经网络作为一种非线性模型，在监督学习领域取得了state-of-art的效果，其中反向传播算法的提出居功至伟，到如今仍然是主流的优化神经网络参数的算法. 递归神经网络、卷积神经网络以及深度神经网络作为人工神经网络的"变种"，仍然延续了ANN的诸多特质，如权值连接，激励函数，以神经元为计算单元等，只不过因为应用场景的不同衍生了不同的特性，如：处理变长数据、权值共享等。

为了介绍RNN，先简单的介绍ANN. ANN的结构很容易理解，一般是三层结构（输入层-隐含层-输出层）. 隐含层输出$$o_j$$ 和输出层输出$$o_k$$如下。其中$$net_j$$为隐含层第$$j$$个神经元的输入,$$u$$为输入层和隐含层的连接权值矩阵，$$v$$为隐含层和输出层之间的连接权值矩阵.

$$
o_j=f(net_j)
$$

$$
o_k=f(net_k)
$$

$$
net_j=\sum_i(x_{i}u_{i,j})+b_j
$$

$$
net_k=\sum_j(o_{j}v_{j,k})+b_k
$$

定义损失函数为$$E_p=\frac{1}{2}\sum_k (o_k - d_k)^2$$ ,其中$$p$$为样本下标，$$o^k$$为第$$k$$个输出层神经元的输出,$$d^k$$为样本在第$k$个编码值。然后分别对参数$$v_{j,k}$$、$$u_{i,j}$$ 进行求导，可得：

$$
\frac{\partial E_p}{\partial v_{j,k}} \\
= \frac{\partial E_p}{\partial net_k} \frac{\partial net_k}{\partial v_{j,k}} \\
= \frac{\partial E_p}{\partial net_k}o_j \\
= \frac{\partial E_p}{\partial o_k}\frac{\partial o_k}{\partial net_k}o_j \\
= (o_k-d_k)o_k(1-o_k)O_j
$$

$$
\begin{align}
\sqrt{37} & = \sqrt{\frac{73^2-1}{12^2}} \\
 & = \sqrt{\frac{73^2}{12^2}\cdot\frac{73^2-1}{73^2}} \\ 
 & = \sqrt{\frac{73^2}{12^2}}\sqrt{\frac{73^2-1}{73^2}} \\
 & = \frac{73}{12}\sqrt{1 - \frac{1}{73^2}} \\ 
 & \approx \frac{73}{12}\left(1 - \frac{1}{2\cdot73^2}\right)
\end{align}
$$


$$
\frac{\partial E}{\partial net_k} = \\
= ni \\
= nhao \\
$$














