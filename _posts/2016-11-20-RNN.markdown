---
layout: post
title:  "RNN"
date:  2016-12-01 19:24:56
categories: MachineLearning
---


递归神经网络的一些小事儿
-----

### 1. 从神经网络谈起

了解神经网络的都知道，神经网络作为一种非线性模型，在监督学习领域取得了state-of-art的效果，其中反向传播算法的提出居功至伟，到如今仍然是主流的优化神经网络参数的算法. 递归神经网络、卷积神经网络以及深度神经网络作为人工神经网络的"变种"，仍然延续了ANN的诸多特质，如权值连接，激励函数，以神经元为计算单元等，只不过因为应用场景的不同衍生了不同的特性，如：处理变长数据、权值共享等。

为了介绍RNN，先简单的介绍ANN. ANN的结构很容易理解，一般是三层结构（输入层-隐含层-输出层）. 隐含层输出$$o_j$$ 和输出层输出$$o_k$$如下。其中$$net_j$$为隐含层第$$j$$个神经元的输入,$$u$$为输入层和隐含层的连接权值矩阵，$$v$$为隐含层和输出层之间的连接权值矩阵.
<p>
$$
\begin{align}
o_j & =f(net_j) \\
o_k & =f(net_k) \\ 
net_j & =\sum_i(x_{i}u_{i,j})+b_j \\
net_k & =\sum_j(o_{j}v_{j,k})+b_k
\end{align}
$$
</p>

定义损失函数为$$E_p=\frac{1}{2}\sum_k (o_k - d_k)^2$$ ,其中$$p$$为样本下标，$$o^k$$为第$$k$$个输出层神经元的输出,$$d^k$$为样本在第$k$个编码值。然后分别对参数$$v_{j,k}$$、$$u_{i,j}$$ 进行求导，可得：
<p>
$$
\begin{align}
\frac{\partial E_p}{\partial v_{j,k}} & = \frac{\partial E_p}{\partial net_k} \frac{\partial net_k}{\partial v_{j,k}} \\
& = \frac{\partial E_p}{\partial net_k}o_j \\
& = \frac{\partial E_p}{\partial o_k}\frac{\partial o_k}{\partial net_k}o_j \\
& = (o_k-d_k)o_k(1-o_k)o_j
\end{align}
$$
</p>

<p>
$$
\begin{align}
\frac{\partial E_p} {\partial u_{i,j}} & = \frac{\partial E_p} {\partial net_j} \frac{\partial net_j} {\partial u_{i,j}} \\
& =x_i \sum_k \frac{\partial E_p} {\partial net_k} \frac{\partial net_k}{\partial o_j} \frac{\partial o_j}{\partial net_j}  \\
& =x_i \sum_k \frac{\partial E_p}{\partial net_k} v_{j,k} o_j(1-o_j) \\
& = x_i o_j(1-o_j) \sum_k \frac{\partial E_p}{\partial net_k} v_{j,k} 
\end{align}
$$
</p>

从对$\frac{\partial E_p} {\partial u_{i,j}}$的推导可以得到反向传播的核心思路，令误差项$\beta_k = \frac{\partial E_p} {\partial net_k}$, 则有：

<p>
$$
\beta_k=o_l(1-o_l)\sum_l\beta_lw_{lk}
$$
</p>

反向传播的实质是基于梯度下降的优化方法，只不过在优化的过程使用了一种更为优雅的权值更新方式。

### 2. 循环神经网络

  传统的神经网络一般都是全连接结构，且非相邻两层之间是没有连接的。一般而言，定长输入的样本很容易通过神经网络来解决，但是类似于NLP中的序列标注这样的非定长输入，前向神经网络却无能为力。
  
  于是有人提出了循环神经网络(Recurrent Neural Network)，这是一种无法像前向神经网络一样有可以具象的网络结构的模型，一般认为是网络隐层节点之间有相互作用的连接，其实质可以认为是多个具有相同结构和参数的前向神经网络的stacking, 前向神经网络的数目和输入序列的长度一致，且序列中毗邻的元素对应的前向神经网络的隐层之间有互联结构，其图示( [来源](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) )如下.
:![](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg)

 上图只是一个比较抽象的结构，下面是一个以时间展开的更为具体的结构([来源](http://www.cnblogs.com/YiXiaoZhou/p/6058890.html))~
:![](http://images2015.cnblogs.com/blog/1027162/201611/1027162-20161113162111280-1753976877.png)

从图中可以看出，每个输出层神经元的输出和前向神经网络








