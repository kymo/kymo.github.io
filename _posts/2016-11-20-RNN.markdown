---
layout: post
title:  "递归神经网络的一些小事儿"
date:  2016-12-01 19:24:56
categories: MachineLearning
---

### 1. 从神经网络谈起

了解神经网络的都知道，神经网络作为一种非线性模型，在监督学习领域取得了state-of-art的效果，其中反向传播算法的提出居功至伟，到如今仍然是主流的优化神经网络参数的算法. 递归神经网络、卷积神经网络以及深度神经网络作为人工神经网络的"变种"，仍然延续了ANN的诸多特质，如权值连接，激励函数，以神经元为计算单元等，只不过因为应用场景的不同衍生了不同的特性，如：处理变长数据、权值共享等。

为了介绍RNN，先简单的介绍ANN. ANN的结构很容易理解，一般是三层结构（输入层-隐含层-输出层）. 隐含层输出$$o_j$$ 和输出层输出$$o_k$$如下。其中$$net_j$$为隐含层第$$j$$个神经元的输入,$$u$$为输入层和隐含层的连接权值矩阵，$$v$$为隐含层和输出层之间的连接权值矩阵.
<p>
$$
\begin{align}
o_j & =f(net_j) \\
o_k & =f(net_k) \\ 
net_j & =\sum_i(x_{i}u_{i,j})+b_j \\
net_k & =\sum_j(o_{j}v_{j,k})+b_k
\end{align}
$$
</p>

定义损失函数为$$E_p=\frac{1}{2}\sum_k (o_k - d_k)^2$$ ,其中$$p$$为样本下标，$$o^k$$为第$$k$$个输出层神经元的输出,$$d^k$$为样本在第$k$个编码值。然后分别对参数$$v_{j,k}$$、$$u_{i,j}$$ 进行求导，可得：
<p>
$$
\begin{align}
\frac{\partial E_p}{\partial v_{j,k}} & = \frac{\partial E_p}{\partial net_k} \frac{\partial net_k}{\partial v_{j,k}} \\
& = \frac{\partial E_p}{\partial net_k}o_j \\
& = \frac{\partial E_p}{\partial o_k}\frac{\partial o_k}{\partial net_k}o_j \\
& = (o_k-d_k)o_k(1-o_k)o_j
\end{align}
$$
</p>

<p>
$$
\begin{align}
\frac{\partial E_p} {\partial u_{i,j}} & = \frac{\partial E_p} {\partial net_j} \frac{\partial net_j} {\partial u_{i,j}} \\
& =x_i \sum_k \frac{\partial E_p} {\partial net_k} \frac{\partial net_k}{\partial o_j} \frac{\partial o_j}{\partial net_j}  \\
& =x_i \sum_k \frac{\partial E_p}{\partial net_k} v_{j,k} o_j(1-o_j) \\
& = x_i o_j(1-o_j) \sum_k \frac{\partial E_p}{\partial net_k} v_{j,k} 
\end{align}
$$
</p>

从对$\frac{\partial E_p} {\partial u_{i,j}}$的推导可以得到反向传播的核心思路，令误差项$\beta_k = \frac{\partial E_p} {\partial net_k}$, 则有：

<p>
$$
\beta_k=o_l(1-o_l)\sum_l\beta_lw_{lk}
$$
</p>

反向传播的实质是基于梯度下降的优化方法，只不过在优化的过程使用了一种更为优雅的权值更新方式。

### 2. 循环神经网络

  传统的神经网络一般都是全连接结构，且非相邻两层之间是没有连接的。一般而言，定长输入的样本很容易通过神经网络来解决，但是类似于NLP中的序列标注这样的非定长输入，前向神经网络却无能为力。

  于是有人提出了循环神经网络(Recurrent Neural Network)，这是一种无法像前向神经网络一样有可以具象的网络结构的模型，一般认为是网络隐层节点之间有相互作用的连接，其实质可以认为是多个具有相同结构和参数的前向神经网络的stacking, 前向神经网络的数目和输入序列的长度一致，且序列中毗邻的元素对应的前向神经网络的隐层之间有互联结构，其图示( [图片来源](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) )如下.
![](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg)

 上图只是一个比较抽象的结构，下面是一个以时间展开的更为具体的结构([图片来源](http://www.cnblogs.com/YiXiaoZhou/p/6058890.html)).
![](http://images2015.cnblogs.com/blog/1027162/201611/1027162-20161113162111280-1753976877.png)

从图中可以看出，输出层神经元的输入输出和前向神经网络中没有什么差异，仅仅在于隐层除了要接收输入层的输入外，还需要接受来自于自身的输入(可以理解为t时刻的隐层需要接收来自于t-1时刻隐层的输入, 当然这仅限于单向RNN的情况，在双向RNN还需要接受来自t+1时刻的输入). 

RNN的隐层是控制信息传递的重要单元，不同时刻隐层之间的连接权值决定了过去时刻对当前时刻的影响，所以会存在时间跨度过大而导致这种影响会削弱甚至消失的现象，称之为梯度消失，改进一般都是针对隐层做文章，LSTM(控制输入量，补充新的信息然后输出)，GRU(更新信息然后输出)等都是这类的改进算法. 

下图为某时刻隐层单元的结构示意图([图片来源](http://www.cnblogs.com/YiXiaoZhou/p/6058890.html)).

![](http://images2015.cnblogs.com/blog/1027162/201611/1027162-20161113162105295-307972897.png)

虽说处理的是不定长输入数据，但是某个时刻的输入还是定长的。令t时刻:输入$x_t \in R^{xdim}$ ,t隐层输出$$h_t\in R^{hdim}$$, 输出层$y_t \in R^{ydim}$, RNN和CNN有着同样的共享权值的属性，输入层到隐层的权值矩阵$V\in R^{xdim\times hdim}$, 隐层到输出层的权值矩阵$W \in R^{hdim\times ydim}$, 不同时刻隐层自连接权值矩阵$U\in R^{hdim\times hdim}$[1]. RNN有着类似于CNN的权值共享特点，所以不同时刻的U,V,W都是相同的，所有整个网络的学习目标就是优化这些参数以及偏置. RNN和普通神经网络一样，也有超过三层的结构，下文的推导仅以三层为例.

令隐含层的激励函数$f(x)$, 输出层的激励函数为$g(x)$. 则有：
<p>
$$
\begin{align}
h^t & = f(net_h^t) \\
net_h^t & = x^tV + h^{t-1}U + b_h \\
y^t & = g(net_y^t) \\
net_y^t & = h^tW + b^y
\end{align}
$$
</p>

对于单个样本，定义我们的cost function $E^t = \frac{1}{2}\|\|d^t- y^t \|\|^2$，则对权值$W_{j,k}$、$V_{i,j}$、$U_{j,r}$的求导分别如下，其中$j$表示隐层单元下标,$k$表示输出层下标,$i$表示输入层下标,$r$表示下一时刻隐含层下标.$net^t_{hj}$表示t时刻隐层第j个神经元的加权输入，$net^t_{yk}$表示t时刻输出层第k个神经元的加权输入。
<p>
$$
\begin{align}
\frac{\partial E} {\partial W_{jk}} & = \frac{\partial E} {\partial net^t_{yk}} \frac{\partial net^t_{yk}} {\partial W_{jk} } \\
& =  \frac{\partial E} {\partial net^t_{yk}} h^t_j \\
& = (d_k^t - y_k^t)y^t_k(1-y^t_k)h_j^t \\

\frac{\partial E} {\partial V_{ij}} & = \frac{\partial E} {\partial net^t_{hj}} \frac{\partial net^t_{hj}} {\partial V_{ij}} \\
& = \frac{\partial E} {\partial net^t_{hj}} x^t_i \\
& = (\sum_k \frac{\partial E}{\partial net^t_{yk}} \frac{\partial net^t_{yk}}{\partial h^t_j} \frac{\partial h^t_j}{\partial net^t_{hj}} +
\color{red}{ \sum_r \frac{\partial E}{\partial net^{t+1}_{hr}} \frac{\partial net^{t+1}_{hr}}{\partial h^{t}_{j}} \frac{\partial h^{t}_{j}} {\partial net^t_{hj}} })x^t_i \\ 
& = (\sum_k \frac{\partial E}{\partial net^t_{yk}} W_{jk} + \color{red} { \sum_r \frac{\partial E}{\partial net^{t+1}_{hr}}U_{jr}} ) \frac{\partial h^{t}_{j}} {\partial net^t_{hj}} x^t_i \\

\frac{\partial E} {\partial U_{jr}} & =  \frac{\partial E} {\partial net^{t+1}_r} \frac{\partial net^{t+1}_r}{\partial U_{jr}} \\
& = \frac{\partial E}{\partial net^{t+1}_r} h^t_j \\
& = \sum_k \frac{\partial E} {\partial net^{t+1}_{yk}} \frac {\partial net^{t+1}_{yk}} {\partial h^{t+1}_r}  \frac {\partial h^{t+1}_r} {\partial net^{t+1}_r} h^t_j \\
\end{align}
$$

上述 公式其实在了解反向传播之后就能够很容易推导，对于$W_{jk}$、$U_{jr}$的推导套用反向传播公式即可，而对于$V_{i,j}$需要加上来自于下一时刻的误差，如以上式子中红色部分所示. 对于LSTM，GRU的推导也类似，了解清楚误差传递的来源和去向，就很容易得到对当前参数的推导链式规则了.

</p>

### 参考资料
\[1\][RNN求解过程推导与实现](http://www.cnblogs.com/YiXiaoZhou/p/6058890.html)




